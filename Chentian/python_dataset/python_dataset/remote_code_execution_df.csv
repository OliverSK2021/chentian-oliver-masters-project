,id,code,label
0,9b7805119938343fcac9dc929d8882f1d97cf14a,"vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    #subprocess escapes the username stopping code injection
    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",0
1,9b7805119938343fcac9dc929d8882f1d97cf14a,"/vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd 

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    os.system(""useradd -G docker,wheel -p ""+encPass+"" ""+username)

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",1
2,9b7805119938343fcac9dc929d8882f1d97cf14a,"vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    #subprocess escapes the username stopping code injection
    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",0
3,9b7805119938343fcac9dc929d8882f1d97cf14a,"/vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd 

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    os.system(""useradd -G docker,wheel -p ""+encPass+"" ""+username)

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",1
4,269b8c87afc149911af3ae63b3ccbfc77ffb223d,"hyperion/hyperion.py/n/n#! /usr/bin/env python
from libtmux import Server
from yaml import load, dump
from setupParser import Loader
from DepTree import Node, dep_resolve, CircularReferenceException
import logging
import os
import socket
import argparse
from psutil import Process
from subprocess import call
from graphviz import Digraph
from enum import Enum
from time import sleep

import sys
from PyQt4 import QtGui
import hyperGUI

FORMAT = ""%(asctime)s: %(name)s [%(levelname)s]:\t%(message)s""

logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')
TMP_SLAVE_DIR = ""/tmp/Hyperion/slave/components""
TMP_COMP_DIR = ""/tmp/Hyperion/components""
TMP_LOG_PATH = ""/tmp/Hyperion/log""

BASE_DIR = os.path.dirname(__file__)
SCRIPT_CLONE_PATH = (""%s/scripts/start_named_clone_session.sh"" % BASE_DIR)


class CheckState(Enum):
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4


class ControlCenter:

    def __init__(self, configfile=None):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.nodes = {}
        self.server = []
        self.host_list = []

        if configfile:
            self.load_config(configfile)
            self.session_name = self.config[""name""]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug(""Loading config was successful"")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    ""session_name"": self.session_name
                })

                self.logger.info('found running session by name ""%s"" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name ""%s"" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name=""Main""
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")

        else:
            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug(""Checking component '%s' in group '%s' on host '%s'"" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != ""localhost"" and not self.run_on_localhost(comp):
                        self.copy_component_to_remote(comp, comp['name'], comp['host'])

            # Remove duplicate hosts
            self.host_list = list(set(self.host_list))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.addEdge(node)

            # Add edges based on dependencies specified in the configuration
            if ""depends"" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.addEdge(self.nodes[dep])
                    else:
                        self.logger.error(""Unmet dependency: '%s' for component '%s'!"" % (dep, node.comp_name))
                        if exit_on_fail:
                            exit(1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = """"
            for node in res:
                if node is not master_node:
                    dep_string = ""%s -> %s"" % (dep_string, node.comp_name)
            self.logger.debug(""Dependency tree for start all: %s"" % dep_string)
        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            if exit_on_fail:
                exit(1)

    def copy_component_to_remote(self, infile, comp, host):
        self.host_list.append(host)

        self.logger.debug(""Saving component to tmp"")
        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(infile, outfile, default_flow_style=False)

        self.logger.debug('Copying component ""%s"" to remote host ""%s""' % (comp, host))
        cmd = (""ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml"" %
               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))
        self.logger.debug(cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Stopping remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.stop_remote_component(comp['name'], comp['host'])
        else:
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""window '%s' found running"" % comp['name'])
                self.logger.info(""Shutting down window..."")
                kill_window(window)
                self.logger.info(""... done!"")

    def stop_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave --kill'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug(""node name '%s' vs. comp name '%s'"" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug(""Checking and starting %s"" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug(""Component %s is already running, skipping to next in line"" % comp['name'])
                else:
                    self.logger.debug(""Start component '%s' as dependency of '%s'"" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    tries = 0
                    while True:
                        self.logger.debug(""Checking %s resulted in checkstate %s"" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 100:
                            return False
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug(""All dependencies satisfied, starting '%s'"" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug(""Component %s is already running. Skipping start"" % comp['name'])
        else:
            self.start_component_without_deps(comp)
        return True

    def start_component_without_deps(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Starting remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.start_remote_component(comp['name'], comp['host'])
        else:
            log_file = (""%s/%s"" % (TMP_LOG_PATH, comp['name']))
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""Restarting '%s' in old window"" % comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])
            else:
                self.logger.info(""creating window '%s'"" % comp['name'])
                window = self.session.new_window(comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])

    def start_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        if self.run_on_localhost(comp):
            return check_component(comp, self.session, self.logger)
        else:
            self.logger.debug(""Starting remote check"")
            cmd = ""ssh %s 'hyperion --config %s/%s.yaml slave -c'"" % (comp['host'], TMP_SLAVE_DIR, comp['name'])
            ret = call(cmd, shell=True)
            return CheckState(ret)

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        try:
            hn_out = socket.gethostbyname(hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug(""Host '%s' is localhost"" % hostname)
                return True
            else:
                self.logger.debug(""Host '%s' is not localhost"" % hostname)
                return False
        except socket.gaierror:
            sys.exit(""Host '%s' is unknown! Update your /etc/hosts file!"" % hostname)

    def run_on_localhost(self, comp):
        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        cmd = ""ssh -t %s 'tmux kill-session -t %s'"" % (host, name)
        send_main_session_command(self.session, cmd)

    def start_clone_session(self, comp_name, session_name):
        cmd = ""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name)
        send_main_session_command(self.session, cmd)

    def start_remote_clone_session(self, comp_name, session_name, hostname):
        remote_cmd = (""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = ""ssh %s 'bash -s' < %s"" % (hostname, remote_cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Visualisation
    ###################
    def draw_graph(self):
        deps = Digraph(""Deps"", strict=True)
        deps.graph_attr.update(rankdir=""BT"")
        try:
            node = self.nodes.get('master_node')

            for current in node.depends_on:
                deps.node(current.comp_name)

                res = []
                unres = []
                dep_resolve(current, res, unres)
                for node in res:
                    if ""depends"" in node.component:
                        for dep in node.component['depends']:
                            if dep not in self.nodes:
                                deps.node(dep, color=""red"")
                                deps.edge(node.comp_name, dep, ""missing"", color=""red"")
                            elif node.comp_name is not ""master_node"":
                                deps.edge(node.comp_name, dep)

        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            deps.edge(ex.node1, ex.node2, ""circular error"", color=""red"")
            deps.edge(ex.node2, ex.node1, color=""red"")

        deps.view()


class SlaveLauncher:

    def __init__(self, configfile=None, kill_mode=False, check_mode=False):
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.config = None
        self.session = None
        if kill_mode:
            self.logger.info(""started slave with kill mode"")
        if check_mode:
            self.logger.info(""started slave with check mode"")
        self.server = Server()

        if self.server.has_session(""slave-session""):
            self.session = self.server.find_where({
                ""session_name"": ""slave-session""
            })

            self.logger.info('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.info('starting new slave session on server')
            self.session = self.server.new_session(
                session_name=""slave-session""
            )

        else:
            self.logger.info(""No slave session found on server. Aborting"")
            exit(CheckState.STOPPED)

        if configfile:
            self.load_config(configfile)
            self.window_name = self.config['name']
            self.flag_path = (""/tmp/Hyperion/slaves/%s"" % self.window_name)
            self.log_file = (""/tmp/Hyperion/log/%s"" % self.window_name)
            ensure_dir(self.log_file)
        else:
            self.logger.error(""No slave component config provided"")

    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
        else:
            self.logger.debug(self.config)
            window = find_window(self.session, self.window_name)

            if window:
                self.logger.debug(""window '%s' found running"" % self.window_name)
                if self.kill_mode:
                    self.logger.info(""Shutting down window..."")
                    kill_window(window)
                    self.logger.info(""... done!"")
            elif not self.kill_mode:
                self.logger.info(""creating window '%s'"" % self.window_name)
                window = self.session.new_window(self.window_name)
                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)

            else:
                self.logger.info(""There is no component running by the name '%s'. Exiting kill mode"" %
                                 self.window_name)

    def run_check(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
            exit(CheckState.STOPPED.value)

        check_state = check_component(self.config, self.session, self.logger)
        exit(check_state.value)

###################
# Component Management
###################
def run_component_check(comp):
    if call(comp['cmd'][1]['check'], shell=True) == 0:
        return True
    else:
        return False


def check_component(comp, session, logger):
    logger.debug(""Running component check for %s"" % comp['name'])
    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
    window = find_window(session, comp['name'])
    if window:
        pid = get_window_pid(window)
        logger.debug(""Found window pid: %s"" % pid)

        # May return more child pids if logging is done via tee (which then was started twice in the window too)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))
        pids = [p.pid for p in procs]
        logger.debug(""Window is running %s child processes"" % len(pids))

        # TODO: Investigate minimum process number on hosts
        # TODO: Change this when more logging options are introduced
        if len(pids) < 2:
            logger.debug(""Main window process has finished. Running custom check if available"")
            if check_available and run_component_check(comp):
                logger.debug(""Process terminated but check was successful"")
                return CheckState.STOPPED_BUT_SUCCESSFUL
            else:
                logger.debug(""Check failed or no check available: returning false"")
                return CheckState.STOPPED
        elif check_available and run_component_check(comp):
            logger.debug(""Check succeeded"")
            return CheckState.RUNNING
        elif not check_available:
            logger.debug(""No custom check specified and got sufficient pid amount: returning true"")
            return CheckState.RUNNING
        else:
            logger.debug(""Check failed: returning false"")
            return CheckState.STOPPED
    else:
        logger.debug(""%s window is not running. Running custom check"" % comp['name'])
        if check_available and run_component_check(comp):
            logger.debug(""Component was not started by Hyperion, but the check succeeded"")
            return CheckState.STARTED_BY_HAND
        else:
            logger.debug(""Window not running and no check command is available or it failed: returning false"")
            return CheckState.STOPPED


def get_window_pid(window):
    r = window.cmd('list-panes',
                   ""-F #{pane_pid}"")
    return [int(p) for p in r.stdout]

###################
# TMUX
###################
def kill_session_by_name(server, name):
    session = server.find_where({
        ""session_name"": name
    })
    session.kill_session()


def kill_window(window):
    window.cmd(""send-keys"", """", ""C-c"")
    window.kill_window()


def start_window(window, cmd, log_file, comp_name):
    setup_log(window, log_file, comp_name)
    window.cmd(""send-keys"", cmd, ""Enter"")


def find_window(session, window_name):
    window = session.find_where({
        ""window_name"": window_name
    })
    return window


def send_main_session_command(session, cmd):
    window = find_window(session, ""Main"")
    window.cmd(""send-keys"", cmd, ""Enter"")

###################
# Logging
###################
def setup_log(window, file, comp_name):
    clear_log(file)
    # Reroute stderr to log file
    window.cmd(""send-keys"", ""exec 2> >(exec tee -i -a '%s')"" % file, ""Enter"")
    # Reroute stdin to log file
    window.cmd(""send-keys"", ""exec 1> >(exec tee -i -a '%s')"" % file, ""Enter"")
    window.cmd(""send-keys"", ('echo ""#Hyperion component start: %s\n$(date)""' % comp_name), ""Enter"")


def clear_log(file_path):
    if os.path.isfile(file_path):
        os.remove(file_path)


def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

###################
# Startup
###################
def main():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    parser = argparse.ArgumentParser()

    # Create top level parser
    parser.add_argument(""--config"", '-c', type=str,
                        default='test.yaml',
                        help=""YAML config file. see sample-config.yaml. Default: test.yaml"")
    subparsers = parser.add_subparsers(dest=""cmd"")

    # Create parser for the editor command
    subparser_editor = subparsers.add_parser('edit', help=""Launches the editor to edit or create new systems and ""
                                                          ""components"")
    # Create parser for the run command
    subparser_run = subparsers.add_parser('run', help=""Launches the setup specified by the --config argument"")
    # Create parser for validator
    subparser_val = subparsers.add_parser('validate', help=""Validate the setup specified by the --config argument"")

    subparser_remote = subparsers.add_parser('slave', help=""Run a component locally without controlling it. The ""
                                                           ""control is taken care of the remote master invoking ""
                                                           ""this command.\nIf run with the --kill flag, the ""
                                                           ""passed component will be killed"")

    subparser_val.add_argument(""--visual"", help=""Generate and show a graph image"", action=""store_true"")

    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)

    remote_mutex.add_argument('-k', '--kill', help=""switch to kill mode"", action=""store_true"")
    remote_mutex.add_argument('-c', '--check', help=""Run a component check"", action=""store_true"")

    args = parser.parse_args()
    logger.debug(args)

    if args.cmd == 'edit':
        logger.debug(""Launching editor mode"")

    elif args.cmd == 'run':
        logger.debug(""Launching runner mode"")

        cc = ControlCenter(args.config)
        cc.init()
        start_gui(cc)

    elif args.cmd == 'validate':
        logger.debug(""Launching validation mode"")
        cc = ControlCenter(args.config)
        if args.visual:
            cc.set_dependencies(False)
            cc.draw_graph()
        else:
            cc.set_dependencies(True)

    elif args.cmd == 'slave':
        logger.debug(""Launching slave mode"")
        sl = SlaveLauncher(args.config, args.kill, args.check)

        if args.check:
            sl.run_check()
        else:
            sl.init()


###################
# GUI
###################
def start_gui(control_center):
    app = QtGui.QApplication(sys.argv)
    main_window = QtGui.QMainWindow()
    ui = hyperGUI.UiMainWindow()
    ui.ui_init(main_window, control_center)
    main_window.show()
    sys.exit(app.exec_())
/n/n/n",0
5,269b8c87afc149911af3ae63b3ccbfc77ffb223d,"/hyperion/hyperion.py/n/n#! /usr/bin/env python
from libtmux import Server
from yaml import load, dump
from setupParser import Loader
from DepTree import Node, dep_resolve, CircularReferenceException
import logging
import os
import socket
import argparse
from psutil import Process
from subprocess import call
from graphviz import Digraph
from enum import Enum
from time import sleep

import sys
from PyQt4 import QtGui
import hyperGUI

FORMAT = ""%(asctime)s: %(name)s [%(levelname)s]:\t%(message)s""

logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')
TMP_SLAVE_DIR = ""/tmp/Hyperion/slave/components""
TMP_COMP_DIR = ""/tmp/Hyperion/components""
TMP_LOG_PATH = ""/tmp/Hyperion/log""

BASE_DIR = os.path.dirname(__file__)
SCRIPT_CLONE_PATH = (""%s/scripts/start_named_clone_session.sh"" % BASE_DIR)


class CheckState(Enum):
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4


class ControlCenter:

    def __init__(self, configfile=None):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.nodes = {}
        self.server = []
        self.host_list = []

        if configfile:
            self.load_config(configfile)
            self.session_name = self.config[""name""]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug(""Loading config was successful"")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    ""session_name"": self.session_name
                })

                self.logger.info('found running session by name ""%s"" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name ""%s"" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name=""Main""
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")

        else:
            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug(""Checking component '%s' in group '%s' on host '%s'"" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != ""localhost"" and not self.run_on_localhost(comp):
                        self.copy_component_to_remote(comp, comp['name'], comp['host'])

            # Remove duplicate hosts
            self.host_list = list(set(self.host_list))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.addEdge(node)

            # Add edges based on dependencies specified in the configuration
            if ""depends"" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.addEdge(self.nodes[dep])
                    else:
                        self.logger.error(""Unmet dependency: '%s' for component '%s'!"" % (dep, node.comp_name))
                        if exit_on_fail:
                            exit(1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = """"
            for node in res:
                if node is not master_node:
                    dep_string = ""%s -> %s"" % (dep_string, node.comp_name)
            self.logger.debug(""Dependency tree for start all: %s"" % dep_string)
        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            if exit_on_fail:
                exit(1)

    def copy_component_to_remote(self, infile, comp, host):
        self.host_list.append(host)

        self.logger.debug(""Saving component to tmp"")
        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(infile, outfile, default_flow_style=False)

        self.logger.debug('Copying component ""%s"" to remote host ""%s""' % (comp, host))
        cmd = (""ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml"" %
               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))
        self.logger.debug(cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Stopping remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.stop_remote_component(comp['name'], comp['host'])
        else:
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""window '%s' found running"" % comp['name'])
                self.logger.info(""Shutting down window..."")
                kill_window(window)
                self.logger.info(""... done!"")

    def stop_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave --kill'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug(""node name '%s' vs. comp name '%s'"" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug(""Checking and starting %s"" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug(""Component %s is already running, skipping to next in line"" % comp['name'])
                else:
                    self.logger.debug(""Start component '%s' as dependency of '%s'"" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    tries = 0
                    while True:
                        self.logger.debug(""Checking %s resulted in checkstate %s"" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 100:
                            return False
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug(""All dependencies satisfied, starting '%s'"" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug(""Component %s is already running. Skipping start"" % comp['name'])
        else:
            self.start_component_without_deps(comp)
        return True

    def start_component_without_deps(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Starting remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.start_remote_component(comp['name'], comp['host'])
        else:
            log_file = (""%s/%s"" % (TMP_LOG_PATH, comp['name']))
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""Restarting '%s' in old window"" % comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])
            else:
                self.logger.info(""creating window '%s'"" % comp['name'])
                window = self.session.new_window(comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])

    def start_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        return check_component(comp, self.session, self.logger)

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        try:
            hn_out = socket.gethostbyname(hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug(""Host '%s' is localhost"" % hostname)
                return True
            else:
                self.logger.debug(""Host '%s' is not localhost"" % hostname)
                return False
        except socket.gaierror:
            sys.exit(""Host '%s' is unknown! Update your /etc/hosts file!"" % hostname)

    def run_on_localhost(self, comp):
        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        cmd = ""ssh -t %s 'tmux kill-session -t %s'"" % (host, name)
        send_main_session_command(self.session, cmd)

    def start_clone_session(self, comp_name, session_name):
        cmd = ""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name)
        send_main_session_command(self.session, cmd)

    def start_remote_clone_session(self, comp_name, session_name, hostname):
        remote_cmd = (""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = ""ssh %s 'bash -s' < %s"" % (hostname, remote_cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Visualisation
    ###################
    def draw_graph(self):
        deps = Digraph(""Deps"", strict=True)
        deps.graph_attr.update(rankdir=""BT"")
        try:
            node = self.nodes.get('master_node')

            for current in node.depends_on:
                deps.node(current.comp_name)

                res = []
                unres = []
                dep_resolve(current, res, unres)
                for node in res:
                    if ""depends"" in node.component:
                        for dep in node.component['depends']:
                            if dep not in self.nodes:
                                deps.node(dep, color=""red"")
                                deps.edge(node.comp_name, dep, ""missing"", color=""red"")
                            elif node.comp_name is not ""master_node"":
                                deps.edge(node.comp_name, dep)

        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            deps.edge(ex.node1, ex.node2, ""circular error"", color=""red"")
            deps.edge(ex.node2, ex.node1, color=""red"")

        deps.view()


class SlaveLauncher:

    def __init__(self, configfile=None, kill_mode=False, check_mode=False):
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.config = None
        self.session = None
        if kill_mode:
            self.logger.info(""started slave with kill mode"")
        if check_mode:
            self.logger.info(""started slave with check mode"")
        self.server = Server()

        if self.server.has_session(""slave-session""):
            self.session = self.server.find_where({
                ""session_name"": ""slave-session""
            })

            self.logger.info('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.info('starting new slave session on server')
            self.session = self.server.new_session(
                session_name=""slave-session""
            )

        else:
            self.logger.info(""No slave session found on server. Aborting"")
            exit(CheckState.STOPPED)

        if configfile:
            self.load_config(configfile)
            self.window_name = self.config['name']
            self.flag_path = (""/tmp/Hyperion/slaves/%s"" % self.window_name)
            self.log_file = (""/tmp/Hyperion/log/%s"" % self.window_name)
            ensure_dir(self.log_file)
        else:
            self.logger.error(""No slave component config provided"")

    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
        else:
            self.logger.debug(self.config)
            window = find_window(self.session, self.window_name)

            if window:
                self.logger.debug(""window '%s' found running"" % self.window_name)
                if self.kill_mode:
                    self.logger.info(""Shutting down window..."")
                    kill_window(window)
                    self.logger.info(""... done!"")
            elif not self.kill_mode:
                self.logger.info(""creating window '%s'"" % self.window_name)
                window = self.session.new_window(self.window_name)
                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)

            else:
                self.logger.info(""There is no component running by the name '%s'. Exiting kill mode"" %
                                 self.window_name)

    def run_check(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
            exit(CheckState.STOPPED.value)

        check_state = check_component(self.config, self.session, self.logger)
        exit(check_state.value)

###################
# Component Management
###################
def run_component_check(comp):
    if call(comp['cmd'][1]['check'], shell=True) == 0:
        return True
    else:
        return False


def check_component(comp, session, logger):
    logger.debug(""Running component check for %s"" % comp['name'])
    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
    window = find_window(session, comp['name'])
    if window:
        pid = get_window_pid(window)
        logger.debug(""Found window pid: %s"" % pid)

        # May return more child pids if logging is done via tee (which then was started twice in the window too)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))
        pids = [p.pid for p in procs]
        logger.debug(""Window is running %s child processes"" % len(pids))

        # Two processes are tee logging
        # TODO: Change this when more logging options are introduced
        if len(pids) < 3:
            logger.debug(""Main window process has finished. Running custom check if available"")
            if check_available and run_component_check(comp):
                logger.debug(""Process terminated but check was successful"")
                return CheckState.STOPPED_BUT_SUCCESSFUL
            else:
                logger.debug(""Check failed or no check available: returning false"")
                return CheckState.STOPPED
        elif check_available and run_component_check(comp):
            logger.debug(""Check succeeded"")
            return CheckState.RUNNING
        elif not check_available:
            logger.debug(""No custom check specified and got sufficient pid amount: returning true"")
            return CheckState.RUNNING
        else:
            logger.debug(""Check failed: returning false"")
            return CheckState.STOPPED
    else:
        logger.debug(""%s window is not running. Running custom check"" % comp['name'])
        if check_available and run_component_check(comp):
            logger.debug(""Component was not started by Hyperion, but the check succeeded"")
            return CheckState.STARTED_BY_HAND
        else:
            logger.debug(""Window not running and no check command is available or it failed: returning false"")
            return CheckState.STOPPED


def get_window_pid(window):
    r = window.cmd('list-panes',
                   ""-F #{pane_pid}"")
    return [int(p) for p in r.stdout]

###################
# TMUX
###################
def kill_session_by_name(server, name):
    session = server.find_where({
        ""session_name"": name
    })
    session.kill_session()


def kill_window(window):
    window.cmd(""send-keys"", """", ""C-c"")
    window.kill_window()


def start_window(window, cmd, log_file, comp_name):
    setup_log(window, log_file, comp_name)
    window.cmd(""send-keys"", cmd, ""Enter"")


def find_window(session, window_name):
    window = session.find_where({
        ""window_name"": window_name
    })
    return window


def send_main_session_command(session, cmd):
    window = find_window(session, ""Main"")
    window.cmd(""send-keys"", cmd, ""Enter"")


###################
# Logging
###################
def setup_log(window, file, comp_name):
    clear_log(file)
    # Reroute stderr to log file
    window.cmd(""send-keys"", ""exec 2> >(exec tee -i -a '%s')"" % file, ""Enter"")
    # Reroute stdin to log file
    window.cmd(""send-keys"", ""exec 1> >(exec tee -i -a '%s')"" % file, ""Enter"")
    window.cmd(""send-keys"", ('echo ""#Hyperion component start: %s\n$(date)""' % comp_name), ""Enter"")


def clear_log(file_path):
    if os.path.isfile(file_path):
        os.remove(file_path)


def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

###################
# Startup
###################
def main():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    parser = argparse.ArgumentParser()

    # Create top level parser
    parser.add_argument(""--config"", '-c', type=str,
                        default='test.yaml',
                        help=""YAML config file. see sample-config.yaml. Default: test.yaml"")
    subparsers = parser.add_subparsers(dest=""cmd"")

    # Create parser for the editor command
    subparser_editor = subparsers.add_parser('edit', help=""Launches the editor to edit or create new systems and ""
                                                          ""components"")
    # Create parser for the run command
    subparser_run = subparsers.add_parser('run', help=""Launches the setup specified by the --config argument"")
    # Create parser for validator
    subparser_val = subparsers.add_parser('validate', help=""Validate the setup specified by the --config argument"")

    subparser_remote = subparsers.add_parser('slave', help=""Run a component locally without controlling it. The ""
                                                           ""control is taken care of the remote master invoking ""
                                                           ""this command.\nIf run with the --kill flag, the ""
                                                           ""passed component will be killed"")

    subparser_val.add_argument(""--visual"", help=""Generate and show a graph image"", action=""store_true"")

    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)

    remote_mutex.add_argument('-k', '--kill', help=""switch to kill mode"", action=""store_true"")
    remote_mutex.add_argument('-c', '--check', help=""Run a component check"", action=""store_true"")

    args = parser.parse_args()
    logger.debug(args)

    if args.cmd == 'edit':
        logger.debug(""Launching editor mode"")

    elif args.cmd == 'run':
        logger.debug(""Launching runner mode"")

        cc = ControlCenter(args.config)
        cc.init()
        start_gui(cc)

    elif args.cmd == 'validate':
        logger.debug(""Launching validation mode"")
        cc = ControlCenter(args.config)
        if args.visual:
            cc.set_dependencies(False)
            cc.draw_graph()
        else:
            cc.set_dependencies(True)

    elif args.cmd == 'slave':
        logger.debug(""Launching slave mode"")
        sl = SlaveLauncher(args.config, args.kill, args.check)

        if args.check:
            sl.run_check()
        else:
            sl.init()


###################
# GUI
###################
def start_gui(control_center):
    app = QtGui.QApplication(sys.argv)
    main_window = QtGui.QMainWindow()
    ui = hyperGUI.UiMainWindow()
    ui.ui_init(main_window, control_center)
    main_window.show()
    sys.exit(app.exec_())
/n/n/n",1
6,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
import re
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.text import encode_for_xml
from invenio.utils.filedownload import download_url

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""
    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        line_nb = len(text_lines)
        word_nb = 0
        for line in text_lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))
        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            local_file = download_url(entry)
            text_lines = extractor.text_lines_from_local_file(local_file)
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Output keywords reading a local file.

    Arguments and output are the same as for :see: get_keywords_from_text().
    """"""
    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings.

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""
    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext.

    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/ninvenio/legacy/bibclassify/ontology_reader.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify ontology reader.

The ontology reader reads currently either a RDF/SKOS taxonomy or a
simple controlled vocabulary file (1 word per line). The first role of
this module is to manage the cached version of the ontology file. The
second role is to hold all methods responsible for the creation of
regular expressions. These methods are grammatically related as we take
care of different forms of the same words.  The grammatical rules can be
configured via the configuration file.

The main method from this module is get_regular_expressions.
""""""

from __future__ import print_function

from datetime import datetime, timedelta
from six import iteritems
from six.moves import cPickle

import os
import re
import sys
import tempfile
import time
import urllib2
import traceback
import xml.sax
import thread
import rdflib

from invenio.legacy.bibclassify import config as bconfig
from invenio.modules.classifier.errors import TaxonomyError

log = bconfig.get_logger(""bibclassify.ontology_reader"")
from invenio import config

from invenio.modules.classifier.registry import taxonomies

# only if not running in a stanalone mode
if bconfig.STANDALONE:
    dbquery = None
    from urllib2 import urlopen
else:
    from invenio.legacy import dbquery
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

_contains_digit = re.compile(""\d"")
_starts_with_non = re.compile(""(?i)^non[a-z]"")
_starts_with_anti = re.compile(""(?i)^anti[a-z]"")
_split_by_punctuation = re.compile(""(\W+)"")

_CACHE = {}


def get_cache(taxonomy_id):
    """"""Return thread-safe cache for the given taxonomy id.

    :param taxonomy_id: identifier of the taxonomy
    :type taxonomy_id: str

    :return: dictionary object (empty if no taxonomy_id
        is found), you must not change anything inside it.
        Create a new dictionary and use set_cache if you want
        to update the cache!
    """"""
    # Because of a standalone mode, we don't use the
    # invenio.data_cacher.DataCacher, but it has no effect
    # on proper functionality.

    if taxonomy_id in _CACHE:
        ctime, taxonomy = _CACHE[taxonomy_id]

        # check it is fresh version
        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)
        cache_path = _get_cache_path(onto_name)

        # if source exists and is newer than the cache hold in memory
        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:
            log.info('Forcing taxonomy rebuild as cached'
                     ' version is newer/updated.')
            return {}  # force cache rebuild

        # if cache exists and is newer than the cache hold in memory
        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:
            log.info('Forcing taxonomy rebuild as source'
                     ' file is newer/updated.')
            return {}
        log.info('Taxonomy retrieved from cache')
        return taxonomy
    return {}


def set_cache(taxonomy_id, contents):
    """"""Update cache in a thread-safe manner.""""""
    lock = thread.allocate_lock()
    lock.acquire()
    try:
        _CACHE[taxonomy_id] = (time.time(), contents)
    finally:
        lock.release()


def get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):
    """"""Return a list of patterns compiled from the RDF/SKOS ontology.

    Uses cache if it exists and if the taxonomy hasn't changed.
    """"""
    # Translate the ontology name into a local path. Check if the name
    # relates to an existing ontology.
    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)
    if not onto_path:
        raise TaxonomyError(""Unable to locate the taxonomy: '%s'.""
                            % taxonomy_name)

    cache_path = _get_cache_path(onto_name)
    log.debug('Taxonomy discovered, now we load it '
              '(from cache: %s, onto_path: %s, cache_path: %s)'
              % (not no_cache, onto_path, cache_path))

    if os.access(cache_path, os.R_OK):
        if os.access(onto_path, os.R_OK):
            if rebuild or no_cache:
                log.debug(""Cache generation was manually forced."")
                return _build_cache(onto_path, skip_cache=no_cache)
        else:
            # ontology file not found. Use the cache instead.
            log.warning(""The ontology couldn't be located. However ""
                        ""a cached version of it is available. Using it as a ""
                        ""reference."")
            return _get_cache(cache_path, source_file=onto_path)

        if (os.path.getmtime(cache_path) >
                os.path.getmtime(onto_path)):
            # Cache is more recent than the ontology: use cache.
            log.debug(""Normal situation, cache is older than ontology,""
                      "" so we load it from cache"")
            return _get_cache(cache_path, source_file=onto_path)
        else:
            # Ontology is more recent than the cache: rebuild cache.
            log.warning(""Cache '%s' is older than '%s'. ""
                        ""We will rebuild the cache"" %
                        (cache_path, onto_path))
            return _build_cache(onto_path, skip_cache=no_cache)

    elif os.access(onto_path, os.R_OK):
        if not no_cache and\
                os.path.exists(cache_path) and\
                not os.access(cache_path, os.W_OK):
            raise TaxonomyError('We cannot read/write into: %s. '
                                'Aborting!' % cache_path)
        elif not no_cache and os.path.exists(cache_path):
            log.warning('Cache %s exists, but is not readable!' % cache_path)
        log.info(""Cache not available. Building it now: %s"" % onto_path)
        return _build_cache(onto_path, skip_cache=no_cache)

    else:
        raise TaxonomyError(""We miss both source and cache""
                            "" of the taxonomy: %s"" % taxonomy_name)


def _get_remote_ontology(onto_url, time_difference=None):
    """"""Check if the online ontology is more recent than the local ontology.

    If yes, try to download and store it in Invenio's cache directory.

    Return a boolean describing the success of the operation.

    :return: path to the downloaded ontology.
    """"""
    if onto_url is None:
        return False

    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +
              ""bibclassify"" + os.sep)
    if not os.path.exists(dl_dir):
        os.mkdir(dl_dir)

    local_file = dl_dir + os.path.basename(onto_url)
    remote_modif_time = _get_last_modification_date(onto_url)
    try:
        local_modif_seconds = os.path.getmtime(local_file)
    except OSError:
        # The local file does not exist. Download the ontology.
        download = True
        log.info(""The local ontology could not be found."")
    else:
        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])
        # Let's set a time delta of 1 hour and 10 minutes.
        time_difference = time_difference or timedelta(hours=1, minutes=10)
        download = remote_modif_time > local_modif_time + time_difference
        if download:
            log.info(""The remote ontology '%s' is more recent ""
                     ""than the local ontology."" % onto_url)

    if download:
        if not _download_ontology(onto_url, local_file):
            log.warning(""Error downloading the ontology from: %s"" % onto_url)

    return local_file


def _get_ontology(ontology):
    """"""Return the (name, path, url) to the short ontology name.

    :param ontology: name of the ontology or path to the file or url.
    """"""
    onto_name = onto_path = onto_url = None

    # first assume we got the path to the file
    if os.path.exists(ontology):
        onto_name = os.path.split(os.path.abspath(ontology))[1]
        onto_path = os.path.abspath(ontology)
        onto_url = """"
    else:
        # if not, try to find it in a known locations
        discovered_file = _discover_ontology(ontology)
        if discovered_file:
            onto_name = os.path.split(discovered_file)[1]
            onto_path = discovered_file
            # i know, this sucks
            x = ontology.lower()
            if ""http:"" in x or ""https:"" in x or ""ftp:"" in x or ""file:"" in x:
                onto_url = ontology
            else:
                onto_url = """"
        else:
            # not found, look into a database
            # (it is last because when bibclassify
            # runs in a standalone mode,
            # it has no database - [rca, old-heritage]
            if not bconfig.STANDALONE:
                result = dbquery.run_sql(""SELECT name, location from clsMETHOD WHERE name LIKE %s"",
                                         ('%' + ontology + '%',))
                for onto_short_name, url in result:
                    onto_name = onto_short_name
                    onto_path = _get_remote_ontology(url)
                    onto_url = url

    return (onto_name, onto_path, onto_url)


def _discover_ontology(ontology_name):
    """"""Look for the file in a known places.

    Inside invenio/etc/bibclassify and a few other places
    like current directory.

    :param ontology: name or path name or url
    :type ontology: str

    :return: absolute path of a file if found, or None
    """"""
    last_part = os.path.split(os.path.abspath(ontology_name))[1]
    if last_part in taxonomies:
        return taxonomies.get(last_part)
    elif last_part + "".rdf"" in taxonomies:
        return taxonomies.get(last_part + "".rdf"")
    else:
        log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)

    # LEGACY
    possible_patterns = [last_part, last_part.lower()]
    if not last_part.endswith('.rdf'):
        possible_patterns.append(last_part + '.rdf')
    places = [config.CFG_CACHEDIR,
              config.CFG_ETCDIR,
              os.path.join(config.CFG_CACHEDIR, ""bibclassify""),
              os.path.join(config.CFG_ETCDIR, ""bibclassify""),
              os.path.abspath('.'),
              os.path.abspath(os.path.join(os.path.dirname(__file__),
                                           ""../../../etc/bibclassify"")),
              os.path.join(os.path.dirname(__file__), ""bibclassify""),
              config.CFG_WEBDIR]

    log.debug(""Searching for taxonomy using string: %s"" % last_part)
    log.debug(""Possible patterns: %s"" % possible_patterns)
    for path in places:

        try:
            if os.path.isdir(path):
                log.debug(""Listing: %s"" % path)
                for filename in os.listdir(path):
                    #log.debug('Testing: %s' % filename)
                    for pattern in possible_patterns:
                        filename_lc = filename.lower()
                        if pattern == filename_lc and\
                                os.path.exists(os.path.join(path, filename)):
                            filepath = os.path.abspath(os.path.join(path,
                                                                    filename))
                            if (os.access(filepath, os.R_OK)):
                                log.debug(""Found taxonomy at: %s"" % filepath)
                                return filepath
                            else:
                                log.warning('Found taxonony at: %s, but it is'
                                            ' not readable. '
                                            'Continue searching...'
                                            % filepath)
        except OSError, os_error_msg:
            log.warning('OS Error when listing path '
                        '""%s"": %s' % (str(path), str(os_error_msg)))
    log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)


class KeywordToken:

    """"""KeywordToken is a class used for the extracted keywords.

    It can be initialized with values from RDF store or from
    simple strings. Specialty of this class is that objects are
    hashable by subject - so in the dictionary two objects with the
    same subject appears as one -- :see: self.__hash__ and self.__cmp__.
    """"""

    def __init__(self, subject, store=None, namespace=None, type='HEP'):
        """"""Initialize KeywordToken with a subject.

        :param subject: string or RDF object
        :param store: RDF graph object
                      (will be used to get info about the subject)
        :param namespace: RDF namespace object, used together with store
        :param type: type of this keyword.
        """"""
        self.id = subject
        self.type = type
        self.short_id = subject
        self.concept = """"
        self.regex = []
        self.nostandalone = False
        self.spires = False
        self.fieldcodes = []
        self.compositeof = []
        self.core = False
        # True means composite keyword
        self._composite = '#Composite' in subject
        self.__hash = None

        # the tokens are coming possibly from a normal text file
        if store is None:
            subject = subject.strip()
            self.concept = subject
            self.regex = _get_searchable_regex(basic=[subject])
            self.nostandalone = False
            self.fieldcodes = []
            self.core = False
            if subject.find(' ') > -1:
                self._composite = True

        # definitions from rdf
        else:
            self.short_id = self.short_id.split('#')[-1]

            # find alternate names for this label
            basic_labels = []

            # turn those patterns into regexes only for simple keywords
            if self._composite is False:
                try:
                    for label in store.objects(subject,
                                               namespace[""prefLabel""]):
                        # XXX shall i make it unicode?
                        basic_labels.append(str(label))
                except TypeError:
                    pass
                self.concept = basic_labels[0]
            else:
                try:
                    self.concept = str(store.value(subject,
                                                   namespace[""prefLabel""],
                                                   any=True))
                except KeyError:
                    log.warning(""Keyword with subject %s has no prefLabel.""
                                "" We use raw name"" %
                                self.short_id)
                    self.concept = self.short_id

            # this is common both to composite and simple keywords
            try:
                for label in store.objects(subject, namespace[""altLabel""]):
                    basic_labels.append(str(label))
            except TypeError:
                pass

            # hidden labels are special (possibly regex) codes
            hidden_labels = []
            try:
                for label in store.objects(subject, namespace[""hiddenLabel""]):
                    hidden_labels.append(unicode(label))
            except TypeError:
                pass

            # compile regular expression that will identify this token
            self.regex = _get_searchable_regex(basic_labels, hidden_labels)

            try:
                for note in map(lambda s: str(s).lower().strip(),
                                store.objects(subject, namespace[""note""])):
                    if note == 'core':
                        self.core = True
                    elif note in (""nostandalone"", ""nonstandalone""):
                        self.nostandalone = True
                    elif 'fc:' in note:
                        self.fieldcodes.append(note[3:].strip())
            except TypeError:
                pass

            # spiresLabel does not have multiple values
            spires_label = store.value(subject, namespace[""spiresLabel""])
            if spires_label:
                self.spires = str(spires_label)

        # important for comparisons
        self.__hash = hash(self.short_id)

        # extract composite parts ids
        if store is not None and self.isComposite():
            small_subject = self.id.split(""#Composite."")[-1]
            component_positions = []
            for label in store.objects(self.id, namespace[""compositeOf""]):
                strlabel = str(label).split(""#"")[-1]
                component_name = label.split(""#"")[-1]
                component_positions.append((small_subject.find(component_name),
                                            strlabel))
            component_positions.sort()
            if not component_positions:
                log.error(""Keyword is marked as composite, ""
                          ""but no composite components refs found: %s""
                          % self.short_id)
            else:
                self.compositeof = map(lambda x: x[1], component_positions)

    def refreshCompositeOf(self, single_keywords, composite_keywords,
                           store=None, namespace=None):
        """"""Re-check sub-parts of this keyword.

        This should be called after the whole RDF was processed, because
        it is using a cache of single keywords and if that
        one is incomplete, you will not identify all parts.
        """"""
        def _get_ckw_components(new_vals, label):
            if label in single_keywords:
                new_vals.append(single_keywords[label])
            elif ('Composite.%s' % label) in composite_keywords:
                for l in composite_keywords['Composite.%s' % label].compositeof:
                    _get_ckw_components(new_vals, l)
            elif label in composite_keywords:
                for l in composite_keywords[label].compositeof:
                    _get_ckw_components(new_vals, l)
            else:
                # One single or composite keyword is missing from the taxonomy.
                # This is due to an error in the taxonomy description.
                message = ""The composite term \""%s\""""\
                          "" should be made of single keywords,""\
                          "" but at least one is missing."" % self.id
                if store is not None:
                    message += ""Needed components: %s""\
                               % list(store.objects(self.id,
                                      namespace[""compositeOf""]))
                message += "" Missing is: %s"" % label
                raise TaxonomyError(message)

        if self.compositeof:
            new_vals = []
            try:
                for label in self.compositeof:
                    _get_ckw_components(new_vals, label)
                self.compositeof = new_vals
            except TaxonomyError as err:
                # the composites will be empty
                # (better than to have confusing, partial matches)
                self.compositeof = []
                log.error(err)

    def isComposite(self):
        """"""Return value of _composite.""""""
        return self._composite

    def getComponents(self):
        """"""Return value of compositeof.""""""
        return self.compositeof

    def getType(self):
        """"""Return value of type.""""""
        return self.type

    def setType(self, value):
        """"""Set value of value.""""""
        self.type = value

    def __hash__(self):
        """"""Return _hash.

        This might change in the future but for the moment we want to
        think that if the concept is the same, then it is the same
        keyword - this sucks, but it is sort of how it is necessary
        to use now.
        """"""
        return self.__hash

    def __cmp__(self, other):
        """"""Compare objects using _hash.""""""
        if self.__hash < other.__hash__():
            return -1
        elif self.__hash == other.__hash__():
            return 0
        else:
            return 1

    def __str__(self, spires=False):
        """"""Return the best output for the keyword.""""""
        if spires:
            if self.spires:
                return self.spires
            elif self._composite:
                return self.concept.replace(':', ',')
            # default action
        return self.concept

    def output(self, spires=False):
        """"""Return string representation with spires value.""""""
        return self.__str__(spires=spires)

    def __repr__(self):
        """"""Class representation.""""""
        return ""<KeywordToken: %s>"" % self.short_id


def _build_cache(source_file, skip_cache=False):
    """"""Build the cached data.

    Either by parsing the RDF taxonomy file or a vocabulary file.

    :param source_file: source file of the taxonomy, RDF file
    :param skip_cache: if True, build cache will not be
        saved (pickled) - it is saved as <source_file.db>
    """"""
    store = rdflib.ConjunctiveGraph()

    if skip_cache:
        log.info(""You requested not to save the cache to disk."")
    else:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        # Make sure we have a cache_dir readable and writable.
        try:
            os.makedirs(cache_dir)
        except:
            pass
        if os.access(cache_dir, os.R_OK):
            if not os.access(cache_dir, os.W_OK):
                raise TaxonomyError(""Cache directory exists but is not""
                                    "" writable. Check your permissions""
                                    "" for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    timer_start = time.clock()

    namespace = None
    single_keywords, composite_keywords = {}, {}

    try:
        log.info(""Building RDFLib's conjunctive graph from: %s"" % source_file)
        try:
            store.parse(source_file)
        except urllib2.URLError:
            if source_file[0] == '/':
                store.parse(""file://"" + source_file)
            else:
                store.parse(""file:///"" + source_file)

    except rdflib.exceptions.Error as e:
        log.error(""Serious error reading RDF file"")
        log.error(e)
        log.error(traceback.format_exc())
        raise rdflib.exceptions.Error(e)

    except (xml.sax.SAXParseException, ImportError) as e:
        # File is not a RDF file. We assume it is a controlled vocabulary.
        log.error(e)
        log.warning(""The ontology file is probably not a valid RDF file. \
            Assuming it is a controlled vocabulary file."")

        filestream = open(source_file, ""r"")
        for line in filestream:
            keyword = line.strip()
            kt = KeywordToken(keyword)
            single_keywords[kt.short_id] = kt
        if not len(single_keywords):
            raise TaxonomyError('The ontology file is not well formated')

    else:  # ok, no exception happened
        log.info(""Now building cache of keywords"")
        # File is a RDF file.
        namespace = rdflib.Namespace(""http://www.w3.org/2004/02/skos/core#"")

        single_count = 0
        composite_count = 0

        subject_objects = store.subject_objects(namespace[""prefLabel""])
        for subject, pref_label in subject_objects:
            kt = KeywordToken(subject, store=store, namespace=namespace)
            if kt.isComposite():
                composite_count += 1
                composite_keywords[kt.short_id] = kt
            else:
                single_keywords[kt.short_id] = kt
                single_count += 1

    cached_data = {}
    cached_data[""single""] = single_keywords
    cached_data[""composite""] = composite_keywords
    cached_data[""creation_time""] = time.gmtime()
    cached_data[""version_info""] = {'rdflib': rdflib.__version__,
                                   'bibclassify': bconfig.VERSION}
    log.debug(""Building taxonomy... %d terms built in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    log.info(""Total count of single keywords: %d ""
             % len(single_keywords))
    log.info(""Total count of composite keywords: %d ""
             % len(composite_keywords))

    if not skip_cache:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        log.debug(""Writing the cache into: %s"" % cache_path)
        # test again, it could have changed
        if os.access(cache_dir, os.R_OK):
            if os.access(cache_dir, os.W_OK):
                # Serialize.
                filestream = None
                try:
                    filestream = open(cache_path, ""wb"")
                except IOError as msg:
                    # Impossible to write the cache.
                    log.error(""Impossible to write cache to '%s'.""
                              % cache_path)
                    log.error(msg)
                else:
                    log.debug(""Writing cache to file %s"" % cache_path)
                    cPickle.dump(cached_data, filestream, 1)
                if filestream:
                    filestream.close()

            else:
                raise TaxonomyError(""Cache directory exists but is not ""
                                    ""writable. Check your permissions ""
                                    ""for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    # now when the whole taxonomy was parsed,
    # find sub-components of the composite kws
    # it is important to keep this call after the taxonomy was saved,
    # because we don't  want to pickle regexes multiple times
    # (as they are must be re-compiled at load time)
    for kt in composite_keywords.values():
        kt.refreshCompositeOf(single_keywords, composite_keywords,
                              store=store, namespace=namespace)

    # house-cleaning
    if store:
        store.close()

    return (single_keywords, composite_keywords)


def _capitalize_first_letter(word):
    """"""Return a regex pattern with the first letter.

    Accepts both lowercase and uppercase.
    """"""
    if word[0].isalpha():
        # These two cases are necessary in order to get a regex pattern
        # starting with '[xX]' and not '[Xx]'. This allows to check for
        # colliding regex afterwards.
        if word[0].isupper():
            return ""["" + word[0].swapcase() + word[0] + ""]"" + word[1:]
        else:
            return ""["" + word[0] + word[0].swapcase() + ""]"" + word[1:]
    return word


def _convert_punctuation(punctuation, conversion_table):
    """"""Return a regular expression for a punctuation string.""""""
    if punctuation in conversion_table:
        return conversion_table[punctuation]
    return re.escape(punctuation)


def _convert_word(word):
    """"""Return the plural form of the word if it exists.

    Otherwise return the word itself.
    """"""
    out = None

    # Acronyms.
    if word.isupper():
        out = word + ""s?""
    # Proper nouns or word with digits.
    elif word.istitle():
        out = word + ""('?s)?""
    elif _contains_digit.search(word):
        out = word

    if out is not None:
        return out

    # Words with non or anti prefixes.
    if _starts_with_non.search(word):
        word = ""non-?"" + _capitalize_first_letter(_convert_word(word[3:]))
    elif _starts_with_anti.search(word):
        word = ""anti-?"" + _capitalize_first_letter(_convert_word(word[4:]))

    if out is not None:
        return _capitalize_first_letter(out)

    # A few invariable words.
    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:
        return _capitalize_first_letter(word)

    # Some exceptions that would not produce good results with the set of
    # general_regular_expressions.
    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS
    if word in regexes:
        return _capitalize_first_letter(regexes[word])

    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS
    for regex in regexes:
        if regex.search(word) is not None:
            return _capitalize_first_letter(word)

    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS
    for regex, replacement in regexes:
        stemmed = regex.sub(replacement, word)
        if stemmed != word:
            return _capitalize_first_letter(stemmed)

    return _capitalize_first_letter(word + ""s?"")


def _get_cache(cache_file, source_file=None):
    """"""Get cached taxonomy using the cPickle module.

    No check is done at that stage.

    :param cache_file: full path to the file holding pickled data
    :param source_file: if we discover the cache is obsolete, we
        will build a new cache, therefore we need the source path
        of the cache
    :return: (single_keywords, composite_keywords).
    """"""
    timer_start = time.clock()

    filestream = open(cache_file, ""rb"")
    try:
        cached_data = cPickle.load(filestream)
        version_info = cached_data['version_info']
        if version_info['rdflib'] != rdflib.__version__\
                or version_info['bibclassify'] != bconfig.VERSION:
            raise KeyError
    except (cPickle.UnpicklingError, ImportError,
            AttributeError, DeprecationWarning, EOFError):
        log.warning(""The existing cache in %s is not readable. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        return _build_cache(source_file)
    except KeyError:
        log.warning(""The existing cache %s is not up-to-date. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        if source_file and os.path.exists(source_file):
            return _build_cache(source_file)
        else:
            log.error(""The cache contains obsolete data (and it was deleted), ""
                      ""however I can't build a new cache, the source does not ""
                      ""exist or is inaccessible! - %s"" % source_file)
    filestream.close()

    single_keywords = cached_data[""single""]
    composite_keywords = cached_data[""composite""]

    # the cache contains only keys of the composite keywords, not the objects
    # so now let's resolve them into objects
    for kw in composite_keywords.values():
        kw.refreshCompositeOf(single_keywords, composite_keywords)

    log.debug(""Retrieved taxonomy from cache %s created on %s"" %
              (cache_file, time.asctime(cached_data[""creation_time""])))

    log.debug(""%d terms read in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    return (single_keywords, composite_keywords)


def _get_cache_path(source_file):
    """"""Return the path where the cache should be written/located.

    :param onto_name: name of the ontology or the full path
    :return: string, abs path to the cache file in the tmpdir/bibclassify
    """"""
    local_name = os.path.basename(source_file)
    cache_name = local_name + "".db""
    cache_dir = os.path.join(config.CFG_CACHEDIR, ""bibclassify"")

    if not os.path.isdir(cache_dir):
        os.makedirs(cache_dir)

    return os.path.abspath(os.path.join(cache_dir, cache_name))


def _get_last_modification_date(url):
    """"""Get the last modification date of the ontology.""""""
    request = urllib2.Request(url)
    request.get_method = lambda: ""HEAD""
    http_file = urlopen(request)
    date_string = http_file.headers[""last-modified""]
    parsed = time.strptime(date_string, ""%a, %d %b %Y %H:%M:%S %Z"")
    return datetime(*(parsed)[0:6])


def _download_ontology(url, local_file):
    """"""Download the ontology and stores it in CFG_CACHEDIR.""""""
    log.debug(""Copying remote ontology '%s' to file '%s'."" % (url,
                                                              local_file))
    try:
        url_desc = urlopen(url)
        file_desc = open(local_file, 'w')
        file_desc.write(url_desc.read())
        file_desc.close()
    except IOError as e:
        print(e)
        return False
    except:
        log.warning(""Unable to download the ontology. '%s'"" %
                    sys.exc_info()[0])
        return False
    else:
        log.debug(""Done copying."")
        return True


def _get_searchable_regex(basic=None, hidden=None):
    """"""Return the searchable regular expressions for the single keyword.""""""
    # Hidden labels are used to store regular expressions.
    basic = basic or []
    hidden = hidden or []

    hidden_regex_dict = {}
    for hidden_label in hidden:
        if _is_regex(hidden_label):
            hidden_regex_dict[hidden_label] = \
                re.compile(
                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]
                )
        else:
            pattern = _get_regex_pattern(hidden_label)
            hidden_regex_dict[hidden_label] = re.compile(
                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
            )

    # We check if the basic label (preferred or alternative) is matched
    # by a hidden label regex. If yes, discard it.
    regex_dict = {}
    # Create regex for plural forms and add them to the hidden labels.
    for label in basic:
        pattern = _get_regex_pattern(label)
        regex_dict[label] = re.compile(
            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
        )

    # Merge both dictionaries.
    regex_dict.update(hidden_regex_dict)

    return regex_dict.values()


def _get_regex_pattern(label):
    """"""Return a regular expression of the label.

    This takes care of plural and different kinds of separators.
    """"""
    parts = _split_by_punctuation.split(label)

    for index, part in enumerate(parts):
        if index % 2 == 0:
            # Word
            if not parts[index].isdigit() and len(parts[index]) > 1:
                parts[index] = _convert_word(parts[index])
        else:
            # Punctuation
            if not parts[index + 1]:
                # The separator is not followed by another word. Treat
                # it as a symbol.
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SYMBOLS
                )
            else:
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SEPARATORS
                )

    return """".join(parts)


def _is_regex(string):
    """"""Check if a concept is a regular expression.""""""
    return string[0] == ""/"" and string[-1] == ""/""


def check_taxonomy(taxonomy):
    """"""Check the consistency of the taxonomy.

    Outputs a list of errors and warnings.
    """"""
    log.info(""Building graph with Python RDFLib version %s"" %
             rdflib.__version__)

    store = rdflib.ConjunctiveGraph()

    try:
        store.parse(taxonomy)
    except:
        log.error(""The taxonomy is not a valid RDF file. Are you ""
                  ""trying to check a controlled vocabulary?"")
        raise TaxonomyError('Error in RDF file')

    log.info(""Graph was successfully built."")

    prefLabel = ""prefLabel""
    hiddenLabel = ""hiddenLabel""
    altLabel = ""altLabel""
    composite = ""composite""
    compositeOf = ""compositeOf""
    note = ""note""

    both_skw_and_ckw = []

    # Build a dictionary we will reason on later.
    uniq_subjects = {}
    for subject in store.subjects():
        uniq_subjects[subject] = None

    subjects = {}
    for subject in uniq_subjects:
        strsubject = str(subject).split(""#Composite."")[-1]
        strsubject = strsubject.split(""#"")[-1]
        if (strsubject == ""http://cern.ch/thesauri/HEPontology.rdf"" or
           strsubject == ""compositeOf""):
            continue
        components = {}
        for predicate, value in store.predicate_objects(subject):
            strpredicate = str(predicate).split(""#"")[-1]
            strobject = str(value).split(""#Composite."")[-1]
            strobject = strobject.split(""#"")[-1]
            components.setdefault(strpredicate, []).append(strobject)
        if strsubject in subjects:
            both_skw_and_ckw.append(strsubject)
        else:
            subjects[strsubject] = components

    log.info(""Taxonomy contains %s concepts."" % len(subjects))

    no_prefLabel = []
    multiple_prefLabels = []
    bad_notes = []
    # Subjects with no composite or compositeOf predicate
    lonely = []
    both_composites = []
    bad_hidden_labels = {}
    bad_alt_labels = {}
    # Problems with composite keywords
    composite_problem1 = []
    composite_problem2 = []
    composite_problem3 = []
    composite_problem4 = {}
    composite_problem5 = []
    composite_problem6 = []

    stemming_collisions = []
    interconcept_collisions = {}

    for subject, predicates in iteritems(subjects):
        # No prefLabel or multiple prefLabels
        try:
            if len(predicates[prefLabel]) > 1:
                multiple_prefLabels.append(subject)
        except KeyError:
            no_prefLabel.append(subject)

        # Lonely and both composites.
        if composite not in predicates and compositeOf not in predicates:
            lonely.append(subject)
        elif composite in predicates and compositeOf in predicates:
            both_composites.append(subject)

        # Multiple or bad notes
        if note in predicates:
            bad_notes += [(subject, n) for n in predicates[note]
                          if n not in ('nostandalone', 'core')]

        # Bad hidden labels
        if hiddenLabel in predicates:
            for lbl in predicates[hiddenLabel]:
                if lbl.startswith(""/"") ^ lbl.endswith(""/""):
                    bad_hidden_labels.setdefault(subject, []).append(lbl)

        # Bad alt labels
        if altLabel in predicates:
            for lbl in predicates[altLabel]:
                if len(re.findall(""/"", lbl)) >= 2 or "":"" in lbl:
                    bad_alt_labels.setdefault(subject, []).append(lbl)

        # Check composite
        if composite in predicates:
            for ckw in predicates[composite]:
                if ckw in subjects:
                    if compositeOf in subjects[ckw]:
                        if subject not in subjects[ckw][compositeOf]:
                            composite_problem3.append((subject, ckw))
                    else:
                        if ckw not in both_skw_and_ckw:
                            composite_problem2.append((subject, ckw))
                else:
                    composite_problem1.append((subject, ckw))

        # Check compositeOf
        if compositeOf in predicates:
            for skw in predicates[compositeOf]:
                if skw in subjects:
                    if composite in subjects[skw]:
                        if subject not in subjects[skw][composite]:
                            composite_problem6.append((subject, skw))
                    else:
                        if skw not in both_skw_and_ckw:
                            composite_problem5.append((subject, skw))
                else:
                    composite_problem4.setdefault(skw, []).append(subject)

        # Check for stemmed labels
        if compositeOf in predicates:
            labels = (altLabel, hiddenLabel)
        else:
            labels = (prefLabel, altLabel, hiddenLabel)

        patterns = {}
        for label in [lbl for lbl in labels if lbl in predicates]:
            for expression in [expr for expr in predicates[label]
                               if not _is_regex(expr)]:
                pattern = _get_regex_pattern(expression)
                interconcept_collisions.setdefault(pattern, []).\
                    append((subject, label))
                if pattern in patterns:
                    stemming_collisions.append(
                        (subject,
                         patterns[pattern],
                         (label, expression)
                         )
                    )
                else:
                    patterns[pattern] = (label, expression)

    print(""\n==== ERRORS ===="")

    if no_prefLabel:
        print(""\nConcepts with no prefLabel: %d"" % len(no_prefLabel))
        print(""\n"".join([""   %s"" % subj for subj in no_prefLabel]))
    if multiple_prefLabels:
        print((""\nConcepts with multiple prefLabels: %d"" %
               len(multiple_prefLabels)))
        print(""\n"".join([""   %s"" % subj for subj in multiple_prefLabels]))
    if both_composites:
        print((""\nConcepts with both composite properties: %d"" %
               len(both_composites)))
        print(""\n"".join([""   %s"" % subj for subj in both_composites]))
    if bad_hidden_labels:
        print(""\nConcepts with bad hidden labels: %d"" % len(bad_hidden_labels))
        for kw, lbls in iteritems(bad_hidden_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if bad_alt_labels:
        print(""\nConcepts with bad alt labels: %d"" % len(bad_alt_labels))
        for kw, lbls in iteritems(bad_alt_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if both_skw_and_ckw:
        print((""\nKeywords that are both skw and ckw: %d"" %
               len(both_skw_and_ckw)))
        print(""\n"".join([""   %s"" % subj for subj in both_skw_and_ckw]))

    print()

    if composite_problem1:
        print(""\n"".join([""SKW '%s' references an unexisting CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem1]))
    if composite_problem2:
        print(""\n"".join([""SKW '%s' references a SKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem2]))
    if composite_problem3:
        print(""\n"".join([""SKW '%s' is not composite of CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem3]))
    if composite_problem4:
        for skw, ckws in iteritems(composite_problem4):
            print(""SKW '%s' does not exist but is "" ""referenced by:"" % skw)
            print(""\n"".join([""    %s"" % ckw for ckw in ckws]))
    if composite_problem5:
        print(""\n"".join([""CKW '%s' references a CKW '%s'."" % kw
                         for kw in composite_problem5]))
    if composite_problem6:
        print(""\n"".join([""CKW '%s' is not composed by SKW '%s'."" % kw
                         for kw in composite_problem6]))

    print(""\n==== WARNINGS ===="")

    if bad_notes:
        print((""\nConcepts with bad notes: %d"" % len(bad_notes)))
        print(""\n"".join([""   '%s': '%s'"" % _note for _note in bad_notes]))
    if stemming_collisions:
        print(""\nFollowing keywords have unnecessary labels that have ""
              ""already been generated by BibClassify."")
        for subj in stemming_collisions:
            print(""   %s:\n     %s\n     and %s"" % subj)

    print(""\nFinished."")
    sys.exit(0)


def test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):
    """"""Test the cache lookup.""""""
    cache = get_cache(taxonomy_name)
    if not cache:
        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,
                                                         rebuild=rebuild_cache,
                                                         no_cache=no_cache))
        cache = get_cache(taxonomy_name)
    return (thread.get_ident(), cache)


log.info('Loaded ontology reader')

if __name__ == '__main__':
    test_cache()
/n/n/ninvenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re

from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Check if a document is a PDF file and returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[-1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    # os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Return the fulltext of the local file.

    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""
    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    # Discard lines that do not contain at least one word.
    return [line for line in lines if _ONE_WORD.search(line) is not None]


def executable_exists(executable):
    """"""Test if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False
/n/n/n",0
7,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"/invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,
                                                       user_agent=make_user_agent_string(""BibClassify""))
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Outputs keywords reading a local file. Arguments and output are the same
    as for :see: get_keywords_from_text() """"""

    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/n/invenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""
BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re
import tempfile
import urllib2
from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Checks if a document is a PDF file. Returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported. Please report this to cds.support@cern.ch."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    #os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Returns the fulltext of the local file.
    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""

    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    if not _is_english_text('\n'.join(lines)):
        log.warning(""It seems the file '%s' is unvalid and doesn't ""
                    ""contain text. Please communicate this file to the Invenio ""
                    ""team."" % document)

    line_nb = len(lines)
    word_nb = 0
    for line in lines:
        word_nb += len(re.findall(""\S+"", line))

    # Discard lines that do not contain at least one word.
    lines = [line for line in lines if _ONE_WORD.search(line) is not None]

    if not remote:
        log.info(""Local file has %d lines and %d words."" % (line_nb, word_nb))

    return lines


def _is_english_text(text):
    """"""
    Checks if a text is correct english.
    Computes the number of words in the text and compares it to the
    expected number of words (based on an average size of words of 5.1
    letters).

    @param text_lines: the text to analyze
    @type text_lines:  string
    @return:           True if the text is English, False otherwise
    @rtype:            Boolean
    """"""
    # Consider one word and one space.
    avg_word_length = 2.55 + 1
    expected_word_number = float(len(text)) / avg_word_length

    words = [word
             for word in re.split('\W', text)
             if word.isalpha()]

    word_number = len(words)

    return word_number > expected_word_number


def text_lines_from_url(url, user_agent=""""):
    """"""Returns the fulltext of the file found at the URL.""""""
    request = urllib2.Request(url)
    if user_agent:
        request.add_header(""User-Agent"", user_agent)
    try:
        distant_stream = urlopen(request)
        # Write the URL content to a temporary file.
        local_file = tempfile.mkstemp(prefix=""bibclassify."")[1]
        local_stream = open(local_file, ""w"")
        local_stream.write(distant_stream.read())
        local_stream.close()
    except:
        log.error(""Unable to read from URL %s."" % url)
        return None
    else:
        # Read lines from the temporary file.
        lines = text_lines_from_local_file(local_file, remote=True)
        os.remove(local_file)

        line_nb = len(lines)
        word_nb = 0
        for line in lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))

        return lines


def executable_exists(executable):
    """"""Tests if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False


/n/n/n",1
8,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
import re
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.text import encode_for_xml
from invenio.utils.filedownload import download_url

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""
    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        line_nb = len(text_lines)
        word_nb = 0
        for line in text_lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))
        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            local_file = download_url(entry)
            text_lines = extractor.text_lines_from_local_file(local_file)
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Output keywords reading a local file.

    Arguments and output are the same as for :see: get_keywords_from_text().
    """"""
    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings.

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""
    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext.

    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/ninvenio/legacy/bibclassify/ontology_reader.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify ontology reader.

The ontology reader reads currently either a RDF/SKOS taxonomy or a
simple controlled vocabulary file (1 word per line). The first role of
this module is to manage the cached version of the ontology file. The
second role is to hold all methods responsible for the creation of
regular expressions. These methods are grammatically related as we take
care of different forms of the same words.  The grammatical rules can be
configured via the configuration file.

The main method from this module is get_regular_expressions.
""""""

from __future__ import print_function

from datetime import datetime, timedelta
from six import iteritems
from six.moves import cPickle

import os
import re
import sys
import tempfile
import time
import urllib2
import traceback
import xml.sax
import thread
import rdflib

from invenio.legacy.bibclassify import config as bconfig
from invenio.modules.classifier.errors import TaxonomyError

log = bconfig.get_logger(""bibclassify.ontology_reader"")
from invenio import config

from invenio.modules.classifier.registry import taxonomies

# only if not running in a stanalone mode
if bconfig.STANDALONE:
    dbquery = None
    from urllib2 import urlopen
else:
    from invenio.legacy import dbquery
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

_contains_digit = re.compile(""\d"")
_starts_with_non = re.compile(""(?i)^non[a-z]"")
_starts_with_anti = re.compile(""(?i)^anti[a-z]"")
_split_by_punctuation = re.compile(""(\W+)"")

_CACHE = {}


def get_cache(taxonomy_id):
    """"""Return thread-safe cache for the given taxonomy id.

    :param taxonomy_id: identifier of the taxonomy
    :type taxonomy_id: str

    :return: dictionary object (empty if no taxonomy_id
        is found), you must not change anything inside it.
        Create a new dictionary and use set_cache if you want
        to update the cache!
    """"""
    # Because of a standalone mode, we don't use the
    # invenio.data_cacher.DataCacher, but it has no effect
    # on proper functionality.

    if taxonomy_id in _CACHE:
        ctime, taxonomy = _CACHE[taxonomy_id]

        # check it is fresh version
        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)
        cache_path = _get_cache_path(onto_name)

        # if source exists and is newer than the cache hold in memory
        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:
            log.info('Forcing taxonomy rebuild as cached'
                     ' version is newer/updated.')
            return {}  # force cache rebuild

        # if cache exists and is newer than the cache hold in memory
        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:
            log.info('Forcing taxonomy rebuild as source'
                     ' file is newer/updated.')
            return {}
        log.info('Taxonomy retrieved from cache')
        return taxonomy
    return {}


def set_cache(taxonomy_id, contents):
    """"""Update cache in a thread-safe manner.""""""
    lock = thread.allocate_lock()
    lock.acquire()
    try:
        _CACHE[taxonomy_id] = (time.time(), contents)
    finally:
        lock.release()


def get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):
    """"""Return a list of patterns compiled from the RDF/SKOS ontology.

    Uses cache if it exists and if the taxonomy hasn't changed.
    """"""
    # Translate the ontology name into a local path. Check if the name
    # relates to an existing ontology.
    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)
    if not onto_path:
        raise TaxonomyError(""Unable to locate the taxonomy: '%s'.""
                            % taxonomy_name)

    cache_path = _get_cache_path(onto_name)
    log.debug('Taxonomy discovered, now we load it '
              '(from cache: %s, onto_path: %s, cache_path: %s)'
              % (not no_cache, onto_path, cache_path))

    if os.access(cache_path, os.R_OK):
        if os.access(onto_path, os.R_OK):
            if rebuild or no_cache:
                log.debug(""Cache generation was manually forced."")
                return _build_cache(onto_path, skip_cache=no_cache)
        else:
            # ontology file not found. Use the cache instead.
            log.warning(""The ontology couldn't be located. However ""
                        ""a cached version of it is available. Using it as a ""
                        ""reference."")
            return _get_cache(cache_path, source_file=onto_path)

        if (os.path.getmtime(cache_path) >
                os.path.getmtime(onto_path)):
            # Cache is more recent than the ontology: use cache.
            log.debug(""Normal situation, cache is older than ontology,""
                      "" so we load it from cache"")
            return _get_cache(cache_path, source_file=onto_path)
        else:
            # Ontology is more recent than the cache: rebuild cache.
            log.warning(""Cache '%s' is older than '%s'. ""
                        ""We will rebuild the cache"" %
                        (cache_path, onto_path))
            return _build_cache(onto_path, skip_cache=no_cache)

    elif os.access(onto_path, os.R_OK):
        if not no_cache and\
                os.path.exists(cache_path) and\
                not os.access(cache_path, os.W_OK):
            raise TaxonomyError('We cannot read/write into: %s. '
                                'Aborting!' % cache_path)
        elif not no_cache and os.path.exists(cache_path):
            log.warning('Cache %s exists, but is not readable!' % cache_path)
        log.info(""Cache not available. Building it now: %s"" % onto_path)
        return _build_cache(onto_path, skip_cache=no_cache)

    else:
        raise TaxonomyError(""We miss both source and cache""
                            "" of the taxonomy: %s"" % taxonomy_name)


def _get_remote_ontology(onto_url, time_difference=None):
    """"""Check if the online ontology is more recent than the local ontology.

    If yes, try to download and store it in Invenio's cache directory.

    Return a boolean describing the success of the operation.

    :return: path to the downloaded ontology.
    """"""
    if onto_url is None:
        return False

    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +
              ""bibclassify"" + os.sep)
    if not os.path.exists(dl_dir):
        os.mkdir(dl_dir)

    local_file = dl_dir + os.path.basename(onto_url)
    remote_modif_time = _get_last_modification_date(onto_url)
    try:
        local_modif_seconds = os.path.getmtime(local_file)
    except OSError:
        # The local file does not exist. Download the ontology.
        download = True
        log.info(""The local ontology could not be found."")
    else:
        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])
        # Let's set a time delta of 1 hour and 10 minutes.
        time_difference = time_difference or timedelta(hours=1, minutes=10)
        download = remote_modif_time > local_modif_time + time_difference
        if download:
            log.info(""The remote ontology '%s' is more recent ""
                     ""than the local ontology."" % onto_url)

    if download:
        if not _download_ontology(onto_url, local_file):
            log.warning(""Error downloading the ontology from: %s"" % onto_url)

    return local_file


def _get_ontology(ontology):
    """"""Return the (name, path, url) to the short ontology name.

    :param ontology: name of the ontology or path to the file or url.
    """"""
    onto_name = onto_path = onto_url = None

    # first assume we got the path to the file
    if os.path.exists(ontology):
        onto_name = os.path.split(os.path.abspath(ontology))[1]
        onto_path = os.path.abspath(ontology)
        onto_url = """"
    else:
        # if not, try to find it in a known locations
        discovered_file = _discover_ontology(ontology)
        if discovered_file:
            onto_name = os.path.split(discovered_file)[1]
            onto_path = discovered_file
            # i know, this sucks
            x = ontology.lower()
            if ""http:"" in x or ""https:"" in x or ""ftp:"" in x or ""file:"" in x:
                onto_url = ontology
            else:
                onto_url = """"
        else:
            # not found, look into a database
            # (it is last because when bibclassify
            # runs in a standalone mode,
            # it has no database - [rca, old-heritage]
            if not bconfig.STANDALONE:
                result = dbquery.run_sql(""SELECT name, location from clsMETHOD WHERE name LIKE %s"",
                                         ('%' + ontology + '%',))
                for onto_short_name, url in result:
                    onto_name = onto_short_name
                    onto_path = _get_remote_ontology(url)
                    onto_url = url

    return (onto_name, onto_path, onto_url)


def _discover_ontology(ontology_name):
    """"""Look for the file in a known places.

    Inside invenio/etc/bibclassify and a few other places
    like current directory.

    :param ontology: name or path name or url
    :type ontology: str

    :return: absolute path of a file if found, or None
    """"""
    last_part = os.path.split(os.path.abspath(ontology_name))[1]
    if last_part in taxonomies:
        return taxonomies.get(last_part)
    elif last_part + "".rdf"" in taxonomies:
        return taxonomies.get(last_part + "".rdf"")
    else:
        log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)

    # LEGACY
    possible_patterns = [last_part, last_part.lower()]
    if not last_part.endswith('.rdf'):
        possible_patterns.append(last_part + '.rdf')
    places = [config.CFG_CACHEDIR,
              config.CFG_ETCDIR,
              os.path.join(config.CFG_CACHEDIR, ""bibclassify""),
              os.path.join(config.CFG_ETCDIR, ""bibclassify""),
              os.path.abspath('.'),
              os.path.abspath(os.path.join(os.path.dirname(__file__),
                                           ""../../../etc/bibclassify"")),
              os.path.join(os.path.dirname(__file__), ""bibclassify""),
              config.CFG_WEBDIR]

    log.debug(""Searching for taxonomy using string: %s"" % last_part)
    log.debug(""Possible patterns: %s"" % possible_patterns)
    for path in places:

        try:
            if os.path.isdir(path):
                log.debug(""Listing: %s"" % path)
                for filename in os.listdir(path):
                    #log.debug('Testing: %s' % filename)
                    for pattern in possible_patterns:
                        filename_lc = filename.lower()
                        if pattern == filename_lc and\
                                os.path.exists(os.path.join(path, filename)):
                            filepath = os.path.abspath(os.path.join(path,
                                                                    filename))
                            if (os.access(filepath, os.R_OK)):
                                log.debug(""Found taxonomy at: %s"" % filepath)
                                return filepath
                            else:
                                log.warning('Found taxonony at: %s, but it is'
                                            ' not readable. '
                                            'Continue searching...'
                                            % filepath)
        except OSError, os_error_msg:
            log.warning('OS Error when listing path '
                        '""%s"": %s' % (str(path), str(os_error_msg)))
    log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)


class KeywordToken:

    """"""KeywordToken is a class used for the extracted keywords.

    It can be initialized with values from RDF store or from
    simple strings. Specialty of this class is that objects are
    hashable by subject - so in the dictionary two objects with the
    same subject appears as one -- :see: self.__hash__ and self.__cmp__.
    """"""

    def __init__(self, subject, store=None, namespace=None, type='HEP'):
        """"""Initialize KeywordToken with a subject.

        :param subject: string or RDF object
        :param store: RDF graph object
                      (will be used to get info about the subject)
        :param namespace: RDF namespace object, used together with store
        :param type: type of this keyword.
        """"""
        self.id = subject
        self.type = type
        self.short_id = subject
        self.concept = """"
        self.regex = []
        self.nostandalone = False
        self.spires = False
        self.fieldcodes = []
        self.compositeof = []
        self.core = False
        # True means composite keyword
        self._composite = '#Composite' in subject
        self.__hash = None

        # the tokens are coming possibly from a normal text file
        if store is None:
            subject = subject.strip()
            self.concept = subject
            self.regex = _get_searchable_regex(basic=[subject])
            self.nostandalone = False
            self.fieldcodes = []
            self.core = False
            if subject.find(' ') > -1:
                self._composite = True

        # definitions from rdf
        else:
            self.short_id = self.short_id.split('#')[-1]

            # find alternate names for this label
            basic_labels = []

            # turn those patterns into regexes only for simple keywords
            if self._composite is False:
                try:
                    for label in store.objects(subject,
                                               namespace[""prefLabel""]):
                        # XXX shall i make it unicode?
                        basic_labels.append(str(label))
                except TypeError:
                    pass
                self.concept = basic_labels[0]
            else:
                try:
                    self.concept = str(store.value(subject,
                                                   namespace[""prefLabel""],
                                                   any=True))
                except KeyError:
                    log.warning(""Keyword with subject %s has no prefLabel.""
                                "" We use raw name"" %
                                self.short_id)
                    self.concept = self.short_id

            # this is common both to composite and simple keywords
            try:
                for label in store.objects(subject, namespace[""altLabel""]):
                    basic_labels.append(str(label))
            except TypeError:
                pass

            # hidden labels are special (possibly regex) codes
            hidden_labels = []
            try:
                for label in store.objects(subject, namespace[""hiddenLabel""]):
                    hidden_labels.append(unicode(label))
            except TypeError:
                pass

            # compile regular expression that will identify this token
            self.regex = _get_searchable_regex(basic_labels, hidden_labels)

            try:
                for note in map(lambda s: str(s).lower().strip(),
                                store.objects(subject, namespace[""note""])):
                    if note == 'core':
                        self.core = True
                    elif note in (""nostandalone"", ""nonstandalone""):
                        self.nostandalone = True
                    elif 'fc:' in note:
                        self.fieldcodes.append(note[3:].strip())
            except TypeError:
                pass

            # spiresLabel does not have multiple values
            spires_label = store.value(subject, namespace[""spiresLabel""])
            if spires_label:
                self.spires = str(spires_label)

        # important for comparisons
        self.__hash = hash(self.short_id)

        # extract composite parts ids
        if store is not None and self.isComposite():
            small_subject = self.id.split(""#Composite."")[-1]
            component_positions = []
            for label in store.objects(self.id, namespace[""compositeOf""]):
                strlabel = str(label).split(""#"")[-1]
                component_name = label.split(""#"")[-1]
                component_positions.append((small_subject.find(component_name),
                                            strlabel))
            component_positions.sort()
            if not component_positions:
                log.error(""Keyword is marked as composite, ""
                          ""but no composite components refs found: %s""
                          % self.short_id)
            else:
                self.compositeof = map(lambda x: x[1], component_positions)

    def refreshCompositeOf(self, single_keywords, composite_keywords,
                           store=None, namespace=None):
        """"""Re-check sub-parts of this keyword.

        This should be called after the whole RDF was processed, because
        it is using a cache of single keywords and if that
        one is incomplete, you will not identify all parts.
        """"""
        def _get_ckw_components(new_vals, label):
            if label in single_keywords:
                new_vals.append(single_keywords[label])
            elif ('Composite.%s' % label) in composite_keywords:
                for l in composite_keywords['Composite.%s' % label].compositeof:
                    _get_ckw_components(new_vals, l)
            elif label in composite_keywords:
                for l in composite_keywords[label].compositeof:
                    _get_ckw_components(new_vals, l)
            else:
                # One single or composite keyword is missing from the taxonomy.
                # This is due to an error in the taxonomy description.
                message = ""The composite term \""%s\""""\
                          "" should be made of single keywords,""\
                          "" but at least one is missing."" % self.id
                if store is not None:
                    message += ""Needed components: %s""\
                               % list(store.objects(self.id,
                                      namespace[""compositeOf""]))
                message += "" Missing is: %s"" % label
                raise TaxonomyError(message)

        if self.compositeof:
            new_vals = []
            try:
                for label in self.compositeof:
                    _get_ckw_components(new_vals, label)
                self.compositeof = new_vals
            except TaxonomyError as err:
                # the composites will be empty
                # (better than to have confusing, partial matches)
                self.compositeof = []
                log.error(err)

    def isComposite(self):
        """"""Return value of _composite.""""""
        return self._composite

    def getComponents(self):
        """"""Return value of compositeof.""""""
        return self.compositeof

    def getType(self):
        """"""Return value of type.""""""
        return self.type

    def setType(self, value):
        """"""Set value of value.""""""
        self.type = value

    def __hash__(self):
        """"""Return _hash.

        This might change in the future but for the moment we want to
        think that if the concept is the same, then it is the same
        keyword - this sucks, but it is sort of how it is necessary
        to use now.
        """"""
        return self.__hash

    def __cmp__(self, other):
        """"""Compare objects using _hash.""""""
        if self.__hash < other.__hash__():
            return -1
        elif self.__hash == other.__hash__():
            return 0
        else:
            return 1

    def __str__(self, spires=False):
        """"""Return the best output for the keyword.""""""
        if spires:
            if self.spires:
                return self.spires
            elif self._composite:
                return self.concept.replace(':', ',')
            # default action
        return self.concept

    def output(self, spires=False):
        """"""Return string representation with spires value.""""""
        return self.__str__(spires=spires)

    def __repr__(self):
        """"""Class representation.""""""
        return ""<KeywordToken: %s>"" % self.short_id


def _build_cache(source_file, skip_cache=False):
    """"""Build the cached data.

    Either by parsing the RDF taxonomy file or a vocabulary file.

    :param source_file: source file of the taxonomy, RDF file
    :param skip_cache: if True, build cache will not be
        saved (pickled) - it is saved as <source_file.db>
    """"""
    store = rdflib.ConjunctiveGraph()

    if skip_cache:
        log.info(""You requested not to save the cache to disk."")
    else:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        # Make sure we have a cache_dir readable and writable.
        try:
            os.makedirs(cache_dir)
        except:
            pass
        if os.access(cache_dir, os.R_OK):
            if not os.access(cache_dir, os.W_OK):
                raise TaxonomyError(""Cache directory exists but is not""
                                    "" writable. Check your permissions""
                                    "" for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    timer_start = time.clock()

    namespace = None
    single_keywords, composite_keywords = {}, {}

    try:
        log.info(""Building RDFLib's conjunctive graph from: %s"" % source_file)
        try:
            store.parse(source_file)
        except urllib2.URLError:
            if source_file[0] == '/':
                store.parse(""file://"" + source_file)
            else:
                store.parse(""file:///"" + source_file)

    except rdflib.exceptions.Error as e:
        log.error(""Serious error reading RDF file"")
        log.error(e)
        log.error(traceback.format_exc())
        raise rdflib.exceptions.Error(e)

    except (xml.sax.SAXParseException, ImportError) as e:
        # File is not a RDF file. We assume it is a controlled vocabulary.
        log.error(e)
        log.warning(""The ontology file is probably not a valid RDF file. \
            Assuming it is a controlled vocabulary file."")

        filestream = open(source_file, ""r"")
        for line in filestream:
            keyword = line.strip()
            kt = KeywordToken(keyword)
            single_keywords[kt.short_id] = kt
        if not len(single_keywords):
            raise TaxonomyError('The ontology file is not well formated')

    else:  # ok, no exception happened
        log.info(""Now building cache of keywords"")
        # File is a RDF file.
        namespace = rdflib.Namespace(""http://www.w3.org/2004/02/skos/core#"")

        single_count = 0
        composite_count = 0

        subject_objects = store.subject_objects(namespace[""prefLabel""])
        for subject, pref_label in subject_objects:
            kt = KeywordToken(subject, store=store, namespace=namespace)
            if kt.isComposite():
                composite_count += 1
                composite_keywords[kt.short_id] = kt
            else:
                single_keywords[kt.short_id] = kt
                single_count += 1

    cached_data = {}
    cached_data[""single""] = single_keywords
    cached_data[""composite""] = composite_keywords
    cached_data[""creation_time""] = time.gmtime()
    cached_data[""version_info""] = {'rdflib': rdflib.__version__,
                                   'bibclassify': bconfig.VERSION}
    log.debug(""Building taxonomy... %d terms built in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    log.info(""Total count of single keywords: %d ""
             % len(single_keywords))
    log.info(""Total count of composite keywords: %d ""
             % len(composite_keywords))

    if not skip_cache:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        log.debug(""Writing the cache into: %s"" % cache_path)
        # test again, it could have changed
        if os.access(cache_dir, os.R_OK):
            if os.access(cache_dir, os.W_OK):
                # Serialize.
                filestream = None
                try:
                    filestream = open(cache_path, ""wb"")
                except IOError as msg:
                    # Impossible to write the cache.
                    log.error(""Impossible to write cache to '%s'.""
                              % cache_path)
                    log.error(msg)
                else:
                    log.debug(""Writing cache to file %s"" % cache_path)
                    cPickle.dump(cached_data, filestream, 1)
                if filestream:
                    filestream.close()

            else:
                raise TaxonomyError(""Cache directory exists but is not ""
                                    ""writable. Check your permissions ""
                                    ""for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    # now when the whole taxonomy was parsed,
    # find sub-components of the composite kws
    # it is important to keep this call after the taxonomy was saved,
    # because we don't  want to pickle regexes multiple times
    # (as they are must be re-compiled at load time)
    for kt in composite_keywords.values():
        kt.refreshCompositeOf(single_keywords, composite_keywords,
                              store=store, namespace=namespace)

    # house-cleaning
    if store:
        store.close()

    return (single_keywords, composite_keywords)


def _capitalize_first_letter(word):
    """"""Return a regex pattern with the first letter.

    Accepts both lowercase and uppercase.
    """"""
    if word[0].isalpha():
        # These two cases are necessary in order to get a regex pattern
        # starting with '[xX]' and not '[Xx]'. This allows to check for
        # colliding regex afterwards.
        if word[0].isupper():
            return ""["" + word[0].swapcase() + word[0] + ""]"" + word[1:]
        else:
            return ""["" + word[0] + word[0].swapcase() + ""]"" + word[1:]
    return word


def _convert_punctuation(punctuation, conversion_table):
    """"""Return a regular expression for a punctuation string.""""""
    if punctuation in conversion_table:
        return conversion_table[punctuation]
    return re.escape(punctuation)


def _convert_word(word):
    """"""Return the plural form of the word if it exists.

    Otherwise return the word itself.
    """"""
    out = None

    # Acronyms.
    if word.isupper():
        out = word + ""s?""
    # Proper nouns or word with digits.
    elif word.istitle():
        out = word + ""('?s)?""
    elif _contains_digit.search(word):
        out = word

    if out is not None:
        return out

    # Words with non or anti prefixes.
    if _starts_with_non.search(word):
        word = ""non-?"" + _capitalize_first_letter(_convert_word(word[3:]))
    elif _starts_with_anti.search(word):
        word = ""anti-?"" + _capitalize_first_letter(_convert_word(word[4:]))

    if out is not None:
        return _capitalize_first_letter(out)

    # A few invariable words.
    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:
        return _capitalize_first_letter(word)

    # Some exceptions that would not produce good results with the set of
    # general_regular_expressions.
    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS
    if word in regexes:
        return _capitalize_first_letter(regexes[word])

    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS
    for regex in regexes:
        if regex.search(word) is not None:
            return _capitalize_first_letter(word)

    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS
    for regex, replacement in regexes:
        stemmed = regex.sub(replacement, word)
        if stemmed != word:
            return _capitalize_first_letter(stemmed)

    return _capitalize_first_letter(word + ""s?"")


def _get_cache(cache_file, source_file=None):
    """"""Get cached taxonomy using the cPickle module.

    No check is done at that stage.

    :param cache_file: full path to the file holding pickled data
    :param source_file: if we discover the cache is obsolete, we
        will build a new cache, therefore we need the source path
        of the cache
    :return: (single_keywords, composite_keywords).
    """"""
    timer_start = time.clock()

    filestream = open(cache_file, ""rb"")
    try:
        cached_data = cPickle.load(filestream)
        version_info = cached_data['version_info']
        if version_info['rdflib'] != rdflib.__version__\
                or version_info['bibclassify'] != bconfig.VERSION:
            raise KeyError
    except (cPickle.UnpicklingError, ImportError,
            AttributeError, DeprecationWarning, EOFError):
        log.warning(""The existing cache in %s is not readable. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        return _build_cache(source_file)
    except KeyError:
        log.warning(""The existing cache %s is not up-to-date. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        if source_file and os.path.exists(source_file):
            return _build_cache(source_file)
        else:
            log.error(""The cache contains obsolete data (and it was deleted), ""
                      ""however I can't build a new cache, the source does not ""
                      ""exist or is inaccessible! - %s"" % source_file)
    filestream.close()

    single_keywords = cached_data[""single""]
    composite_keywords = cached_data[""composite""]

    # the cache contains only keys of the composite keywords, not the objects
    # so now let's resolve them into objects
    for kw in composite_keywords.values():
        kw.refreshCompositeOf(single_keywords, composite_keywords)

    log.debug(""Retrieved taxonomy from cache %s created on %s"" %
              (cache_file, time.asctime(cached_data[""creation_time""])))

    log.debug(""%d terms read in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    return (single_keywords, composite_keywords)


def _get_cache_path(source_file):
    """"""Return the path where the cache should be written/located.

    :param onto_name: name of the ontology or the full path
    :return: string, abs path to the cache file in the tmpdir/bibclassify
    """"""
    local_name = os.path.basename(source_file)
    cache_name = local_name + "".db""
    cache_dir = os.path.join(config.CFG_CACHEDIR, ""bibclassify"")

    if not os.path.isdir(cache_dir):
        os.makedirs(cache_dir)

    return os.path.abspath(os.path.join(cache_dir, cache_name))


def _get_last_modification_date(url):
    """"""Get the last modification date of the ontology.""""""
    request = urllib2.Request(url)
    request.get_method = lambda: ""HEAD""
    http_file = urlopen(request)
    date_string = http_file.headers[""last-modified""]
    parsed = time.strptime(date_string, ""%a, %d %b %Y %H:%M:%S %Z"")
    return datetime(*(parsed)[0:6])


def _download_ontology(url, local_file):
    """"""Download the ontology and stores it in CFG_CACHEDIR.""""""
    log.debug(""Copying remote ontology '%s' to file '%s'."" % (url,
                                                              local_file))
    try:
        url_desc = urlopen(url)
        file_desc = open(local_file, 'w')
        file_desc.write(url_desc.read())
        file_desc.close()
    except IOError as e:
        print(e)
        return False
    except:
        log.warning(""Unable to download the ontology. '%s'"" %
                    sys.exc_info()[0])
        return False
    else:
        log.debug(""Done copying."")
        return True


def _get_searchable_regex(basic=None, hidden=None):
    """"""Return the searchable regular expressions for the single keyword.""""""
    # Hidden labels are used to store regular expressions.
    basic = basic or []
    hidden = hidden or []

    hidden_regex_dict = {}
    for hidden_label in hidden:
        if _is_regex(hidden_label):
            hidden_regex_dict[hidden_label] = \
                re.compile(
                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]
                )
        else:
            pattern = _get_regex_pattern(hidden_label)
            hidden_regex_dict[hidden_label] = re.compile(
                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
            )

    # We check if the basic label (preferred or alternative) is matched
    # by a hidden label regex. If yes, discard it.
    regex_dict = {}
    # Create regex for plural forms and add them to the hidden labels.
    for label in basic:
        pattern = _get_regex_pattern(label)
        regex_dict[label] = re.compile(
            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
        )

    # Merge both dictionaries.
    regex_dict.update(hidden_regex_dict)

    return regex_dict.values()


def _get_regex_pattern(label):
    """"""Return a regular expression of the label.

    This takes care of plural and different kinds of separators.
    """"""
    parts = _split_by_punctuation.split(label)

    for index, part in enumerate(parts):
        if index % 2 == 0:
            # Word
            if not parts[index].isdigit() and len(parts[index]) > 1:
                parts[index] = _convert_word(parts[index])
        else:
            # Punctuation
            if not parts[index + 1]:
                # The separator is not followed by another word. Treat
                # it as a symbol.
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SYMBOLS
                )
            else:
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SEPARATORS
                )

    return """".join(parts)


def _is_regex(string):
    """"""Check if a concept is a regular expression.""""""
    return string[0] == ""/"" and string[-1] == ""/""


def check_taxonomy(taxonomy):
    """"""Check the consistency of the taxonomy.

    Outputs a list of errors and warnings.
    """"""
    log.info(""Building graph with Python RDFLib version %s"" %
             rdflib.__version__)

    store = rdflib.ConjunctiveGraph()

    try:
        store.parse(taxonomy)
    except:
        log.error(""The taxonomy is not a valid RDF file. Are you ""
                  ""trying to check a controlled vocabulary?"")
        raise TaxonomyError('Error in RDF file')

    log.info(""Graph was successfully built."")

    prefLabel = ""prefLabel""
    hiddenLabel = ""hiddenLabel""
    altLabel = ""altLabel""
    composite = ""composite""
    compositeOf = ""compositeOf""
    note = ""note""

    both_skw_and_ckw = []

    # Build a dictionary we will reason on later.
    uniq_subjects = {}
    for subject in store.subjects():
        uniq_subjects[subject] = None

    subjects = {}
    for subject in uniq_subjects:
        strsubject = str(subject).split(""#Composite."")[-1]
        strsubject = strsubject.split(""#"")[-1]
        if (strsubject == ""http://cern.ch/thesauri/HEPontology.rdf"" or
           strsubject == ""compositeOf""):
            continue
        components = {}
        for predicate, value in store.predicate_objects(subject):
            strpredicate = str(predicate).split(""#"")[-1]
            strobject = str(value).split(""#Composite."")[-1]
            strobject = strobject.split(""#"")[-1]
            components.setdefault(strpredicate, []).append(strobject)
        if strsubject in subjects:
            both_skw_and_ckw.append(strsubject)
        else:
            subjects[strsubject] = components

    log.info(""Taxonomy contains %s concepts."" % len(subjects))

    no_prefLabel = []
    multiple_prefLabels = []
    bad_notes = []
    # Subjects with no composite or compositeOf predicate
    lonely = []
    both_composites = []
    bad_hidden_labels = {}
    bad_alt_labels = {}
    # Problems with composite keywords
    composite_problem1 = []
    composite_problem2 = []
    composite_problem3 = []
    composite_problem4 = {}
    composite_problem5 = []
    composite_problem6 = []

    stemming_collisions = []
    interconcept_collisions = {}

    for subject, predicates in iteritems(subjects):
        # No prefLabel or multiple prefLabels
        try:
            if len(predicates[prefLabel]) > 1:
                multiple_prefLabels.append(subject)
        except KeyError:
            no_prefLabel.append(subject)

        # Lonely and both composites.
        if composite not in predicates and compositeOf not in predicates:
            lonely.append(subject)
        elif composite in predicates and compositeOf in predicates:
            both_composites.append(subject)

        # Multiple or bad notes
        if note in predicates:
            bad_notes += [(subject, n) for n in predicates[note]
                          if n not in ('nostandalone', 'core')]

        # Bad hidden labels
        if hiddenLabel in predicates:
            for lbl in predicates[hiddenLabel]:
                if lbl.startswith(""/"") ^ lbl.endswith(""/""):
                    bad_hidden_labels.setdefault(subject, []).append(lbl)

        # Bad alt labels
        if altLabel in predicates:
            for lbl in predicates[altLabel]:
                if len(re.findall(""/"", lbl)) >= 2 or "":"" in lbl:
                    bad_alt_labels.setdefault(subject, []).append(lbl)

        # Check composite
        if composite in predicates:
            for ckw in predicates[composite]:
                if ckw in subjects:
                    if compositeOf in subjects[ckw]:
                        if subject not in subjects[ckw][compositeOf]:
                            composite_problem3.append((subject, ckw))
                    else:
                        if ckw not in both_skw_and_ckw:
                            composite_problem2.append((subject, ckw))
                else:
                    composite_problem1.append((subject, ckw))

        # Check compositeOf
        if compositeOf in predicates:
            for skw in predicates[compositeOf]:
                if skw in subjects:
                    if composite in subjects[skw]:
                        if subject not in subjects[skw][composite]:
                            composite_problem6.append((subject, skw))
                    else:
                        if skw not in both_skw_and_ckw:
                            composite_problem5.append((subject, skw))
                else:
                    composite_problem4.setdefault(skw, []).append(subject)

        # Check for stemmed labels
        if compositeOf in predicates:
            labels = (altLabel, hiddenLabel)
        else:
            labels = (prefLabel, altLabel, hiddenLabel)

        patterns = {}
        for label in [lbl for lbl in labels if lbl in predicates]:
            for expression in [expr for expr in predicates[label]
                               if not _is_regex(expr)]:
                pattern = _get_regex_pattern(expression)
                interconcept_collisions.setdefault(pattern, []).\
                    append((subject, label))
                if pattern in patterns:
                    stemming_collisions.append(
                        (subject,
                         patterns[pattern],
                         (label, expression)
                         )
                    )
                else:
                    patterns[pattern] = (label, expression)

    print(""\n==== ERRORS ===="")

    if no_prefLabel:
        print(""\nConcepts with no prefLabel: %d"" % len(no_prefLabel))
        print(""\n"".join([""   %s"" % subj for subj in no_prefLabel]))
    if multiple_prefLabels:
        print((""\nConcepts with multiple prefLabels: %d"" %
               len(multiple_prefLabels)))
        print(""\n"".join([""   %s"" % subj for subj in multiple_prefLabels]))
    if both_composites:
        print((""\nConcepts with both composite properties: %d"" %
               len(both_composites)))
        print(""\n"".join([""   %s"" % subj for subj in both_composites]))
    if bad_hidden_labels:
        print(""\nConcepts with bad hidden labels: %d"" % len(bad_hidden_labels))
        for kw, lbls in iteritems(bad_hidden_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if bad_alt_labels:
        print(""\nConcepts with bad alt labels: %d"" % len(bad_alt_labels))
        for kw, lbls in iteritems(bad_alt_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if both_skw_and_ckw:
        print((""\nKeywords that are both skw and ckw: %d"" %
               len(both_skw_and_ckw)))
        print(""\n"".join([""   %s"" % subj for subj in both_skw_and_ckw]))

    print()

    if composite_problem1:
        print(""\n"".join([""SKW '%s' references an unexisting CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem1]))
    if composite_problem2:
        print(""\n"".join([""SKW '%s' references a SKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem2]))
    if composite_problem3:
        print(""\n"".join([""SKW '%s' is not composite of CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem3]))
    if composite_problem4:
        for skw, ckws in iteritems(composite_problem4):
            print(""SKW '%s' does not exist but is "" ""referenced by:"" % skw)
            print(""\n"".join([""    %s"" % ckw for ckw in ckws]))
    if composite_problem5:
        print(""\n"".join([""CKW '%s' references a CKW '%s'."" % kw
                         for kw in composite_problem5]))
    if composite_problem6:
        print(""\n"".join([""CKW '%s' is not composed by SKW '%s'."" % kw
                         for kw in composite_problem6]))

    print(""\n==== WARNINGS ===="")

    if bad_notes:
        print((""\nConcepts with bad notes: %d"" % len(bad_notes)))
        print(""\n"".join([""   '%s': '%s'"" % _note for _note in bad_notes]))
    if stemming_collisions:
        print(""\nFollowing keywords have unnecessary labels that have ""
              ""already been generated by BibClassify."")
        for subj in stemming_collisions:
            print(""   %s:\n     %s\n     and %s"" % subj)

    print(""\nFinished."")
    sys.exit(0)


def test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):
    """"""Test the cache lookup.""""""
    cache = get_cache(taxonomy_name)
    if not cache:
        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,
                                                         rebuild=rebuild_cache,
                                                         no_cache=no_cache))
        cache = get_cache(taxonomy_name)
    return (thread.get_ident(), cache)


log.info('Loaded ontology reader')

if __name__ == '__main__':
    test_cache()
/n/n/ninvenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re

from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Check if a document is a PDF file and returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[-1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    # os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Return the fulltext of the local file.

    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""
    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    # Discard lines that do not contain at least one word.
    return [line for line in lines if _ONE_WORD.search(line) is not None]


def executable_exists(executable):
    """"""Test if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False
/n/n/n",0
9,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"/invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,
                                                       user_agent=make_user_agent_string(""BibClassify""))
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Outputs keywords reading a local file. Arguments and output are the same
    as for :see: get_keywords_from_text() """"""

    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/n/invenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""
BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re
import tempfile
import urllib2
from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Checks if a document is a PDF file. Returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported. Please report this to cds.support@cern.ch."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    #os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Returns the fulltext of the local file.
    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""

    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    if not _is_english_text('\n'.join(lines)):
        log.warning(""It seems the file '%s' is unvalid and doesn't ""
                    ""contain text. Please communicate this file to the Invenio ""
                    ""team."" % document)

    line_nb = len(lines)
    word_nb = 0
    for line in lines:
        word_nb += len(re.findall(""\S+"", line))

    # Discard lines that do not contain at least one word.
    lines = [line for line in lines if _ONE_WORD.search(line) is not None]

    if not remote:
        log.info(""Local file has %d lines and %d words."" % (line_nb, word_nb))

    return lines


def _is_english_text(text):
    """"""
    Checks if a text is correct english.
    Computes the number of words in the text and compares it to the
    expected number of words (based on an average size of words of 5.1
    letters).

    @param text_lines: the text to analyze
    @type text_lines:  string
    @return:           True if the text is English, False otherwise
    @rtype:            Boolean
    """"""
    # Consider one word and one space.
    avg_word_length = 2.55 + 1
    expected_word_number = float(len(text)) / avg_word_length

    words = [word
             for word in re.split('\W', text)
             if word.isalpha()]

    word_number = len(words)

    return word_number > expected_word_number


def text_lines_from_url(url, user_agent=""""):
    """"""Returns the fulltext of the file found at the URL.""""""
    request = urllib2.Request(url)
    if user_agent:
        request.add_header(""User-Agent"", user_agent)
    try:
        distant_stream = urlopen(request)
        # Write the URL content to a temporary file.
        local_file = tempfile.mkstemp(prefix=""bibclassify."")[1]
        local_stream = open(local_file, ""w"")
        local_stream.write(distant_stream.read())
        local_stream.close()
    except:
        log.error(""Unable to read from URL %s."" % url)
        return None
    else:
        # Read lines from the temporary file.
        lines = text_lines_from_local_file(local_file, remote=True)
        os.remove(local_file)

        line_nb = len(lines)
        word_nb = 0
        for line in lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))

        return lines


def executable_exists(executable):
    """"""Tests if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False


/n/n/n",1
10,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
import re
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.text import encode_for_xml
from invenio.utils.filedownload import download_url

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""
    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        line_nb = len(text_lines)
        word_nb = 0
        for line in text_lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))
        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            local_file = download_url(entry)
            text_lines = extractor.text_lines_from_local_file(local_file)
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Output keywords reading a local file.

    Arguments and output are the same as for :see: get_keywords_from_text().
    """"""
    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings.

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""
    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext.

    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/ninvenio/legacy/bibclassify/ontology_reader.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify ontology reader.

The ontology reader reads currently either a RDF/SKOS taxonomy or a
simple controlled vocabulary file (1 word per line). The first role of
this module is to manage the cached version of the ontology file. The
second role is to hold all methods responsible for the creation of
regular expressions. These methods are grammatically related as we take
care of different forms of the same words.  The grammatical rules can be
configured via the configuration file.

The main method from this module is get_regular_expressions.
""""""

from __future__ import print_function

from datetime import datetime, timedelta
from six import iteritems
from six.moves import cPickle

import os
import re
import sys
import tempfile
import time
import urllib2
import traceback
import xml.sax
import thread
import rdflib

from invenio.legacy.bibclassify import config as bconfig
from invenio.modules.classifier.errors import TaxonomyError

log = bconfig.get_logger(""bibclassify.ontology_reader"")
from invenio import config

from invenio.modules.classifier.registry import taxonomies

# only if not running in a stanalone mode
if bconfig.STANDALONE:
    dbquery = None
    from urllib2 import urlopen
else:
    from invenio.legacy import dbquery
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

_contains_digit = re.compile(""\d"")
_starts_with_non = re.compile(""(?i)^non[a-z]"")
_starts_with_anti = re.compile(""(?i)^anti[a-z]"")
_split_by_punctuation = re.compile(""(\W+)"")

_CACHE = {}


def get_cache(taxonomy_id):
    """"""Return thread-safe cache for the given taxonomy id.

    :param taxonomy_id: identifier of the taxonomy
    :type taxonomy_id: str

    :return: dictionary object (empty if no taxonomy_id
        is found), you must not change anything inside it.
        Create a new dictionary and use set_cache if you want
        to update the cache!
    """"""
    # Because of a standalone mode, we don't use the
    # invenio.data_cacher.DataCacher, but it has no effect
    # on proper functionality.

    if taxonomy_id in _CACHE:
        ctime, taxonomy = _CACHE[taxonomy_id]

        # check it is fresh version
        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)
        cache_path = _get_cache_path(onto_name)

        # if source exists and is newer than the cache hold in memory
        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:
            log.info('Forcing taxonomy rebuild as cached'
                     ' version is newer/updated.')
            return {}  # force cache rebuild

        # if cache exists and is newer than the cache hold in memory
        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:
            log.info('Forcing taxonomy rebuild as source'
                     ' file is newer/updated.')
            return {}
        log.info('Taxonomy retrieved from cache')
        return taxonomy
    return {}


def set_cache(taxonomy_id, contents):
    """"""Update cache in a thread-safe manner.""""""
    lock = thread.allocate_lock()
    lock.acquire()
    try:
        _CACHE[taxonomy_id] = (time.time(), contents)
    finally:
        lock.release()


def get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):
    """"""Return a list of patterns compiled from the RDF/SKOS ontology.

    Uses cache if it exists and if the taxonomy hasn't changed.
    """"""
    # Translate the ontology name into a local path. Check if the name
    # relates to an existing ontology.
    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)
    if not onto_path:
        raise TaxonomyError(""Unable to locate the taxonomy: '%s'.""
                            % taxonomy_name)

    cache_path = _get_cache_path(onto_name)
    log.debug('Taxonomy discovered, now we load it '
              '(from cache: %s, onto_path: %s, cache_path: %s)'
              % (not no_cache, onto_path, cache_path))

    if os.access(cache_path, os.R_OK):
        if os.access(onto_path, os.R_OK):
            if rebuild or no_cache:
                log.debug(""Cache generation was manually forced."")
                return _build_cache(onto_path, skip_cache=no_cache)
        else:
            # ontology file not found. Use the cache instead.
            log.warning(""The ontology couldn't be located. However ""
                        ""a cached version of it is available. Using it as a ""
                        ""reference."")
            return _get_cache(cache_path, source_file=onto_path)

        if (os.path.getmtime(cache_path) >
                os.path.getmtime(onto_path)):
            # Cache is more recent than the ontology: use cache.
            log.debug(""Normal situation, cache is older than ontology,""
                      "" so we load it from cache"")
            return _get_cache(cache_path, source_file=onto_path)
        else:
            # Ontology is more recent than the cache: rebuild cache.
            log.warning(""Cache '%s' is older than '%s'. ""
                        ""We will rebuild the cache"" %
                        (cache_path, onto_path))
            return _build_cache(onto_path, skip_cache=no_cache)

    elif os.access(onto_path, os.R_OK):
        if not no_cache and\
                os.path.exists(cache_path) and\
                not os.access(cache_path, os.W_OK):
            raise TaxonomyError('We cannot read/write into: %s. '
                                'Aborting!' % cache_path)
        elif not no_cache and os.path.exists(cache_path):
            log.warning('Cache %s exists, but is not readable!' % cache_path)
        log.info(""Cache not available. Building it now: %s"" % onto_path)
        return _build_cache(onto_path, skip_cache=no_cache)

    else:
        raise TaxonomyError(""We miss both source and cache""
                            "" of the taxonomy: %s"" % taxonomy_name)


def _get_remote_ontology(onto_url, time_difference=None):
    """"""Check if the online ontology is more recent than the local ontology.

    If yes, try to download and store it in Invenio's cache directory.

    Return a boolean describing the success of the operation.

    :return: path to the downloaded ontology.
    """"""
    if onto_url is None:
        return False

    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +
              ""bibclassify"" + os.sep)
    if not os.path.exists(dl_dir):
        os.mkdir(dl_dir)

    local_file = dl_dir + os.path.basename(onto_url)
    remote_modif_time = _get_last_modification_date(onto_url)
    try:
        local_modif_seconds = os.path.getmtime(local_file)
    except OSError:
        # The local file does not exist. Download the ontology.
        download = True
        log.info(""The local ontology could not be found."")
    else:
        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])
        # Let's set a time delta of 1 hour and 10 minutes.
        time_difference = time_difference or timedelta(hours=1, minutes=10)
        download = remote_modif_time > local_modif_time + time_difference
        if download:
            log.info(""The remote ontology '%s' is more recent ""
                     ""than the local ontology."" % onto_url)

    if download:
        if not _download_ontology(onto_url, local_file):
            log.warning(""Error downloading the ontology from: %s"" % onto_url)

    return local_file


def _get_ontology(ontology):
    """"""Return the (name, path, url) to the short ontology name.

    :param ontology: name of the ontology or path to the file or url.
    """"""
    onto_name = onto_path = onto_url = None

    # first assume we got the path to the file
    if os.path.exists(ontology):
        onto_name = os.path.split(os.path.abspath(ontology))[1]
        onto_path = os.path.abspath(ontology)
        onto_url = """"
    else:
        # if not, try to find it in a known locations
        discovered_file = _discover_ontology(ontology)
        if discovered_file:
            onto_name = os.path.split(discovered_file)[1]
            onto_path = discovered_file
            # i know, this sucks
            x = ontology.lower()
            if ""http:"" in x or ""https:"" in x or ""ftp:"" in x or ""file:"" in x:
                onto_url = ontology
            else:
                onto_url = """"
        else:
            # not found, look into a database
            # (it is last because when bibclassify
            # runs in a standalone mode,
            # it has no database - [rca, old-heritage]
            if not bconfig.STANDALONE:
                result = dbquery.run_sql(""SELECT name, location from clsMETHOD WHERE name LIKE %s"",
                                         ('%' + ontology + '%',))
                for onto_short_name, url in result:
                    onto_name = onto_short_name
                    onto_path = _get_remote_ontology(url)
                    onto_url = url

    return (onto_name, onto_path, onto_url)


def _discover_ontology(ontology_name):
    """"""Look for the file in a known places.

    Inside invenio/etc/bibclassify and a few other places
    like current directory.

    :param ontology: name or path name or url
    :type ontology: str

    :return: absolute path of a file if found, or None
    """"""
    last_part = os.path.split(os.path.abspath(ontology_name))[1]
    if last_part in taxonomies:
        return taxonomies.get(last_part)
    elif last_part + "".rdf"" in taxonomies:
        return taxonomies.get(last_part + "".rdf"")
    else:
        log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)

    # LEGACY
    possible_patterns = [last_part, last_part.lower()]
    if not last_part.endswith('.rdf'):
        possible_patterns.append(last_part + '.rdf')
    places = [config.CFG_CACHEDIR,
              config.CFG_ETCDIR,
              os.path.join(config.CFG_CACHEDIR, ""bibclassify""),
              os.path.join(config.CFG_ETCDIR, ""bibclassify""),
              os.path.abspath('.'),
              os.path.abspath(os.path.join(os.path.dirname(__file__),
                                           ""../../../etc/bibclassify"")),
              os.path.join(os.path.dirname(__file__), ""bibclassify""),
              config.CFG_WEBDIR]

    log.debug(""Searching for taxonomy using string: %s"" % last_part)
    log.debug(""Possible patterns: %s"" % possible_patterns)
    for path in places:

        try:
            if os.path.isdir(path):
                log.debug(""Listing: %s"" % path)
                for filename in os.listdir(path):
                    #log.debug('Testing: %s' % filename)
                    for pattern in possible_patterns:
                        filename_lc = filename.lower()
                        if pattern == filename_lc and\
                                os.path.exists(os.path.join(path, filename)):
                            filepath = os.path.abspath(os.path.join(path,
                                                                    filename))
                            if (os.access(filepath, os.R_OK)):
                                log.debug(""Found taxonomy at: %s"" % filepath)
                                return filepath
                            else:
                                log.warning('Found taxonony at: %s, but it is'
                                            ' not readable. '
                                            'Continue searching...'
                                            % filepath)
        except OSError, os_error_msg:
            log.warning('OS Error when listing path '
                        '""%s"": %s' % (str(path), str(os_error_msg)))
    log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)


class KeywordToken:

    """"""KeywordToken is a class used for the extracted keywords.

    It can be initialized with values from RDF store or from
    simple strings. Specialty of this class is that objects are
    hashable by subject - so in the dictionary two objects with the
    same subject appears as one -- :see: self.__hash__ and self.__cmp__.
    """"""

    def __init__(self, subject, store=None, namespace=None, type='HEP'):
        """"""Initialize KeywordToken with a subject.

        :param subject: string or RDF object
        :param store: RDF graph object
                      (will be used to get info about the subject)
        :param namespace: RDF namespace object, used together with store
        :param type: type of this keyword.
        """"""
        self.id = subject
        self.type = type
        self.short_id = subject
        self.concept = """"
        self.regex = []
        self.nostandalone = False
        self.spires = False
        self.fieldcodes = []
        self.compositeof = []
        self.core = False
        # True means composite keyword
        self._composite = '#Composite' in subject
        self.__hash = None

        # the tokens are coming possibly from a normal text file
        if store is None:
            subject = subject.strip()
            self.concept = subject
            self.regex = _get_searchable_regex(basic=[subject])
            self.nostandalone = False
            self.fieldcodes = []
            self.core = False
            if subject.find(' ') > -1:
                self._composite = True

        # definitions from rdf
        else:
            self.short_id = self.short_id.split('#')[-1]

            # find alternate names for this label
            basic_labels = []

            # turn those patterns into regexes only for simple keywords
            if self._composite is False:
                try:
                    for label in store.objects(subject,
                                               namespace[""prefLabel""]):
                        # XXX shall i make it unicode?
                        basic_labels.append(str(label))
                except TypeError:
                    pass
                self.concept = basic_labels[0]
            else:
                try:
                    self.concept = str(store.value(subject,
                                                   namespace[""prefLabel""],
                                                   any=True))
                except KeyError:
                    log.warning(""Keyword with subject %s has no prefLabel.""
                                "" We use raw name"" %
                                self.short_id)
                    self.concept = self.short_id

            # this is common both to composite and simple keywords
            try:
                for label in store.objects(subject, namespace[""altLabel""]):
                    basic_labels.append(str(label))
            except TypeError:
                pass

            # hidden labels are special (possibly regex) codes
            hidden_labels = []
            try:
                for label in store.objects(subject, namespace[""hiddenLabel""]):
                    hidden_labels.append(unicode(label))
            except TypeError:
                pass

            # compile regular expression that will identify this token
            self.regex = _get_searchable_regex(basic_labels, hidden_labels)

            try:
                for note in map(lambda s: str(s).lower().strip(),
                                store.objects(subject, namespace[""note""])):
                    if note == 'core':
                        self.core = True
                    elif note in (""nostandalone"", ""nonstandalone""):
                        self.nostandalone = True
                    elif 'fc:' in note:
                        self.fieldcodes.append(note[3:].strip())
            except TypeError:
                pass

            # spiresLabel does not have multiple values
            spires_label = store.value(subject, namespace[""spiresLabel""])
            if spires_label:
                self.spires = str(spires_label)

        # important for comparisons
        self.__hash = hash(self.short_id)

        # extract composite parts ids
        if store is not None and self.isComposite():
            small_subject = self.id.split(""#Composite."")[-1]
            component_positions = []
            for label in store.objects(self.id, namespace[""compositeOf""]):
                strlabel = str(label).split(""#"")[-1]
                component_name = label.split(""#"")[-1]
                component_positions.append((small_subject.find(component_name),
                                            strlabel))
            component_positions.sort()
            if not component_positions:
                log.error(""Keyword is marked as composite, ""
                          ""but no composite components refs found: %s""
                          % self.short_id)
            else:
                self.compositeof = map(lambda x: x[1], component_positions)

    def refreshCompositeOf(self, single_keywords, composite_keywords,
                           store=None, namespace=None):
        """"""Re-check sub-parts of this keyword.

        This should be called after the whole RDF was processed, because
        it is using a cache of single keywords and if that
        one is incomplete, you will not identify all parts.
        """"""
        def _get_ckw_components(new_vals, label):
            if label in single_keywords:
                new_vals.append(single_keywords[label])
            elif ('Composite.%s' % label) in composite_keywords:
                for l in composite_keywords['Composite.%s' % label].compositeof:
                    _get_ckw_components(new_vals, l)
            elif label in composite_keywords:
                for l in composite_keywords[label].compositeof:
                    _get_ckw_components(new_vals, l)
            else:
                # One single or composite keyword is missing from the taxonomy.
                # This is due to an error in the taxonomy description.
                message = ""The composite term \""%s\""""\
                          "" should be made of single keywords,""\
                          "" but at least one is missing."" % self.id
                if store is not None:
                    message += ""Needed components: %s""\
                               % list(store.objects(self.id,
                                      namespace[""compositeOf""]))
                message += "" Missing is: %s"" % label
                raise TaxonomyError(message)

        if self.compositeof:
            new_vals = []
            try:
                for label in self.compositeof:
                    _get_ckw_components(new_vals, label)
                self.compositeof = new_vals
            except TaxonomyError as err:
                # the composites will be empty
                # (better than to have confusing, partial matches)
                self.compositeof = []
                log.error(err)

    def isComposite(self):
        """"""Return value of _composite.""""""
        return self._composite

    def getComponents(self):
        """"""Return value of compositeof.""""""
        return self.compositeof

    def getType(self):
        """"""Return value of type.""""""
        return self.type

    def setType(self, value):
        """"""Set value of value.""""""
        self.type = value

    def __hash__(self):
        """"""Return _hash.

        This might change in the future but for the moment we want to
        think that if the concept is the same, then it is the same
        keyword - this sucks, but it is sort of how it is necessary
        to use now.
        """"""
        return self.__hash

    def __cmp__(self, other):
        """"""Compare objects using _hash.""""""
        if self.__hash < other.__hash__():
            return -1
        elif self.__hash == other.__hash__():
            return 0
        else:
            return 1

    def __str__(self, spires=False):
        """"""Return the best output for the keyword.""""""
        if spires:
            if self.spires:
                return self.spires
            elif self._composite:
                return self.concept.replace(':', ',')
            # default action
        return self.concept

    def output(self, spires=False):
        """"""Return string representation with spires value.""""""
        return self.__str__(spires=spires)

    def __repr__(self):
        """"""Class representation.""""""
        return ""<KeywordToken: %s>"" % self.short_id


def _build_cache(source_file, skip_cache=False):
    """"""Build the cached data.

    Either by parsing the RDF taxonomy file or a vocabulary file.

    :param source_file: source file of the taxonomy, RDF file
    :param skip_cache: if True, build cache will not be
        saved (pickled) - it is saved as <source_file.db>
    """"""
    store = rdflib.ConjunctiveGraph()

    if skip_cache:
        log.info(""You requested not to save the cache to disk."")
    else:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        # Make sure we have a cache_dir readable and writable.
        try:
            os.makedirs(cache_dir)
        except:
            pass
        if os.access(cache_dir, os.R_OK):
            if not os.access(cache_dir, os.W_OK):
                raise TaxonomyError(""Cache directory exists but is not""
                                    "" writable. Check your permissions""
                                    "" for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    timer_start = time.clock()

    namespace = None
    single_keywords, composite_keywords = {}, {}

    try:
        log.info(""Building RDFLib's conjunctive graph from: %s"" % source_file)
        try:
            store.parse(source_file)
        except urllib2.URLError:
            if source_file[0] == '/':
                store.parse(""file://"" + source_file)
            else:
                store.parse(""file:///"" + source_file)

    except rdflib.exceptions.Error as e:
        log.error(""Serious error reading RDF file"")
        log.error(e)
        log.error(traceback.format_exc())
        raise rdflib.exceptions.Error(e)

    except (xml.sax.SAXParseException, ImportError) as e:
        # File is not a RDF file. We assume it is a controlled vocabulary.
        log.error(e)
        log.warning(""The ontology file is probably not a valid RDF file. \
            Assuming it is a controlled vocabulary file."")

        filestream = open(source_file, ""r"")
        for line in filestream:
            keyword = line.strip()
            kt = KeywordToken(keyword)
            single_keywords[kt.short_id] = kt
        if not len(single_keywords):
            raise TaxonomyError('The ontology file is not well formated')

    else:  # ok, no exception happened
        log.info(""Now building cache of keywords"")
        # File is a RDF file.
        namespace = rdflib.Namespace(""http://www.w3.org/2004/02/skos/core#"")

        single_count = 0
        composite_count = 0

        subject_objects = store.subject_objects(namespace[""prefLabel""])
        for subject, pref_label in subject_objects:
            kt = KeywordToken(subject, store=store, namespace=namespace)
            if kt.isComposite():
                composite_count += 1
                composite_keywords[kt.short_id] = kt
            else:
                single_keywords[kt.short_id] = kt
                single_count += 1

    cached_data = {}
    cached_data[""single""] = single_keywords
    cached_data[""composite""] = composite_keywords
    cached_data[""creation_time""] = time.gmtime()
    cached_data[""version_info""] = {'rdflib': rdflib.__version__,
                                   'bibclassify': bconfig.VERSION}
    log.debug(""Building taxonomy... %d terms built in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    log.info(""Total count of single keywords: %d ""
             % len(single_keywords))
    log.info(""Total count of composite keywords: %d ""
             % len(composite_keywords))

    if not skip_cache:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        log.debug(""Writing the cache into: %s"" % cache_path)
        # test again, it could have changed
        if os.access(cache_dir, os.R_OK):
            if os.access(cache_dir, os.W_OK):
                # Serialize.
                filestream = None
                try:
                    filestream = open(cache_path, ""wb"")
                except IOError as msg:
                    # Impossible to write the cache.
                    log.error(""Impossible to write cache to '%s'.""
                              % cache_path)
                    log.error(msg)
                else:
                    log.debug(""Writing cache to file %s"" % cache_path)
                    cPickle.dump(cached_data, filestream, 1)
                if filestream:
                    filestream.close()

            else:
                raise TaxonomyError(""Cache directory exists but is not ""
                                    ""writable. Check your permissions ""
                                    ""for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    # now when the whole taxonomy was parsed,
    # find sub-components of the composite kws
    # it is important to keep this call after the taxonomy was saved,
    # because we don't  want to pickle regexes multiple times
    # (as they are must be re-compiled at load time)
    for kt in composite_keywords.values():
        kt.refreshCompositeOf(single_keywords, composite_keywords,
                              store=store, namespace=namespace)

    # house-cleaning
    if store:
        store.close()

    return (single_keywords, composite_keywords)


def _capitalize_first_letter(word):
    """"""Return a regex pattern with the first letter.

    Accepts both lowercase and uppercase.
    """"""
    if word[0].isalpha():
        # These two cases are necessary in order to get a regex pattern
        # starting with '[xX]' and not '[Xx]'. This allows to check for
        # colliding regex afterwards.
        if word[0].isupper():
            return ""["" + word[0].swapcase() + word[0] + ""]"" + word[1:]
        else:
            return ""["" + word[0] + word[0].swapcase() + ""]"" + word[1:]
    return word


def _convert_punctuation(punctuation, conversion_table):
    """"""Return a regular expression for a punctuation string.""""""
    if punctuation in conversion_table:
        return conversion_table[punctuation]
    return re.escape(punctuation)


def _convert_word(word):
    """"""Return the plural form of the word if it exists.

    Otherwise return the word itself.
    """"""
    out = None

    # Acronyms.
    if word.isupper():
        out = word + ""s?""
    # Proper nouns or word with digits.
    elif word.istitle():
        out = word + ""('?s)?""
    elif _contains_digit.search(word):
        out = word

    if out is not None:
        return out

    # Words with non or anti prefixes.
    if _starts_with_non.search(word):
        word = ""non-?"" + _capitalize_first_letter(_convert_word(word[3:]))
    elif _starts_with_anti.search(word):
        word = ""anti-?"" + _capitalize_first_letter(_convert_word(word[4:]))

    if out is not None:
        return _capitalize_first_letter(out)

    # A few invariable words.
    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:
        return _capitalize_first_letter(word)

    # Some exceptions that would not produce good results with the set of
    # general_regular_expressions.
    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS
    if word in regexes:
        return _capitalize_first_letter(regexes[word])

    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS
    for regex in regexes:
        if regex.search(word) is not None:
            return _capitalize_first_letter(word)

    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS
    for regex, replacement in regexes:
        stemmed = regex.sub(replacement, word)
        if stemmed != word:
            return _capitalize_first_letter(stemmed)

    return _capitalize_first_letter(word + ""s?"")


def _get_cache(cache_file, source_file=None):
    """"""Get cached taxonomy using the cPickle module.

    No check is done at that stage.

    :param cache_file: full path to the file holding pickled data
    :param source_file: if we discover the cache is obsolete, we
        will build a new cache, therefore we need the source path
        of the cache
    :return: (single_keywords, composite_keywords).
    """"""
    timer_start = time.clock()

    filestream = open(cache_file, ""rb"")
    try:
        cached_data = cPickle.load(filestream)
        version_info = cached_data['version_info']
        if version_info['rdflib'] != rdflib.__version__\
                or version_info['bibclassify'] != bconfig.VERSION:
            raise KeyError
    except (cPickle.UnpicklingError, ImportError,
            AttributeError, DeprecationWarning, EOFError):
        log.warning(""The existing cache in %s is not readable. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        return _build_cache(source_file)
    except KeyError:
        log.warning(""The existing cache %s is not up-to-date. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        if source_file and os.path.exists(source_file):
            return _build_cache(source_file)
        else:
            log.error(""The cache contains obsolete data (and it was deleted), ""
                      ""however I can't build a new cache, the source does not ""
                      ""exist or is inaccessible! - %s"" % source_file)
    filestream.close()

    single_keywords = cached_data[""single""]
    composite_keywords = cached_data[""composite""]

    # the cache contains only keys of the composite keywords, not the objects
    # so now let's resolve them into objects
    for kw in composite_keywords.values():
        kw.refreshCompositeOf(single_keywords, composite_keywords)

    log.debug(""Retrieved taxonomy from cache %s created on %s"" %
              (cache_file, time.asctime(cached_data[""creation_time""])))

    log.debug(""%d terms read in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    return (single_keywords, composite_keywords)


def _get_cache_path(source_file):
    """"""Return the path where the cache should be written/located.

    :param onto_name: name of the ontology or the full path
    :return: string, abs path to the cache file in the tmpdir/bibclassify
    """"""
    local_name = os.path.basename(source_file)
    cache_name = local_name + "".db""
    cache_dir = os.path.join(config.CFG_CACHEDIR, ""bibclassify"")

    if not os.path.isdir(cache_dir):
        os.makedirs(cache_dir)

    return os.path.abspath(os.path.join(cache_dir, cache_name))


def _get_last_modification_date(url):
    """"""Get the last modification date of the ontology.""""""
    request = urllib2.Request(url)
    request.get_method = lambda: ""HEAD""
    http_file = urlopen(request)
    date_string = http_file.headers[""last-modified""]
    parsed = time.strptime(date_string, ""%a, %d %b %Y %H:%M:%S %Z"")
    return datetime(*(parsed)[0:6])


def _download_ontology(url, local_file):
    """"""Download the ontology and stores it in CFG_CACHEDIR.""""""
    log.debug(""Copying remote ontology '%s' to file '%s'."" % (url,
                                                              local_file))
    try:
        url_desc = urlopen(url)
        file_desc = open(local_file, 'w')
        file_desc.write(url_desc.read())
        file_desc.close()
    except IOError as e:
        print(e)
        return False
    except:
        log.warning(""Unable to download the ontology. '%s'"" %
                    sys.exc_info()[0])
        return False
    else:
        log.debug(""Done copying."")
        return True


def _get_searchable_regex(basic=None, hidden=None):
    """"""Return the searchable regular expressions for the single keyword.""""""
    # Hidden labels are used to store regular expressions.
    basic = basic or []
    hidden = hidden or []

    hidden_regex_dict = {}
    for hidden_label in hidden:
        if _is_regex(hidden_label):
            hidden_regex_dict[hidden_label] = \
                re.compile(
                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]
                )
        else:
            pattern = _get_regex_pattern(hidden_label)
            hidden_regex_dict[hidden_label] = re.compile(
                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
            )

    # We check if the basic label (preferred or alternative) is matched
    # by a hidden label regex. If yes, discard it.
    regex_dict = {}
    # Create regex for plural forms and add them to the hidden labels.
    for label in basic:
        pattern = _get_regex_pattern(label)
        regex_dict[label] = re.compile(
            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
        )

    # Merge both dictionaries.
    regex_dict.update(hidden_regex_dict)

    return regex_dict.values()


def _get_regex_pattern(label):
    """"""Return a regular expression of the label.

    This takes care of plural and different kinds of separators.
    """"""
    parts = _split_by_punctuation.split(label)

    for index, part in enumerate(parts):
        if index % 2 == 0:
            # Word
            if not parts[index].isdigit() and len(parts[index]) > 1:
                parts[index] = _convert_word(parts[index])
        else:
            # Punctuation
            if not parts[index + 1]:
                # The separator is not followed by another word. Treat
                # it as a symbol.
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SYMBOLS
                )
            else:
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SEPARATORS
                )

    return """".join(parts)


def _is_regex(string):
    """"""Check if a concept is a regular expression.""""""
    return string[0] == ""/"" and string[-1] == ""/""


def check_taxonomy(taxonomy):
    """"""Check the consistency of the taxonomy.

    Outputs a list of errors and warnings.
    """"""
    log.info(""Building graph with Python RDFLib version %s"" %
             rdflib.__version__)

    store = rdflib.ConjunctiveGraph()

    try:
        store.parse(taxonomy)
    except:
        log.error(""The taxonomy is not a valid RDF file. Are you ""
                  ""trying to check a controlled vocabulary?"")
        raise TaxonomyError('Error in RDF file')

    log.info(""Graph was successfully built."")

    prefLabel = ""prefLabel""
    hiddenLabel = ""hiddenLabel""
    altLabel = ""altLabel""
    composite = ""composite""
    compositeOf = ""compositeOf""
    note = ""note""

    both_skw_and_ckw = []

    # Build a dictionary we will reason on later.
    uniq_subjects = {}
    for subject in store.subjects():
        uniq_subjects[subject] = None

    subjects = {}
    for subject in uniq_subjects:
        strsubject = str(subject).split(""#Composite."")[-1]
        strsubject = strsubject.split(""#"")[-1]
        if (strsubject == ""http://cern.ch/thesauri/HEPontology.rdf"" or
           strsubject == ""compositeOf""):
            continue
        components = {}
        for predicate, value in store.predicate_objects(subject):
            strpredicate = str(predicate).split(""#"")[-1]
            strobject = str(value).split(""#Composite."")[-1]
            strobject = strobject.split(""#"")[-1]
            components.setdefault(strpredicate, []).append(strobject)
        if strsubject in subjects:
            both_skw_and_ckw.append(strsubject)
        else:
            subjects[strsubject] = components

    log.info(""Taxonomy contains %s concepts."" % len(subjects))

    no_prefLabel = []
    multiple_prefLabels = []
    bad_notes = []
    # Subjects with no composite or compositeOf predicate
    lonely = []
    both_composites = []
    bad_hidden_labels = {}
    bad_alt_labels = {}
    # Problems with composite keywords
    composite_problem1 = []
    composite_problem2 = []
    composite_problem3 = []
    composite_problem4 = {}
    composite_problem5 = []
    composite_problem6 = []

    stemming_collisions = []
    interconcept_collisions = {}

    for subject, predicates in iteritems(subjects):
        # No prefLabel or multiple prefLabels
        try:
            if len(predicates[prefLabel]) > 1:
                multiple_prefLabels.append(subject)
        except KeyError:
            no_prefLabel.append(subject)

        # Lonely and both composites.
        if composite not in predicates and compositeOf not in predicates:
            lonely.append(subject)
        elif composite in predicates and compositeOf in predicates:
            both_composites.append(subject)

        # Multiple or bad notes
        if note in predicates:
            bad_notes += [(subject, n) for n in predicates[note]
                          if n not in ('nostandalone', 'core')]

        # Bad hidden labels
        if hiddenLabel in predicates:
            for lbl in predicates[hiddenLabel]:
                if lbl.startswith(""/"") ^ lbl.endswith(""/""):
                    bad_hidden_labels.setdefault(subject, []).append(lbl)

        # Bad alt labels
        if altLabel in predicates:
            for lbl in predicates[altLabel]:
                if len(re.findall(""/"", lbl)) >= 2 or "":"" in lbl:
                    bad_alt_labels.setdefault(subject, []).append(lbl)

        # Check composite
        if composite in predicates:
            for ckw in predicates[composite]:
                if ckw in subjects:
                    if compositeOf in subjects[ckw]:
                        if subject not in subjects[ckw][compositeOf]:
                            composite_problem3.append((subject, ckw))
                    else:
                        if ckw not in both_skw_and_ckw:
                            composite_problem2.append((subject, ckw))
                else:
                    composite_problem1.append((subject, ckw))

        # Check compositeOf
        if compositeOf in predicates:
            for skw in predicates[compositeOf]:
                if skw in subjects:
                    if composite in subjects[skw]:
                        if subject not in subjects[skw][composite]:
                            composite_problem6.append((subject, skw))
                    else:
                        if skw not in both_skw_and_ckw:
                            composite_problem5.append((subject, skw))
                else:
                    composite_problem4.setdefault(skw, []).append(subject)

        # Check for stemmed labels
        if compositeOf in predicates:
            labels = (altLabel, hiddenLabel)
        else:
            labels = (prefLabel, altLabel, hiddenLabel)

        patterns = {}
        for label in [lbl for lbl in labels if lbl in predicates]:
            for expression in [expr for expr in predicates[label]
                               if not _is_regex(expr)]:
                pattern = _get_regex_pattern(expression)
                interconcept_collisions.setdefault(pattern, []).\
                    append((subject, label))
                if pattern in patterns:
                    stemming_collisions.append(
                        (subject,
                         patterns[pattern],
                         (label, expression)
                         )
                    )
                else:
                    patterns[pattern] = (label, expression)

    print(""\n==== ERRORS ===="")

    if no_prefLabel:
        print(""\nConcepts with no prefLabel: %d"" % len(no_prefLabel))
        print(""\n"".join([""   %s"" % subj for subj in no_prefLabel]))
    if multiple_prefLabels:
        print((""\nConcepts with multiple prefLabels: %d"" %
               len(multiple_prefLabels)))
        print(""\n"".join([""   %s"" % subj for subj in multiple_prefLabels]))
    if both_composites:
        print((""\nConcepts with both composite properties: %d"" %
               len(both_composites)))
        print(""\n"".join([""   %s"" % subj for subj in both_composites]))
    if bad_hidden_labels:
        print(""\nConcepts with bad hidden labels: %d"" % len(bad_hidden_labels))
        for kw, lbls in iteritems(bad_hidden_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if bad_alt_labels:
        print(""\nConcepts with bad alt labels: %d"" % len(bad_alt_labels))
        for kw, lbls in iteritems(bad_alt_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if both_skw_and_ckw:
        print((""\nKeywords that are both skw and ckw: %d"" %
               len(both_skw_and_ckw)))
        print(""\n"".join([""   %s"" % subj for subj in both_skw_and_ckw]))

    print()

    if composite_problem1:
        print(""\n"".join([""SKW '%s' references an unexisting CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem1]))
    if composite_problem2:
        print(""\n"".join([""SKW '%s' references a SKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem2]))
    if composite_problem3:
        print(""\n"".join([""SKW '%s' is not composite of CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem3]))
    if composite_problem4:
        for skw, ckws in iteritems(composite_problem4):
            print(""SKW '%s' does not exist but is "" ""referenced by:"" % skw)
            print(""\n"".join([""    %s"" % ckw for ckw in ckws]))
    if composite_problem5:
        print(""\n"".join([""CKW '%s' references a CKW '%s'."" % kw
                         for kw in composite_problem5]))
    if composite_problem6:
        print(""\n"".join([""CKW '%s' is not composed by SKW '%s'."" % kw
                         for kw in composite_problem6]))

    print(""\n==== WARNINGS ===="")

    if bad_notes:
        print((""\nConcepts with bad notes: %d"" % len(bad_notes)))
        print(""\n"".join([""   '%s': '%s'"" % _note for _note in bad_notes]))
    if stemming_collisions:
        print(""\nFollowing keywords have unnecessary labels that have ""
              ""already been generated by BibClassify."")
        for subj in stemming_collisions:
            print(""   %s:\n     %s\n     and %s"" % subj)

    print(""\nFinished."")
    sys.exit(0)


def test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):
    """"""Test the cache lookup.""""""
    cache = get_cache(taxonomy_name)
    if not cache:
        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,
                                                         rebuild=rebuild_cache,
                                                         no_cache=no_cache))
        cache = get_cache(taxonomy_name)
    return (thread.get_ident(), cache)


log.info('Loaded ontology reader')

if __name__ == '__main__':
    test_cache()
/n/n/ninvenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re

from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Check if a document is a PDF file and returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[-1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    # os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Return the fulltext of the local file.

    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""
    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    # Discard lines that do not contain at least one word.
    return [line for line in lines if _ONE_WORD.search(line) is not None]


def executable_exists(executable):
    """"""Test if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False
/n/n/n",0
11,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"/invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,
                                                       user_agent=make_user_agent_string(""BibClassify""))
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Outputs keywords reading a local file. Arguments and output are the same
    as for :see: get_keywords_from_text() """"""

    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/n/invenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""
BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re
import tempfile
import urllib2
from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Checks if a document is a PDF file. Returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported. Please report this to cds.support@cern.ch."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    #os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Returns the fulltext of the local file.
    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""

    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    if not _is_english_text('\n'.join(lines)):
        log.warning(""It seems the file '%s' is unvalid and doesn't ""
                    ""contain text. Please communicate this file to the Invenio ""
                    ""team."" % document)

    line_nb = len(lines)
    word_nb = 0
    for line in lines:
        word_nb += len(re.findall(""\S+"", line))

    # Discard lines that do not contain at least one word.
    lines = [line for line in lines if _ONE_WORD.search(line) is not None]

    if not remote:
        log.info(""Local file has %d lines and %d words."" % (line_nb, word_nb))

    return lines


def _is_english_text(text):
    """"""
    Checks if a text is correct english.
    Computes the number of words in the text and compares it to the
    expected number of words (based on an average size of words of 5.1
    letters).

    @param text_lines: the text to analyze
    @type text_lines:  string
    @return:           True if the text is English, False otherwise
    @rtype:            Boolean
    """"""
    # Consider one word and one space.
    avg_word_length = 2.55 + 1
    expected_word_number = float(len(text)) / avg_word_length

    words = [word
             for word in re.split('\W', text)
             if word.isalpha()]

    word_number = len(words)

    return word_number > expected_word_number


def text_lines_from_url(url, user_agent=""""):
    """"""Returns the fulltext of the file found at the URL.""""""
    request = urllib2.Request(url)
    if user_agent:
        request.add_header(""User-Agent"", user_agent)
    try:
        distant_stream = urlopen(request)
        # Write the URL content to a temporary file.
        local_file = tempfile.mkstemp(prefix=""bibclassify."")[1]
        local_stream = open(local_file, ""w"")
        local_stream.write(distant_stream.read())
        local_stream.close()
    except:
        log.error(""Unable to read from URL %s."" % url)
        return None
    else:
        # Read lines from the temporary file.
        lines = text_lines_from_local_file(local_file, remote=True)
        os.remove(local_file)

        line_nb = len(lines)
        word_nb = 0
        for line in lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))

        return lines


def executable_exists(executable):
    """"""Tests if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False


/n/n/n",1
12,2191fe6c5a850ddcf7a78f7913881cef1677500d,"src/main/python/monitoring_config_generator/yaml_tools/readers.py/n/nimport datetime
import os
import os.path
import urlparse
import socket
from time import localtime, strftime, time

from requests.exceptions import RequestException, ConnectionError, Timeout
import requests
import yaml

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException
from monitoring_config_generator.yaml_tools.merger import merge_yaml_files

def is_file(parsed_uri):
    return parsed_uri.scheme in ['', 'file']


def is_host(parsed_uri):
    return parsed_uri.scheme in ['http', 'https']


def read_config(uri):
    uri_parsed = urlparse.urlparse(uri)
    if is_file(uri_parsed):
        return read_config_from_file(uri_parsed.path)
    elif is_host(uri_parsed):
        return read_config_from_host(uri)
    else:
        raise ValueError('Given url was not acceptable %s' % uri)


def read_config_from_file(path):
    yaml_config = merge_yaml_files(path)
    etag = None
    mtime = os.path.getmtime(path)
    return yaml_config, Header(etag=etag, mtime=mtime)


def read_config_from_host(url):
    try:
        response = requests.get(url)
    except socket.error as e:
        msg = ""Could not open socket for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except ConnectionError as e:
        msg = ""Could not establish connection for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except Timeout as e:
        msg = ""Connect timed out for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except RequestException as e:
        msg = ""Could not get monitoring yaml from '%s', error: %s"" % (url, e)
        raise MonitoringConfigGeneratorException(msg)

    def get_from_header(field):
        return response.headers[field] if field in response.headers else None

    if response.status_code == 200:
        yaml_config = yaml.safe_load(response.content)
        etag = get_from_header('etag')
        mtime = get_from_header('last-modified')
        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())
    else:
        msg = ""Request %s returned with status %s. I don't know how to handle that."" % (url, response.status_code)
        raise MonitoringConfigGeneratorException(msg)

    return yaml_config, Header(etag=etag, mtime=mtime)


class Header(object):
    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'
    ETAG_COMMENT = '# ETag: '
    MTIME_COMMMENT = '# MTime: '

    def __init__(self, etag=None, mtime=0):
        self.etag = etag
        self.mtime = int(mtime)

    def __nonzero__(self):
        return self.etag is None and self.mtime is 0

    def __eq__(self, other):
        return self.etag == other.etag and self.mtime == other.mtime

    def __repr__(self):
        return ""Header(%s, %d)"" % (self.etag, self.mtime)

    def is_newer_than(self, other):
        if self.etag != other.etag or self.etag is None:
            return cmp(self.mtime, other.mtime) > 0
        else:
            return False

    def serialize(self):
        lines = []
        time_string = strftime(""%Y-%m-%d %H:%M:%S"", localtime())
        lines.append(""%s on %s"" % (Header.MON_CONF_GEN_COMMENT, time_string))
        if self.etag:
            lines.append(""%s%s"" % (Header.ETAG_COMMENT, self.etag))
        if self.mtime:
            lines.append(""%s%d"" % (Header.MTIME_COMMMENT, self.mtime))
        return lines

    @staticmethod
    def parse(file_name):
        etag, mtime = None, 0

        def extract(comment, current_value):
            value = None
            if line.startswith(comment):
                value = line.rstrip()[len(comment):]
            return value or current_value

        try:
            with open(file_name, 'r') as config_file:
                for line in config_file.xreadlines():
                    etag = extract(Header.ETAG_COMMENT, etag)
                    mtime = extract(Header.MTIME_COMMMENT, mtime)
                    if etag and mtime:
                        break
        except IOError as e:
            # it is totally fine to not have an etag, in that case there
            # will just be no caching and the server will have to deliver the data again
            pass
        finally:
            return Header(etag=etag, mtime=mtime)
/n/n/n",0
13,2191fe6c5a850ddcf7a78f7913881cef1677500d,"/src/main/python/monitoring_config_generator/yaml_tools/readers.py/n/nimport datetime
import os
import os.path
import urlparse
import socket
from time import localtime, strftime, time

from requests.exceptions import RequestException, ConnectionError, Timeout
import requests
import yaml

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException
from monitoring_config_generator.yaml_tools.merger import merge_yaml_files

def is_file(parsed_uri):
    return parsed_uri.scheme in ['', 'file']


def is_host(parsed_uri):
    return parsed_uri.scheme in ['http', 'https']


def read_config(uri):
    uri_parsed = urlparse.urlparse(uri)
    if is_file(uri_parsed):
        return read_config_from_file(uri_parsed.path)
    elif is_host(uri_parsed):
        return read_config_from_host(uri)
    else:
        raise ValueError('Given url was not acceptable %s' % uri)


def read_config_from_file(path):
    yaml_config = merge_yaml_files(path)
    etag = None
    mtime = os.path.getmtime(path)
    return yaml_config, Header(etag=etag, mtime=mtime)


def read_config_from_host(url):
    try:
        response = requests.get(url)
    except socket.error as e:
        msg = ""Could not open socket for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except ConnectionError as e:
        msg = ""Could not establish connection for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except Timeout as e:
        msg = ""Connect timed out for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except RequestException as e:
        msg = ""Could not get monitoring yaml from '%s', error: %s"" % (url, e)
        raise MonitoringConfigGeneratorException(msg)

    def get_from_header(field):
        return response.headers[field] if field in response.headers else None

    if response.status_code == 200:
        yaml_config = yaml.load(response.content)
        etag = get_from_header('etag')
        mtime = get_from_header('last-modified')
        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())
    else:
        msg = ""Request %s returned with status %s. I don't know how to handle that."" % (url, response.status_code)
        raise MonitoringConfigGeneratorException(msg)

    return yaml_config, Header(etag=etag, mtime=mtime)


class Header(object):
    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'
    ETAG_COMMENT = '# ETag: '
    MTIME_COMMMENT = '# MTime: '

    def __init__(self, etag=None, mtime=0):
        self.etag = etag
        self.mtime = int(mtime)

    def __nonzero__(self):
        return self.etag is None and self.mtime is 0

    def __eq__(self, other):
        return self.etag == other.etag and self.mtime == other.mtime

    def __repr__(self):
        return ""Header(%s, %d)"" % (self.etag, self.mtime)

    def is_newer_than(self, other):
        if self.etag != other.etag or self.etag is None:
            return cmp(self.mtime, other.mtime) > 0
        else:
            return False

    def serialize(self):
        lines = []
        time_string = strftime(""%Y-%m-%d %H:%M:%S"", localtime())
        lines.append(""%s on %s"" % (Header.MON_CONF_GEN_COMMENT, time_string))
        if self.etag:
            lines.append(""%s%s"" % (Header.ETAG_COMMENT, self.etag))
        if self.mtime:
            lines.append(""%s%d"" % (Header.MTIME_COMMMENT, self.mtime))
        return lines

    @staticmethod
    def parse(file_name):
        etag, mtime = None, 0

        def extract(comment, current_value):
            value = None
            if line.startswith(comment):
                value = line.rstrip()[len(comment):]
            return value or current_value

        try:
            with open(file_name, 'r') as config_file:
                for line in config_file.xreadlines():
                    etag = extract(Header.ETAG_COMMENT, etag)
                    mtime = extract(Header.MTIME_COMMMENT, mtime)
                    if etag and mtime:
                        break
        except IOError as e:
            # it is totally fine to not have an etag, in that case there
            # will just be no caching and the server will have to deliver the data again
            pass
        finally:
            return Header(etag=etag, mtime=mtime)
/n/n/n",1
14,a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7,"src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py/n/n""""""monconfgenerator

Creates an Icinga monitoring configuration. It does it by querying an URL from
which it receives a specially formatted yaml file. This file is transformed into
a valid Icinga configuration file.
If no URL is given it reads it's default configuration from file system. The
configuration file is: /etc/monitoring_config_generator/config.yaml'

Usage:
  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]
  monconfgenerator -h

Options:
  -h                Show this message.
  --debug           Print additional information.
  --targetdir=DIR   The generated Icinga monitoring configuration is written
                    into this directory. If no target directory is given its
                    value is read from /etc/monitoring_config_generator/config.yaml
  --skip-checks     Do not run checks on the yaml file received from the URL.

""""""
from datetime import datetime
import logging
import os
import sys

from docopt import docopt

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \
    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException
from monitoring_config_generator import set_log_level_to_debug
from monitoring_config_generator.yaml_tools.readers import Header, read_config
from monitoring_config_generator.yaml_tools.config import YamlConfig
from monitoring_config_generator.settings import CONFIG


EXIT_CODE_CONFIG_WRITTEN = 0
EXIT_CODE_ERROR = 1
EXIT_CODE_NOT_WRITTEN = 2

LOG = logging.getLogger(""monconfgenerator"")


class MonitoringConfigGenerator(object):
    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):
        self.skip_checks = skip_checks
        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']
        self.source = url

        if debug_enabled:
            set_log_level_to_debug()

        if not self.target_dir or not os.path.isdir(self.target_dir):
            raise MonitoringConfigGeneratorException(""%s is not a directory"" % self.target_dir)

        LOG.debug(""Using %s as target dir"" % self.target_dir)
        LOG.debug(""Using URL: %s"" % self.source)
        LOG.debug(""MonitoringConfigGenerator start: reading from %s, writing to %s"" %
                  (self.source, self.target_dir))

    def _is_newer(self, header_source, hostname):
        if not hostname:
            raise NoSuchHostname('hostname not found')
        output_path = self.output_path(self.create_filename(hostname))
        old_header = Header.parse(output_path)
        return header_source.is_newer_than(old_header)

    def output_path(self, file_name):
        return os.path.join(self.target_dir, file_name)

    def write_output(self, file_name, yaml_icinga):
        lines = yaml_icinga.icinga_lines
        output_writer = OutputWriter(self.output_path(file_name))
        output_writer.write_lines(lines)

    @staticmethod
    def create_filename(hostname):
        name = '%s.cfg' % hostname
        if name != os.path.basename(name):
            msg = ""Directory traversal attempt detected for host name %r""
            raise Exception(msg % hostname)
        return name

    def generate(self):
        file_name = None
        raw_yaml_config, header_source = read_config(self.source)

        if raw_yaml_config is None:
            raise SystemExit(""Raw yaml config from source '%s' is 'None'."" % self.source)

        yaml_config = YamlConfig(raw_yaml_config,
                                 skip_checks=self.skip_checks)

        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):
            file_name = self.create_filename(yaml_config.host_name)
            yaml_icinga = YamlToIcinga(yaml_config, header_source)
            self.write_output(file_name, yaml_icinga)

        if file_name:
            LOG.info(""Icinga config file '%s' created."" % file_name)

        return file_name

class YamlToIcinga(object):
    def __init__(self, yaml_config, header):
        self.icinga_lines = []
        self.indent = CONFIG['INDENT']
        self.icinga_lines.extend(header.serialize())
        self.write_section('host', yaml_config.host)
        for service in yaml_config.services:
            self.write_section('service', service)

    def write_line(self, line):
        self.icinga_lines.append(line)

    def write_section(self, section_name, section_data):
        self.write_line("""")
        self.write_line(""define %s {"" % section_name)
        sorted_keys = section_data.keys()
        sorted_keys.sort()
        for key in sorted_keys:
            value = self.value_to_icinga(section_data[key])
            icinga_line = ""%s%-45s%s"" % (self.indent, key, value)

            if ""\n"" in icinga_line or ""}"" in icinga_line:
                msg = ""Found forbidden newline or '}' character in section %r.""
                raise Exception(msg % section_name)

            self.icinga_lines.append(icinga_line)
        self.write_line(""}"")

    @staticmethod
    def value_to_icinga(value):
        """"""Convert a scalar or list to Icinga value format. Lists are concatenated by ,
        and empty (None) values produce an empty string""""""
        if isinstance(value, list):
            # explicitly set None values to empty string
            return "","".join([str(x) if (x is not None) else """" for x in value])
        else:
            return str(value)


class OutputWriter(object):
    def __init__(self, output_file):
        self.output_file = output_file

    def write_lines(self, lines):
        with open(self.output_file, 'w') as f:
            for line in lines:
                f.write(line + ""\n"")
        LOG.debug(""Created %s"" % self.output_file)


def generate_config():
    arg = docopt(__doc__, version='0.1.0')
    start_time = datetime.now()
    try:
        file_name = MonitoringConfigGenerator(arg['URL'],
                                              arg['--debug'],
                                              arg['--targetdir'],
                                              arg['--skip-checks']).generate()
        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN
    except HostUnreachableException:
        LOG.warn(""Target url {0} unreachable. Could not get yaml config!"".format(arg['URL']))
        exit_code = EXIT_CODE_NOT_WRITTEN
    except ConfigurationContainsUndefinedVariables:
        LOG.error(""Configuration contained undefined variables!"")
        exit_code = EXIT_CODE_ERROR
    except SystemExit as e:
        exit_code = e.code
    except BaseException as e:
        LOG.error(e)
        exit_code = EXIT_CODE_ERROR
    finally:
        stop_time = datetime.now()
        LOG.info(""finished in %s"" % (stop_time - start_time))
    sys.exit(exit_code)


if __name__ == '__main__':
    generate_config()
/n/n/nsrc/unittest/python/YamlToIcinga_tests.py/n/nimport os
import unittest
from mock import Mock

os.environ['MONITORING_CONFIG_GENERATOR_CONFIG'] = ""testdata/testconfig.yaml""
from monitoring_config_generator.MonitoringConfigGenerator import YamlToIcinga


class Test(unittest.TestCase):

    def test_text_to_cvs(self):
        self.assertEquals("""", YamlToIcinga.value_to_icinga(""""))
        self.assertEquals(""text"", YamlToIcinga.value_to_icinga(""text""))

    def test_number_to_cvs(self):
        self.assertEquals(""42"", YamlToIcinga.value_to_icinga(42))
        self.assertEquals(""-1"", YamlToIcinga.value_to_icinga(-1))
        self.assertEquals(""-1.6"", YamlToIcinga.value_to_icinga(-1.6))

    def test_list_to_cvs(self):
        self.assertEquals("""", YamlToIcinga.value_to_icinga([]))
        self.assertEquals(""a"", YamlToIcinga.value_to_icinga([""a""]))
        self.assertEquals(""a,b"", YamlToIcinga.value_to_icinga([""a"", ""b""]))
        self.assertEquals(""a,,b"", YamlToIcinga.value_to_icinga([""a"", None, ""b""]))
        self.assertEquals("",,,"", YamlToIcinga.value_to_icinga([None, None, None, None]))
        self.assertEquals("",23,42,"", YamlToIcinga.value_to_icinga([None, ""23"", 42, None]))

    def _get_config_mock(self, host=None, services=None):
        config = Mock()
        config.host = host or {}
        config.services = services or {}
        return config

    def test_write_section_forbidden_characters(self):
        # Malicious hosts may try to insert new sections, e.g. by setting a
        # value to  ""42\n}\n define command {\n ......"" which would lead to
        # arbitrary code execution. Therefore, certain characters must be
        # forbidden.
        header = Mock()
        header.serialize.return_value = ""the header""

        for forbidden in '\n', '}':
            # Forbidden character in 'host' section.
            config = self._get_config_mock(host={'key': 'xx%syy' % forbidden})
            self.assertRaises(Exception, YamlToIcinga, config, header)
            config = self._get_config_mock(host={'xx%syy' % forbidden: ""value""})
            self.assertRaises(Exception, YamlToIcinga, config, header)

            config = self._get_config_mock(services={'foo': 'xx%syy' % forbidden})
            self.assertRaises(Exception, YamlToIcinga, config, header)
            config = self._get_config_mock(services={'xx%syy' % forbidden: ""value""})
            self.assertRaises(Exception, YamlToIcinga, config, header)
/n/n/n",0
15,a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7,"/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py/n/n""""""monconfgenerator

Creates an Icinga monitoring configuration. It does it by querying an URL from
which it receives a specially formatted yaml file. This file is transformed into
a valid Icinga configuration file.
If no URL is given it reads it's default configuration from file system. The
configuration file is: /etc/monitoring_config_generator/config.yaml'

Usage:
  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]
  monconfgenerator -h

Options:
  -h                Show this message.
  --debug           Print additional information.
  --targetdir=DIR   The generated Icinga monitoring configuration is written
                    into this directory. If no target directory is given its
                    value is read from /etc/monitoring_config_generator/config.yaml
  --skip-checks     Do not run checks on the yaml file received from the URL.

""""""
from datetime import datetime
import logging
import os
import sys

from docopt import docopt

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \
    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException
from monitoring_config_generator import set_log_level_to_debug
from monitoring_config_generator.yaml_tools.readers import Header, read_config
from monitoring_config_generator.yaml_tools.config import YamlConfig
from monitoring_config_generator.settings import CONFIG


EXIT_CODE_CONFIG_WRITTEN = 0
EXIT_CODE_ERROR = 1
EXIT_CODE_NOT_WRITTEN = 2

LOG = logging.getLogger(""monconfgenerator"")


class MonitoringConfigGenerator(object):
    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):
        self.skip_checks = skip_checks
        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']
        self.source = url

        if debug_enabled:
            set_log_level_to_debug()

        if not self.target_dir or not os.path.isdir(self.target_dir):
            raise MonitoringConfigGeneratorException(""%s is not a directory"" % self.target_dir)

        LOG.debug(""Using %s as target dir"" % self.target_dir)
        LOG.debug(""Using URL: %s"" % self.source)
        LOG.debug(""MonitoringConfigGenerator start: reading from %s, writing to %s"" %
                  (self.source, self.target_dir))

    def _is_newer(self, header_source, hostname):
        if not hostname:
            raise NoSuchHostname('hostname not found')
        output_path = self.output_path(self.create_filename(hostname))
        old_header = Header.parse(output_path)
        return header_source.is_newer_than(old_header)

    def output_path(self, file_name):
        return os.path.join(self.target_dir, file_name)

    def write_output(self, file_name, yaml_icinga):
        lines = yaml_icinga.icinga_lines
        output_writer = OutputWriter(self.output_path(file_name))
        output_writer.write_lines(lines)

    @staticmethod
    def create_filename(hostname):
        name = '%s.cfg' % hostname
        if name != os.path.basename(name):
            msg = ""Directory traversal attempt detected for host name %r""
            raise Exception(msg % hostname)
        return name

    def generate(self):
        file_name = None
        raw_yaml_config, header_source = read_config(self.source)

        if raw_yaml_config is None:
            raise SystemExit(""Raw yaml config from source '%s' is 'None'."" % self.source)

        yaml_config = YamlConfig(raw_yaml_config,
                                 skip_checks=self.skip_checks)

        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):
            file_name = self.create_filename(yaml_config.host_name)
            yaml_icinga = YamlToIcinga(yaml_config, header_source)
            self.write_output(file_name, yaml_icinga)

        if file_name:
            LOG.info(""Icinga config file '%s' created."" % file_name)

        return file_name

class YamlToIcinga(object):
    def __init__(self, yaml_config, header):
        self.icinga_lines = []
        self.indent = CONFIG['INDENT']
        self.icinga_lines.extend(header.serialize())
        self.write_section('host', yaml_config.host)
        for service in yaml_config.services:
            self.write_section('service', service)

    def write_line(self, line):
        self.icinga_lines.append(line)

    def write_section(self, section_name, section_data):
        self.write_line("""")
        self.write_line(""define %s {"" % section_name)
        sorted_keys = section_data.keys()
        sorted_keys.sort()
        for key in sorted_keys:
            value = section_data[key]
            self.icinga_lines.append((""%s%-45s%s"" % (self.indent, key, self.value_to_icinga(value))))
        self.write_line(""}"")

    @staticmethod
    def value_to_icinga(value):
        """"""Convert a scalar or list to Icinga value format. Lists are concatenated by ,
        and empty (None) values produce an empty string""""""
        if isinstance(value, list):
            # explicitly set None values to empty string
            return "","".join([str(x) if (x is not None) else """" for x in value])
        else:
            return str(value)


class OutputWriter(object):
    def __init__(self, output_file):
        self.output_file = output_file

    def write_lines(self, lines):
        with open(self.output_file, 'w') as f:
            for line in lines:
                f.write(line + ""\n"")
        LOG.debug(""Created %s"" % self.output_file)


def generate_config():
    arg = docopt(__doc__, version='0.1.0')
    start_time = datetime.now()
    try:
        file_name = MonitoringConfigGenerator(arg['URL'],
                                              arg['--debug'],
                                              arg['--targetdir'],
                                              arg['--skip-checks']).generate()
        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN
    except HostUnreachableException:
        LOG.warn(""Target url {0} unreachable. Could not get yaml config!"".format(arg['URL']))
        exit_code = EXIT_CODE_NOT_WRITTEN
    except ConfigurationContainsUndefinedVariables:
        LOG.error(""Configuration contained undefined variables!"")
        exit_code = EXIT_CODE_ERROR
    except SystemExit as e:
        exit_code = e.code
    except BaseException as e:
        LOG.error(e)
        exit_code = EXIT_CODE_ERROR
    finally:
        stop_time = datetime.now()
        LOG.info(""finished in %s"" % (stop_time - start_time))
    sys.exit(exit_code)


if __name__ == '__main__':
    generate_config()
/n/n/n",1
16,e09ec28786aa04bb7a6459fec6294bbb9368671a,"pep8speaks/helpers.py/n/n# -*- coding: utf-8 -*-

import base64
import collections
import datetime
import hmac
import json
import os
import re
import subprocess
import time

import psycopg2
import requests
import unidiff
import yaml
from flask import abort


def update_users(repository):
    """"""Update users of the integration in the database""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        # Check if repository exists in database
        query = r""INSERT INTO Users (repository, created_at) VALUES ('{}', now());"" \
                """".format(repository)

        try:
            cursor.execute(query)
            conn.commit()
        except psycopg2.IntegrityError:  # If already exists
            conn.rollback()


def follow_user(user):
    """"""Follow the user of the service""""""
    headers = {
        ""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""],
        ""Content-Length"": ""0"",
    }
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/user/following/{}""
    url = url.format(user)
    r = requests.put(url, headers=headers, auth=auth)


def update_dict(base, head):
    """"""
    Recursively merge or update dict-like objects.
    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})

    Source : http://stackoverflow.com/a/32357112/4698026
    """"""
    for key, value in head.items():
        if key in base:
            if isinstance(base, collections.Mapping):
                if isinstance(value, collections.Mapping):
                    base[key] = update_dict(base.get(key, {}), value)
                else:
                    base[key] = head[key]
            else:
                base = {key: head[key]}
    return base


def match_webhook_secret(request):
    """"""Match the webhook secret sent from GitHub""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        header_signature = request.headers.get('X-Hub-Signature')
        if header_signature is None:
            abort(403)
        sha_name, signature = header_signature.split('=')
        if sha_name != 'sha1':
            abort(501)
        mac = hmac.new(os.environ[""GITHUB_PAYLOAD_SECRET""].encode(), msg=request.data,
                       digestmod=""sha1"")
        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):
            abort(403)
    return True


def check_pythonic_pr(data):
    """"""
    Return True if the PR contains at least one Python file
    """"""
    files = list(get_files_involved_in_pr(data).keys())
    pythonic = False
    for file in files:
        if file[-3:] == '.py':
            pythonic = True
            break

    return pythonic


def get_config(data):
    """"""
    Get .pep8speaks.yml config file from the repository and return
    the config dictionary
    """"""

    # Default configuration parameters
    config = {
        ""message"": {
            ""opened"": {
                ""header"": """",
                ""footer"": """"
            },
            ""updated"": {
                ""header"": """",
                ""footer"": """"
            }
        },
        ""scanner"": {""diff_only"": False},
        ""pycodestyle"": {
            ""ignore"": [],
            ""max-line-length"": 79,
            ""count"": False,
            ""first"": False,
            ""show-pep8"": False,
            ""filename"": [],
            ""exclude"": [],
            ""select"": [],
            ""show-source"": False,
            ""statistics"": False,
            ""hang-closing"": False,
        },
        ""no_blank_comment"": True,
        ""only_mention_files_with_errors"": True,
    }

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Configuration file
    url = ""https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml""

    url = url.format(data[""repository""], data[""after_commit_hash""])
    r = requests.get(url, headers=headers, auth=auth)
    if r.status_code == 200:
        try:
            new_config = yaml.load(r.text)
            # overloading the default configuration with the one specified
            config = update_dict(config, new_config)
        except yaml.YAMLError:  # Bad YAML file
            pass

    # Create pycodestyle command line arguments
    arguments = []
    confs = config[""pycodestyle""]
    for key, value in confs.items():
        if value:  # Non empty
            if isinstance(value, int):
                if isinstance(value, bool):
                    arguments.append(""--{}"".format(key))
                else:
                    arguments.append(""--{}={}"".format(key, value))
            elif isinstance(value, list):
                arguments.append(""--{}={}"".format(key, ','.join(value)))
    config[""pycodestyle_cmd_config""] = ' {arguments}'.format(arguments=' '.join(arguments))

    # pycodestyle is case-sensitive
    config[""pycodestyle""][""ignore""] = [e.upper() for e in list(config[""pycodestyle""][""ignore""])]

    return config


def get_files_involved_in_pr(data):
    """"""
    Return a list of file names modified/added in the PR
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    diff_headers = headers.copy()
    diff_headers[""Accept""] = ""application/vnd.github.VERSION.diff""
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]
    diff_url = ""https://api.github.com/repos/{}/pulls/{}""
    diff_url = diff_url.format(repository, str(data[""pr_number""]))
    r = requests.get(diff_url, headers=diff_headers, auth=auth)
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    files = {}

    for patchset in patch:
        file = patchset.target_file[1:]
        files[file] = []
        for hunk in patchset:
            for line in hunk.target_lines():
                if line.is_added:
                    files[file].append(line.target_line_no)

    return files


def get_python_files_involved_in_pr(data):
    files = get_files_involved_in_pr(data)
    for file in list(files.keys()):
        if file[-3:] != "".py"":
            del files[file]

    return files


def run_pycodestyle(data, config):
    """"""
    Run pycodestyle script on the files and update the data
    dictionary
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]

    # Run pycodestyle
    ## All the python files with additions
    # A dictionary with filename paired with list of new line numbers
    py_files = get_python_files_involved_in_pr(data)

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(repository, after_commit_hash, file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_check.py"", 'w+', encoding=r.encoding) as file_to_check:
            file_to_check.write(r.text)

        # Use the command line here
        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(
            config=config)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""extra_results""][filename] = stdout.decode(r.encoding).splitlines()

        # Put only relevant errors in the data[""results""] dictionary
        data[""results""][filename] = []
        for error in list(data[""extra_results""][filename]):
            if re.search(""^file_to_check.py:\d+:\d+:\s[WE]\d+\s.*"", error):
                data[""results""][filename].append(error.replace(""file_to_check.py"", filename))
                data[""extra_results""][filename].remove(error)

        ## Remove errors in case of diff_only = True
        ## which are caused in the whole file
        for error in list(data[""results""][filename]):
            if config[""scanner""][""diff_only""]:
                if not int(error.split("":"")[1]) in py_files[file]:
                    data[""results""][filename].remove(error)

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(repository, after_commit_hash, file)
        os.remove(""file_to_check.py"")


def prepare_comment(request, data, config):
    """"""Construct the string of comment i.e. its header, body and footer""""""
    author = data[""author""]
    # Write the comment body
    ## Header
    comment_header = """"
    if request.json[""action""] == ""opened"":
        if config[""message""][""opened""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for submitting the PR.\n\n""
        else:
            comment_header = config[""message""][""opened""][""header""] + ""\n\n""
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        if config[""message""][""updated""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for updating the PR.\n\n""
        else:
            comment_header = config[""message""][""updated""][""header""] + ""\n\n""

    ## Body
    ERROR = False  # Set to True when any pep8 error exists
    comment_body = []
    for file, issues in data[""results""].items():
        if len(issues) == 0:
            if not config[""only_mention_files_with_errors""]:
                comment_body.append(
                    "" - There are no PEP8 issues in the""
                    "" file [`{0}`]({1}) !"".format(file, data[file + ""_link""]))
        else:
            ERROR = True
            comment_body.append(
                "" - In the file [`{0}`]({1}), following ""
                ""are the PEP8 issues :\n"".format(file, data[file + ""_link""]))
            for issue in issues:
                ## Replace filename with L
                error_string = issue.replace(file + "":"", ""Line "")

                ## Link error codes to search query
                error_string_list = error_string.split("" "")
                code = error_string_list[2]
                code_url = ""https://duckduckgo.com/?q=pep8%20{0}"".format(code)
                error_string_list[2] = ""[{0}]({1})"".format(code, code_url)

                ## Link line numbers in the file
                line, col = error_string_list[1][:-1].split("":"")
                line_url = data[file + ""_link""] + ""#L"" + line
                error_string_list[1] = ""[{0}:{1}]({2}):"".format(line, col, line_url)
                error_string = "" "".join(error_string_list)
                error_string = error_string.replace(""Line ["", ""[Line "")
                comment_body.append(""\n> {0}"".format(error_string))

        comment_body.append(""\n\n"")
        if len(data[""extra_results""][file]) > 0:
            comment_body.append("" - Complete extra results for this file :\n\n"")
            comment_body.append(""> "" + """".join(data[""extra_results""][file]))
            comment_body.append(""---\n\n"")

    if config[""only_mention_files_with_errors""] and not ERROR:
        comment_body.append(""Cheers ! There are no PEP8 issues in this Pull Request. :beers: "")


    comment_body = ''.join(comment_body)


    ## Footer
    comment_footer = []
    if request.json[""action""] == ""opened"":
        comment_footer.append(config[""message""][""opened""][""footer""])
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        comment_footer.append(config[""message""][""updated""][""footer""])

    comment_footer = ''.join(comment_footer)

    return comment_header, comment_body, comment_footer, ERROR


def comment_permission_check(data, comment):
    """"""Check for quite and resume status or duplicate comments""""""
    PERMITTED_TO_COMMENT = True
    repository = data[""repository""]
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Check for duplicate comment
    url = ""https://api.github.com/repos/{}/issues/{}/comments""
    url = url.format(repository, str(data[""pr_number""]))
    comments = requests.get(url, headers=headers, auth=auth).json()

    # Get the last comment by the bot
    last_comment = """"
    for old_comment in reversed(comments):
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment = old_comment[""body""]
            break

    """"""
    # Disabling this because only a single comment is made per PR
    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))
    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))
    if text1 == text2.replace(""submitting"", ""updating""):
        PERMITTED_TO_COMMENT = False
    """"""

    # Check if the bot is asked to keep quiet
    for old_comment in reversed(comments):
        if '@pep8speaks' in old_comment['body']:
            if 'resume' in old_comment['body'].lower():
                break
            elif 'quiet' in old_comment['body'].lower():
                PERMITTED_TO_COMMENT = False


    return PERMITTED_TO_COMMENT


def create_or_update_comment(data, comment):
    comment_mode = None
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    query = ""https://api.github.com/repos/{}/issues/{}/comments""
    query = query.format(data[""repository""], str(data[""pr_number""]))
    comments = requests.get(query, headers=headers, auth=auth).json()

    # Get the last comment id by the bot
    last_comment_id = None
    for old_comment in comments:
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment_id = old_comment[""id""]
            break

    if last_comment_id is None:  # Create a new comment
        response = requests.post(query, json={""body"": comment}, headers=headers, auth=auth)
        data[""comment_response""] = response.json()
    else:  # Update the last comment
        utc_time = datetime.datetime.utcnow()
        time_now = utc_time.strftime(""%B %d, %Y at %H:%M Hours UTC"")
        comment += ""\n\n##### Comment last updated on {}""
        comment = comment.format(time_now)

        query = ""https://api.github.com/repos/{}/issues/comments/{}""
        query = query.format(data[""repository""], str(last_comment_id))
        response = requests.patch(query, json={""body"": comment}, headers=headers, auth=auth)


def autopep8(data, config):
    # Run pycodestyle

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)
    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""diff""][filename] = stdout.decode(r.encoding)

        # Fix the errors
        data[""diff""][filename] = data[""diff""][filename].replace(""file_to_check.py"", filename)
        data[""diff""][filename] = data[""diff""][filename].replace(""\\"", ""\\\\"")

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(data[""repository""], data[""sha""], file)
        os.remove(""file_to_fix.py"")


def create_gist(data, config):
    """"""Create gists for diff files""""""
    REQUEST_JSON = {}
    REQUEST_JSON[""public""] = True
    REQUEST_JSON[""files""] = {}
    REQUEST_JSON[""description""] = ""In response to @{0}'s comment : {1}"".format(
        data[""reviewer""], data[""review_url""])

    for file, diffs in data[""diff""].items():
        if len(diffs) != 0:
            REQUEST_JSON[""files""][file.split(""/"")[-1] + "".diff""] = {
                ""content"": diffs
            }

    # Call github api to create the gist
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/gists""
    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()
    data[""gist_response""] = res
    data[""gist_url""] = res[""html_url""]


def delete_if_forked(data):
    FORKED = False
    url = ""https://api.github.com/user/repos""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    for repo in r.json():
        if repo[""description""]:
            if data[""target_repo_fullname""] in repo[""description""]:
                FORKED = True
                r = requests.delete(""https://api.github.com/repos/""
                                ""{}"".format(repo[""full_name""]),
                                headers=headers, auth=auth)
    return FORKED


def fork_for_pr(data):
    FORKED = False
    url = ""https://api.github.com/repos/{}/forks""
    url = url.format(data[""target_repo_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.post(url, headers=headers, auth=auth)
    if r.status_code == 202:
        data[""fork_fullname""] = r.json()[""full_name""]
        FORKED = True
    else:
        data[""error""] = ""Unable to fork""
    return FORKED


def update_fork_desc(data):
    # Check if forked (takes time)
    url = ""https://api.github.com/repos/{}"".format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    ATTEMPT = 0
    while(r.status_code != 200):
        time.sleep(5)
        r = requests.get(url, headers=headers, auth=auth)
        ATTEMPT += 1
        if ATTEMPT > 10:
            data[""error""] = ""Forking is taking more than usual time""
            break

    full_name = data[""target_repo_fullname""]
    author, name = full_name.split(""/"")
    request_json = {
        ""name"": name,
        ""description"": ""Forked from @{}'s {}"".format(author, full_name)
    }
    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)
    if r.status_code != 200:
        data[""error""] = ""Could not update description of the fork""


def create_new_branch(data):
    url = ""https://api.github.com/repos/{}/git/refs/heads""
    url = url.format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    sha = None
    r = requests.get(url, headers=headers, auth=auth)
    for ref in r.json():
        if ref[""ref""].split(""/"")[-1] == data[""target_repo_branch""]:
            sha = ref[""object""][""sha""]

    url = ""https://api.github.com/repos/{}/git/refs""
    url = url.format(data[""fork_fullname""])
    data[""new_branch""] = ""{}-pep8-patch"".format(data[""target_repo_branch""])
    request_json = {
        ""ref"": ""refs/heads/{}"".format(data[""new_branch""]),
        ""sha"": sha,
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)

    if r.status_code != 200:
        data[""error""] = ""Could not create new branch in the fork""


def autopep8ify(data, config):
    # Run pycodestyle
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)

    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""results""][filename] = stdout.decode(r.encoding)

        os.remove(""file_to_fix.py"")


def commit(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    fullname = data.get(""fork_fullname"")

    for file, new_file in data[""results""].items():
        url = ""https://api.github.com/repos/{}/contents/{}""
        url = url.format(fullname, file)
        params = {""ref"": data[""new_branch""]}
        r = requests.get(url, params=params, headers=headers, auth=auth)
        sha_blob = r.json().get(""sha"")
        params[""path""] = file
        content_code = base64.b64encode(new_file.encode()).decode(""utf-8"")
        request_json = {
            ""path"": file,
            ""message"": ""Fix pep8 errors in {}"".format(file),
            ""content"": content_code,
            ""sha"": sha_blob,
            ""branch"": data.get(""new_branch""),
        }
        r = requests.put(url, json=request_json, headers=headers, auth=auth)


def create_pr(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/repos/{}/pulls""
    url = url.format(data[""target_repo_fullname""])
    request_json = {
        ""title"": ""Fix pep8 errors"",
        ""head"": ""pep8speaks:{}"".format(data[""new_branch""]),
        ""base"": data[""target_repo_branch""],
        ""body"": ""The changes are suggested by autopep8"",
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)
    if r.status_code == 201:
        data[""pr_url""] = r.json()[""html_url""]
    else:
        data[""error""] = ""Pull request could not be created""
/n/n/n",0
17,e09ec28786aa04bb7a6459fec6294bbb9368671a,"/pep8speaks/helpers.py/n/n# -*- coding: utf-8 -*-

import base64
import collections
import datetime
import hmac
import json
import os
import re
import subprocess
import time

import psycopg2
import requests
import unidiff
import yaml
from flask import abort


def update_users(repository):
    """"""Update users of the integration in the database""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        # Check if repository exists in database
        query = r""INSERT INTO Users (repository, created_at) VALUES ('{}', now());"" \
                """".format(repository)

        try:
            cursor.execute(query)
            conn.commit()
        except psycopg2.IntegrityError:  # If already exists
            conn.rollback()


def follow_user(user):
    """"""Follow the user of the service""""""
    headers = {
        ""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""],
        ""Content-Length"": ""0"",
    }
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/user/following/{}""
    url = url.format(user)
    r = requests.put(url, headers=headers, auth=auth)


def update_dict(base, head):
    """"""
    Recursively merge or update dict-like objects.
    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})

    Source : http://stackoverflow.com/a/32357112/4698026
    """"""
    for key, value in head.items():
        if isinstance(base, collections.Mapping):
            if isinstance(value, collections.Mapping):
                base[key] = update_dict(base.get(key, {}), value)
            else:
                base[key] = head[key]
        else:
            base = {key: head[key]}
    return base


def match_webhook_secret(request):
    """"""Match the webhook secret sent from GitHub""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        header_signature = request.headers.get('X-Hub-Signature')
        if header_signature is None:
            abort(403)
        sha_name, signature = header_signature.split('=')
        if sha_name != 'sha1':
            abort(501)
        mac = hmac.new(os.environ[""GITHUB_PAYLOAD_SECRET""].encode(), msg=request.data,
                       digestmod=""sha1"")
        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):
            abort(403)
    return True


def check_pythonic_pr(data):
    """"""
    Return True if the PR contains at least one Python file
    """"""
    files = list(get_files_involved_in_pr(data).keys())
    pythonic = False
    for file in files:
        if file[-3:] == '.py':
            pythonic = True
            break

    return pythonic


def get_config(data):
    """"""
    Get .pep8speaks.yml config file from the repository and return
    the config dictionary
    """"""

    # Default configuration parameters
    config = {
        ""message"": {
            ""opened"": {
                ""header"": """",
                ""footer"": """"
            },
            ""updated"": {
                ""header"": """",
                ""footer"": """"
            }
        },
        ""scanner"": {""diff_only"": False},
        ""pycodestyle"": {
            ""ignore"": [],
            ""max-line-length"": 79,
            ""count"": False,
            ""first"": False,
            ""show-pep8"": False,
            ""filename"": [],
            ""exclude"": [],
            ""select"": [],
            ""show-source"": False,
            ""statistics"": False,
            ""hang-closing"": False,
        },
        ""no_blank_comment"": True,
        ""only_mention_files_with_errors"": True,
    }

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Configuration file
    url = ""https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml""

    url = url.format(data[""repository""], data[""after_commit_hash""])
    r = requests.get(url, headers=headers, auth=auth)
    if r.status_code == 200:
        try:
            new_config = yaml.load(r.text)
            # overloading the default configuration with the one specified
            config = update_dict(config, new_config)
        except yaml.YAMLError:  # Bad YAML file
            pass

    # Create pycodestyle command line arguments
    arguments = []
    confs = config[""pycodestyle""]
    for key, value in confs.items():
        if value:  # Non empty
            if isinstance(value, int):
                if isinstance(value, bool):
                    arguments.append(""--{}"".format(key))
                else:
                    arguments.append(""--{}={}"".format(key, value))
            elif isinstance(value, list):
                arguments.append(""--{}={}"".format(key, ','.join(value)))
    config[""pycodestyle_cmd_config""] = ' {arguments}'.format(arguments=' '.join(arguments))

    # pycodestyle is case-sensitive
    config[""pycodestyle""][""ignore""] = [e.upper() for e in list(config[""pycodestyle""][""ignore""])]

    return config


def get_files_involved_in_pr(data):
    """"""
    Return a list of file names modified/added in the PR
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    diff_headers = headers.copy()
    diff_headers[""Accept""] = ""application/vnd.github.VERSION.diff""
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]
    diff_url = ""https://api.github.com/repos/{}/pulls/{}""
    diff_url = diff_url.format(repository, str(data[""pr_number""]))
    r = requests.get(diff_url, headers=diff_headers, auth=auth)
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    files = {}

    for patchset in patch:
        file = patchset.target_file[1:]
        files[file] = []
        for hunk in patchset:
            for line in hunk.target_lines():
                if line.is_added:
                    files[file].append(line.target_line_no)

    return files


def get_python_files_involved_in_pr(data):
    files = get_files_involved_in_pr(data)
    for file in list(files.keys()):
        if file[-3:] != "".py"":
            del files[file]

    return files


def run_pycodestyle(data, config):
    """"""
    Run pycodestyle script on the files and update the data
    dictionary
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]

    # Run pycodestyle
    ## All the python files with additions
    # A dictionary with filename paired with list of new line numbers
    py_files = get_python_files_involved_in_pr(data)

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(repository, after_commit_hash, file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_check.py"", 'w+', encoding=r.encoding) as file_to_check:
            file_to_check.write(r.text)

        # Use the command line here
        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(
            config=config)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""extra_results""][filename] = stdout.decode(r.encoding).splitlines()

        # Put only relevant errors in the data[""results""] dictionary
        data[""results""][filename] = []
        for error in list(data[""extra_results""][filename]):
            if re.search(""^file_to_check.py:\d+:\d+:\s[WE]\d+\s.*"", error):
                data[""results""][filename].append(error.replace(""file_to_check.py"", filename))
                data[""extra_results""][filename].remove(error)

        ## Remove errors in case of diff_only = True
        ## which are caused in the whole file
        for error in list(data[""results""][filename]):
            if config[""scanner""][""diff_only""]:
                if not int(error.split("":"")[1]) in py_files[file]:
                    data[""results""][filename].remove(error)

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(repository, after_commit_hash, file)
        os.remove(""file_to_check.py"")


def prepare_comment(request, data, config):
    """"""Construct the string of comment i.e. its header, body and footer""""""
    author = data[""author""]
    # Write the comment body
    ## Header
    comment_header = """"
    if request.json[""action""] == ""opened"":
        if config[""message""][""opened""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for submitting the PR.\n\n""
        else:
            comment_header = config[""message""][""opened""][""header""] + ""\n\n""
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        if config[""message""][""updated""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for updating the PR.\n\n""
        else:
            comment_header = config[""message""][""updated""][""header""] + ""\n\n""

    ## Body
    ERROR = False  # Set to True when any pep8 error exists
    comment_body = []
    for file, issues in data[""results""].items():
        if len(issues) == 0:
            if not config[""only_mention_files_with_errors""]:
                comment_body.append(
                    "" - There are no PEP8 issues in the""
                    "" file [`{0}`]({1}) !"".format(file, data[file + ""_link""]))
        else:
            ERROR = True
            comment_body.append(
                "" - In the file [`{0}`]({1}), following ""
                ""are the PEP8 issues :\n"".format(file, data[file + ""_link""]))
            for issue in issues:
                ## Replace filename with L
                error_string = issue.replace(file + "":"", ""Line "")

                ## Link error codes to search query
                error_string_list = error_string.split("" "")
                code = error_string_list[2]
                code_url = ""https://duckduckgo.com/?q=pep8%20{0}"".format(code)
                error_string_list[2] = ""[{0}]({1})"".format(code, code_url)

                ## Link line numbers in the file
                line, col = error_string_list[1][:-1].split("":"")
                line_url = data[file + ""_link""] + ""#L"" + line
                error_string_list[1] = ""[{0}:{1}]({2}):"".format(line, col, line_url)
                error_string = "" "".join(error_string_list)
                error_string = error_string.replace(""Line ["", ""[Line "")
                comment_body.append(""\n> {0}"".format(error_string))

        comment_body.append(""\n\n"")
        if len(data[""extra_results""][file]) > 0:
            comment_body.append("" - Complete extra results for this file :\n\n"")
            comment_body.append(""> "" + """".join(data[""extra_results""][file]))
            comment_body.append(""---\n\n"")

    if config[""only_mention_files_with_errors""] and not ERROR:
        comment_body.append(""Cheers ! There are no PEP8 issues in this Pull Request. :beers: "")


    comment_body = ''.join(comment_body)


    ## Footer
    comment_footer = []
    if request.json[""action""] == ""opened"":
        comment_footer.append(config[""message""][""opened""][""footer""])
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        comment_footer.append(config[""message""][""updated""][""footer""])

    comment_footer = ''.join(comment_footer)

    return comment_header, comment_body, comment_footer, ERROR


def comment_permission_check(data, comment):
    """"""Check for quite and resume status or duplicate comments""""""
    PERMITTED_TO_COMMENT = True
    repository = data[""repository""]
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Check for duplicate comment
    url = ""https://api.github.com/repos/{}/issues/{}/comments""
    url = url.format(repository, str(data[""pr_number""]))
    comments = requests.get(url, headers=headers, auth=auth).json()

    # Get the last comment by the bot
    last_comment = """"
    for old_comment in reversed(comments):
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment = old_comment[""body""]
            break

    """"""
    # Disabling this because only a single comment is made per PR
    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))
    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))
    if text1 == text2.replace(""submitting"", ""updating""):
        PERMITTED_TO_COMMENT = False
    """"""

    # Check if the bot is asked to keep quiet
    for old_comment in reversed(comments):
        if '@pep8speaks' in old_comment['body']:
            if 'resume' in old_comment['body'].lower():
                break
            elif 'quiet' in old_comment['body'].lower():
                PERMITTED_TO_COMMENT = False


    return PERMITTED_TO_COMMENT


def create_or_update_comment(data, comment):
    comment_mode = None
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    query = ""https://api.github.com/repos/{}/issues/{}/comments""
    query = query.format(data[""repository""], str(data[""pr_number""]))
    comments = requests.get(query, headers=headers, auth=auth).json()

    # Get the last comment id by the bot
    last_comment_id = None
    for old_comment in comments:
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment_id = old_comment[""id""]
            break

    if last_comment_id is None:  # Create a new comment
        response = requests.post(query, json={""body"": comment}, headers=headers, auth=auth)
        data[""comment_response""] = response.json()
    else:  # Update the last comment
        utc_time = datetime.datetime.utcnow()
        time_now = utc_time.strftime(""%B %d, %Y at %H:%M Hours UTC"")
        comment += ""\n\n##### Comment last updated on {}""
        comment = comment.format(time_now)

        query = ""https://api.github.com/repos/{}/issues/comments/{}""
        query = query.format(data[""repository""], str(last_comment_id))
        response = requests.patch(query, json={""body"": comment}, headers=headers, auth=auth)


def autopep8(data, config):
    # Run pycodestyle

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)
    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""diff""][filename] = stdout.decode(r.encoding)

        # Fix the errors
        data[""diff""][filename] = data[""diff""][filename].replace(""file_to_check.py"", filename)
        data[""diff""][filename] = data[""diff""][filename].replace(""\\"", ""\\\\"")

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(data[""repository""], data[""sha""], file)
        os.remove(""file_to_fix.py"")


def create_gist(data, config):
    """"""Create gists for diff files""""""
    REQUEST_JSON = {}
    REQUEST_JSON[""public""] = True
    REQUEST_JSON[""files""] = {}
    REQUEST_JSON[""description""] = ""In response to @{0}'s comment : {1}"".format(
        data[""reviewer""], data[""review_url""])

    for file, diffs in data[""diff""].items():
        if len(diffs) != 0:
            REQUEST_JSON[""files""][file.split(""/"")[-1] + "".diff""] = {
                ""content"": diffs
            }

    # Call github api to create the gist
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/gists""
    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()
    data[""gist_response""] = res
    data[""gist_url""] = res[""html_url""]


def delete_if_forked(data):
    FORKED = False
    url = ""https://api.github.com/user/repos""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    for repo in r.json():
        if repo[""description""]:
            if data[""target_repo_fullname""] in repo[""description""]:
                FORKED = True
                r = requests.delete(""https://api.github.com/repos/""
                                ""{}"".format(repo[""full_name""]),
                                headers=headers, auth=auth)
    return FORKED


def fork_for_pr(data):
    FORKED = False
    url = ""https://api.github.com/repos/{}/forks""
    url = url.format(data[""target_repo_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.post(url, headers=headers, auth=auth)
    if r.status_code == 202:
        data[""fork_fullname""] = r.json()[""full_name""]
        FORKED = True
    else:
        data[""error""] = ""Unable to fork""
    return FORKED


def update_fork_desc(data):
    # Check if forked (takes time)
    url = ""https://api.github.com/repos/{}"".format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    ATTEMPT = 0
    while(r.status_code != 200):
        time.sleep(5)
        r = requests.get(url, headers=headers, auth=auth)
        ATTEMPT += 1
        if ATTEMPT > 10:
            data[""error""] = ""Forking is taking more than usual time""
            break

    full_name = data[""target_repo_fullname""]
    author, name = full_name.split(""/"")
    request_json = {
        ""name"": name,
        ""description"": ""Forked from @{}'s {}"".format(author, full_name)
    }
    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)
    if r.status_code != 200:
        data[""error""] = ""Could not update description of the fork""


def create_new_branch(data):
    url = ""https://api.github.com/repos/{}/git/refs/heads""
    url = url.format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    sha = None
    r = requests.get(url, headers=headers, auth=auth)
    for ref in r.json():
        if ref[""ref""].split(""/"")[-1] == data[""target_repo_branch""]:
            sha = ref[""object""][""sha""]

    url = ""https://api.github.com/repos/{}/git/refs""
    url = url.format(data[""fork_fullname""])
    data[""new_branch""] = ""{}-pep8-patch"".format(data[""target_repo_branch""])
    request_json = {
        ""ref"": ""refs/heads/{}"".format(data[""new_branch""]),
        ""sha"": sha,
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)

    if r.status_code != 200:
        data[""error""] = ""Could not create new branch in the fork""


def autopep8ify(data, config):
    # Run pycodestyle
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)

    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""results""][filename] = stdout.decode(r.encoding)

        os.remove(""file_to_fix.py"")


def commit(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    fullname = data.get(""fork_fullname"")

    for file, new_file in data[""results""].items():
        url = ""https://api.github.com/repos/{}/contents/{}""
        url = url.format(fullname, file)
        params = {""ref"": data[""new_branch""]}
        r = requests.get(url, params=params, headers=headers, auth=auth)
        sha_blob = r.json().get(""sha"")
        params[""path""] = file
        content_code = base64.b64encode(new_file.encode()).decode(""utf-8"")
        request_json = {
            ""path"": file,
            ""message"": ""Fix pep8 errors in {}"".format(file),
            ""content"": content_code,
            ""sha"": sha_blob,
            ""branch"": data.get(""new_branch""),
        }
        r = requests.put(url, json=request_json, headers=headers, auth=auth)


def create_pr(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/repos/{}/pulls""
    url = url.format(data[""target_repo_fullname""])
    request_json = {
        ""title"": ""Fix pep8 errors"",
        ""head"": ""pep8speaks:{}"".format(data[""new_branch""]),
        ""base"": data[""target_repo_branch""],
        ""body"": ""The changes are suggested by autopep8"",
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)
    if r.status_code == 201:
        data[""pr_url""] = r.json()[""html_url""]
    else:
        data[""error""] = ""Pull request could not be created""
/n/n/n",1
18,0cd7d78e4d806852fd75fee03c24cce322f76014,"chippyRuxpin.py/n/n#!/usr/bin/python
# Chippy Ruxpin by Next Thing Co
# Powered by C.H.I.P., the world's first $9 computer!

# apt-get install python-setuptools python-dev build-essential espeak alsa-utils
# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer

# IMPORTANT NOTE ABOUT TWITTER STUFF!
# In order to retrieve tweets, you need to authorize this code to use your twitter account.
# This involves obtaining some special tokens that are specific to you.
# Please visit Twitter's website to obtain this information and put the values in the variables below.
# For more information, visit this URL:
# https://dev.twitter.com/oauth/overview/application-owner-access-tokens

consumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'
consumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'
accessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'
accessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'

import sys
import time
import subprocess
import os
from random import randint
from threading import Thread
from chippyRuxpin_audioPlayer import AudioPlayer
from chippyRuxpin_gpio import GPIO
from chippyRuxpin_twitter import ChippyTwitter
from chippyRuxpin_webFramework import WebFramework

fullMsg = """"

MOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0
MOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2
EYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4
EYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6

io = GPIO() #Establish connection to our GPIO pins.
io.setup( MOUTH_OPEN )
io.setup( EYES_OPEN )
io.setup( MOUTH_CLOSE )
io.setup( EYES_CLOSE )

audio = None
isRunning = True

def updateMouth():
    lastMouthEvent = 0
    lastMouthEventTime = 0

    while( audio == None ):
        time.sleep( 0.1 )
        
    while isRunning:
        if( audio.mouthValue != lastMouthEvent ):
            lastMouthEvent = audio.mouthValue
            lastMouthEventTime = time.time()

            if( audio.mouthValue == 1 ):
                io.set( MOUTH_OPEN, 1 )
                io.set( MOUTH_CLOSE, 0 )
            else:
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 1 )
        else:
            if( time.time() - lastMouthEventTime > 0.4 ):
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 0 )

# A routine for blinking the eyes in a semi-random fashion.
def updateEyes():
    while isRunning:
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 1 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 0 )
        time.sleep( randint( 0,7) )
   
def talk(myText):
    if( myText.find( ""twitter"" ) >= 0 ):
        myText += ""0""
        myText = myText[7:-1]
        try:
	    myText = twitter.getTweet( myText )
	except:
	    print( ""!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."")
            return
    
    os.system( ""espeak \"",...\"" 2>/dev/null"" ) # Sometimes the beginning of audio can get cut off. Insert silence.
    time.sleep( 0.5 )
    subprocess.call([""espeak"", ""-w"", ""speech.wav"", myText, ""-s"", ""130""])
    audio.play(""speech.wav"")
    return myText

mouthThread = Thread(target=updateMouth)
mouthThread.start()
eyesThread = Thread(target=updateEyes)
eyesThread.start()     
audio = AudioPlayer()

if( consumerKey.find( 'TWITTER' ) >= 0 ):
    print( ""WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."" )    
else:
    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)

web = WebFramework(talk)
isRunning = False
io.cleanup()
sys.exit(1)
/n/n/n",0
19,0cd7d78e4d806852fd75fee03c24cce322f76014,"/chippyRuxpin.py/n/n#!/usr/bin/python
# Chippy Ruxpin by Next Thing Co
# Powered by C.H.I.P., the world's first $9 computer!

# apt-get install python-setuptools python-dev build-essential espeak alsa-utils
# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer

# IMPORTANT NOTE ABOUT TWITTER STUFF!
# In order to retrieve tweets, you need to authorize this code to use your twitter account.
# This involves obtaining some special tokens that are specific to you.
# Please visit Twitter's website to obtain this information and put the values in the variables below.
# For more information, visit this URL:
# https://dev.twitter.com/oauth/overview/application-owner-access-tokens

consumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'
consumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'
accessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'
accessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'

import sys
import time
import subprocess
import os
from random import randint
from threading import Thread
from chippyRuxpin_audioPlayer import AudioPlayer
from chippyRuxpin_gpio import GPIO
from chippyRuxpin_twitter import ChippyTwitter
from chippyRuxpin_webFramework import WebFramework

fullMsg = """"

MOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0
MOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2
EYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4
EYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6

io = GPIO() #Establish connection to our GPIO pins.
io.setup( MOUTH_OPEN )
io.setup( EYES_OPEN )
io.setup( MOUTH_CLOSE )
io.setup( EYES_CLOSE )

audio = None
isRunning = True

def updateMouth():
    lastMouthEvent = 0
    lastMouthEventTime = 0

    while( audio == None ):
        time.sleep( 0.1 )
        
    while isRunning:
        if( audio.mouthValue != lastMouthEvent ):
            lastMouthEvent = audio.mouthValue
            lastMouthEventTime = time.time()

            if( audio.mouthValue == 1 ):
                io.set( MOUTH_OPEN, 1 )
                io.set( MOUTH_CLOSE, 0 )
            else:
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 1 )
        else:
            if( time.time() - lastMouthEventTime > 0.4 ):
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 0 )

# A routine for blinking the eyes in a semi-random fashion.
def updateEyes():
    while isRunning:
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 1 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 0 )
        time.sleep( randint( 0,7) )
   
def talk(myText):
    if( myText.find( ""twitter"" ) >= 0 ):
        myText += ""0""
        myText = myText[7:-1]
        try:
	    myText = twitter.getTweet( myText )
	except:
	    print( ""!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."")
            return
    
    os.system( ""espeak \"",...\"" 2>/dev/null"" ) # Sometimes the beginning of audio can get cut off. Insert silence.
    time.sleep( 0.5 )
    os.system( ""espeak -w speech.wav \"""" + myText + ""\"" -s 130"" )
    audio.play(""speech.wav"")
    return myText

mouthThread = Thread(target=updateMouth)
mouthThread.start()
eyesThread = Thread(target=updateEyes)
eyesThread.start()     
audio = AudioPlayer()

if( consumerKey.find( 'TWITTER' ) >= 0 ):
    print( ""WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."" )    
else:
    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)

web = WebFramework(talk)
isRunning = False
io.cleanup()
sys.exit(1)
/n/n/n",1
20,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
21,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
22,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
23,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
24,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
25,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
26,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
27,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
28,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
29,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
30,329c0a8ae6fde575a7d9077f1013fa4a86112d0c,"flex/core.py/n/nfrom __future__ import unicode_literals

from six.moves import urllib_parse as urlparse
import os
import collections
import requests

import six
import json
import yaml

from flex.context_managers import ErrorDict
from flex.exceptions import ValidationError
from flex.loading.definitions import (
    definitions_validator,
)
from flex.loading.schema import (
    swagger_schema_validator,
)
from flex.loading.schema.paths.path_item.operation.responses.single.schema import (
    schema_validator,
)
from flex.http import (
    normalize_request,
    normalize_response,
)
from flex.validation.common import validate_object
from flex.validation.request import validate_request
from flex.validation.response import validate_response


def load_source(source):
    """"""
    Common entry point for loading some form of raw swagger schema.

    Supports:
        - python object (dictionary-like)
        - path to yaml file
        - path to json file
        - file object (json or yaml).
        - json string.
        - yaml string.
    """"""
    if isinstance(source, collections.Mapping):
        return source
    elif hasattr(source, 'read') and callable(source.read):
        raw_source = source.read()
    elif os.path.exists(os.path.expanduser(str(source))):
        with open(os.path.expanduser(str(source)), 'r') as source_file:
            raw_source = source_file.read()
    elif isinstance(source, six.string_types):
        parts = urlparse.urlparse(source)
        if parts.scheme and parts.netloc:
            response = requests.get(source)
            if isinstance(response.content, six.binary_type):
                raw_source = six.text_type(response.content, encoding='utf-8')
            else:
                raw_source = response.content
        else:
            raw_source = source

    try:
        try:
            return json.loads(raw_source)
        except ValueError:
            pass

        try:
            return yaml.safe_load(raw_source)
        except (yaml.scanner.ScannerError, yaml.parser.ParserError):
            pass
    except NameError:
        pass

    raise ValueError(
        ""Unable to parse `{0}`.  Tried yaml and json."".format(source),
    )


def parse(raw_schema):
    context = {
        'deferred_references': set(),
    }
    swagger_definitions = definitions_validator(raw_schema, context=context)

    swagger_schema = swagger_schema_validator(
        raw_schema,
        context=swagger_definitions,
    )
    return swagger_schema


def load(target):
    """"""
    Given one of the supported target formats, load a swagger schema into it's
    python representation.
    """"""
    raw_schema = load_source(target)
    return parse(raw_schema)


def validate(raw_schema, target=None, **kwargs):
    """"""
    Given the python representation of a JSONschema as defined in the swagger
    spec, validate that the schema complies to spec.  If `target` is provided,
    that target will be validated against the provided schema.
    """"""
    schema = schema_validator(raw_schema, **kwargs)
    if target is not None:
        validate_object(target, schema=schema, **kwargs)


def validate_api_request(schema, raw_request):
    request = normalize_request(raw_request)

    with ErrorDict():
        validate_request(request=request, schema=schema)


def validate_api_response(schema, raw_response, request_method='get', raw_request=None):
    """"""
    Validate the response of an api call against a swagger schema.
    """"""
    request = None
    if raw_request is not None:
        request = normalize_request(raw_request)

    response = None
    if raw_response is not None:
        response = normalize_response(raw_response, request=request)

    if response is not None:
        validate_response(
            response=response,
            request_method=request_method,
            schema=schema
        )


def validate_api_call(schema, raw_request, raw_response):
    """"""
    Validate the request/response cycle of an api call against a swagger
    schema.  Request/Response objects from the `requests` and `urllib` library
    are supported.
    """"""
    request = normalize_request(raw_request)

    with ErrorDict() as errors:
        try:
            validate_request(
                request=request,
                schema=schema,
            )
        except ValidationError as err:
            errors['request'].add_error(err.messages or getattr(err, 'detail'))
            return

        response = normalize_response(raw_response, raw_request)

        try:
            validate_response(
                response=response,
                request_method=request.method,
                schema=schema
            )
        except ValidationError as err:
            errors['response'].add_error(err.messages or getattr(err, 'detail'))
/n/n/ntests/core/test_load_source.py/n/nfrom __future__ import unicode_literals

import tempfile
import collections

import six

import json
import yaml

from flex.core import load_source


def test_native_mapping_is_passthrough():
    source = {'foo': 'bar'}
    result = load_source(source)

    assert result == source


def test_json_string():
    native = {'foo': 'bar'}
    source = json.dumps(native)
    result = load_source(source)

    assert result == native


def test_yaml_string():
    native = {b'foo': b'bar'}
    source = yaml.dump(native)
    result = load_source(source)

    assert result == native


def test_json_file_object():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.file.seek(0)

    with open(tmp_file.name) as json_file:
        result = load_source(json_file)

    assert result == native


def test_json_file_path():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_yaml_file_object():
    native = {b'foo': b'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.flush()

    with open(tmp_file.name) as yaml_file:
        result = load_source(yaml_file)

    assert result == native


def test_yaml_file_path():
    native = {b'foo': b'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_url(httpbin):
    native = {
        'origin': '127.0.0.1',
        #'headers': {
        #    'Content-Length': '',
        #    'Accept-Encoding': 'gzip, deflate',
        #    'Host': '127.0.0.1:54634',
        #    'Accept': '*/*',
        #    'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Darwin/14.0.0',
        #    'Connection': 'keep-alive',
        #},
        'args': {},
        #'url': 'http://127.0.0.1:54634/get',
    }
    source = httpbin.url + '/get'
    result = load_source(source)
    assert isinstance(result, collections.Mapping)
    result.pop('headers')
    result.pop('url')
    assert result == native
/n/n/n",0
31,329c0a8ae6fde575a7d9077f1013fa4a86112d0c,"/flex/core.py/n/nfrom __future__ import unicode_literals

from six.moves import urllib_parse as urlparse
import os
import collections
import requests

import six
import json
import yaml

from flex.context_managers import ErrorDict
from flex.exceptions import ValidationError
from flex.loading.definitions import (
    definitions_validator,
)
from flex.loading.schema import (
    swagger_schema_validator,
)
from flex.loading.schema.paths.path_item.operation.responses.single.schema import (
    schema_validator,
)
from flex.http import (
    normalize_request,
    normalize_response,
)
from flex.validation.common import validate_object
from flex.validation.request import validate_request
from flex.validation.response import validate_response


def load_source(source):
    """"""
    Common entry point for loading some form of raw swagger schema.

    Supports:
        - python object (dictionary-like)
        - path to yaml file
        - path to json file
        - file object (json or yaml).
        - json string.
        - yaml string.
    """"""
    if isinstance(source, collections.Mapping):
        return source
    elif hasattr(source, 'read') and callable(source.read):
        raw_source = source.read()
    elif os.path.exists(os.path.expanduser(str(source))):
        with open(os.path.expanduser(str(source)), 'r') as source_file:
            raw_source = source_file.read()
    elif isinstance(source, six.string_types):
        parts = urlparse.urlparse(source)
        if parts.scheme and parts.netloc:
            response = requests.get(source)
            if isinstance(response.content, six.binary_type):
                raw_source = six.text_type(response.content, encoding='utf-8')
            else:
                raw_source = response.content
        else:
            raw_source = source

    try:
        try:
            return json.loads(raw_source)
        except ValueError:
            pass

        try:
            return yaml.load(raw_source)
        except (yaml.scanner.ScannerError, yaml.parser.ParserError):
            pass
    except NameError:
        pass

    raise ValueError(
        ""Unable to parse `{0}`.  Tried yaml and json."".format(source),
    )


def parse(raw_schema):
    context = {
        'deferred_references': set(),
    }
    swagger_definitions = definitions_validator(raw_schema, context=context)

    swagger_schema = swagger_schema_validator(
        raw_schema,
        context=swagger_definitions,
    )
    return swagger_schema


def load(target):
    """"""
    Given one of the supported target formats, load a swagger schema into it's
    python representation.
    """"""
    raw_schema = load_source(target)
    return parse(raw_schema)


def validate(raw_schema, target=None, **kwargs):
    """"""
    Given the python representation of a JSONschema as defined in the swagger
    spec, validate that the schema complies to spec.  If `target` is provided,
    that target will be validated against the provided schema.
    """"""
    schema = schema_validator(raw_schema, **kwargs)
    if target is not None:
        validate_object(target, schema=schema, **kwargs)


def validate_api_request(schema, raw_request):
    request = normalize_request(raw_request)

    with ErrorDict():
        validate_request(request=request, schema=schema)


def validate_api_response(schema, raw_response, request_method='get', raw_request=None):
    """"""
    Validate the response of an api call against a swagger schema.
    """"""
    request = None
    if raw_request is not None:
        request = normalize_request(raw_request)

    response = None
    if raw_response is not None:
        response = normalize_response(raw_response, request=request)

    if response is not None:
        validate_response(
            response=response,
            request_method=request_method,
            schema=schema
        )


def validate_api_call(schema, raw_request, raw_response):
    """"""
    Validate the request/response cycle of an api call against a swagger
    schema.  Request/Response objects from the `requests` and `urllib` library
    are supported.
    """"""
    request = normalize_request(raw_request)

    with ErrorDict() as errors:
        try:
            validate_request(
                request=request,
                schema=schema,
            )
        except ValidationError as err:
            errors['request'].add_error(err.messages or getattr(err, 'detail'))
            return

        response = normalize_response(raw_response, raw_request)

        try:
            validate_response(
                response=response,
                request_method=request.method,
                schema=schema
            )
        except ValidationError as err:
            errors['response'].add_error(err.messages or getattr(err, 'detail'))
/n/n/n/tests/core/test_load_source.py/n/nfrom __future__ import unicode_literals

import tempfile
import collections

import six

import json
import yaml

from flex.core import load_source


def test_native_mapping_is_passthrough():
    source = {'foo': 'bar'}
    result = load_source(source)

    assert result == source


def test_json_string():
    native = {'foo': 'bar'}
    source = json.dumps(native)
    result = load_source(source)

    assert result == native


def test_yaml_string():
    native = {'foo': 'bar'}
    source = yaml.dump(native)
    result = load_source(source)

    assert result == native


def test_json_file_object():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.file.seek(0)

    with open(tmp_file.name) as json_file:
        result = load_source(json_file)

    assert result == native


def test_json_file_path():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_yaml_file_object():
    native = {'foo': 'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.flush()

    with open(tmp_file.name) as yaml_file:
        result = load_source(yaml_file)

    assert result == native


def test_yaml_file_path():
    native = {'foo': 'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_url(httpbin):
    native = {
        'origin': '127.0.0.1',
        #'headers': {
        #    'Content-Length': '',
        #    'Accept-Encoding': 'gzip, deflate',
        #    'Host': '127.0.0.1:54634',
        #    'Accept': '*/*',
        #    'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Darwin/14.0.0',
        #    'Connection': 'keep-alive',
        #},
        'args': {},
        #'url': 'http://127.0.0.1:54634/get',
    }
    source = httpbin.url + '/get'
    result = load_source(source)
    assert isinstance(result, collections.Mapping)
    result.pop('headers')
    result.pop('url')
    assert result == native
/n/n/n",1
32,f453ed1c417993eab4fc7b3c5288208d97270d13,"ptvsd/__main__.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import argparse
import os.path
import sys

from ptvsd._local import debug_main, run_main
from ptvsd.socket import Address
from ptvsd.version import __version__, __author__  # noqa


##################################
# the script

""""""
For the PyDevd CLI handling see:

  https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py
  https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py#L1450  (main func)
""""""  # noqa

PYDEVD_OPTS = {
    '--file',
    '--client',
    #'--port',
    '--vm_type',
}

PYDEVD_FLAGS = {
    '--DEBUG',
    '--DEBUG_RECORD_SOCKET_READS',
    '--cmd-line',
    '--module',
    '--multiproc',
    '--multiprocess',
    '--print-in-debugger-startup',
    '--save-signatures',
    '--save-threading',
    '--save-asyncio',
    '--server',
    '--qt-support=auto',
}

USAGE = """"""
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT -m MODULE [arg ...]
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT FILENAME [arg ...]
""""""  # noqa


def parse_args(argv=None):
    """"""Return the parsed args to use in main().""""""
    if argv is None:
        argv = sys.argv
        prog = argv[0]
        if prog == __file__:
            prog = '{} -m ptvsd'.format(os.path.basename(sys.executable))
    else:
        prog = argv[0]
    argv = argv[1:]

    supported, pydevd, script = _group_args(argv)
    args = _parse_args(prog, supported)
    # '--' is used in _run_args to extract pydevd specific args
    extra = pydevd + ['--']
    if script:
        extra += script
    return args, extra


def _group_args(argv):
    supported = []
    pydevd = []
    script = []

    try:
        pos = argv.index('--')
    except ValueError:
        script = []
    else:
        script = argv[pos + 1:]
        argv = argv[:pos]

    for arg in argv:
        if arg == '-h' or arg == '--help':
            return argv, [], script

    gottarget = False
    skip = 0
    for i in range(len(argv)):
        if skip:
            skip -= 1
            continue

        arg = argv[i]
        try:
            nextarg = argv[i + 1]
        except IndexError:
            nextarg = None

        # TODO: Deprecate the PyDevd arg support.
        # PyDevd support
        if gottarget:
            script = argv[i:] + script
            break
        if arg == '--client':
            arg = '--host'
        elif arg == '--file':
            if nextarg is None:  # The filename is missing...
                pydevd.append(arg)
                continue  # This will get handled later.
            if nextarg.endswith(':') and '--module' in pydevd:
                pydevd.remove('--module')
                arg = '-m'
                argv[i + 1] = nextarg = nextarg[:-1]
            else:
                arg = nextarg
                skip += 1

        if arg in PYDEVD_OPTS:
            pydevd.append(arg)
            if nextarg is not None:
                pydevd.append(nextarg)
            skip += 1
        elif arg in PYDEVD_FLAGS:
            pydevd.append(arg)
        elif arg == '--nodebug':
            supported.append(arg)

        # ptvsd support
        elif arg in ('--host', '--server-host', '--port', '-m'):
            if arg == '-m':
                gottarget = True
            supported.append(arg)
            if nextarg is not None:
                supported.append(nextarg)
            skip += 1
        elif arg in ('--single-session', '--wait'):
            supported.append(arg)
        elif not arg.startswith('-'):
            supported.append(arg)
            gottarget = True

        # unsupported arg
        else:
            supported.append(arg)
            break

    return supported, pydevd, script


def _parse_args(prog, argv):
    parser = argparse.ArgumentParser(
        prog=prog,
        usage=USAGE.format(prog),
    )
    parser.add_argument('--nodebug', action='store_true')
    host = parser.add_mutually_exclusive_group()
    host.add_argument('--host')
    host.add_argument('--server-host')
    parser.add_argument('--port', type=int, required=True)

    target = parser.add_mutually_exclusive_group(required=True)
    target.add_argument('-m', dest='module')
    target.add_argument('filename', nargs='?')

    parser.add_argument('--single-session', action='store_true')
    parser.add_argument('--wait', action='store_true')

    parser.add_argument('-V', '--version', action='version')
    parser.version = __version__

    args = parser.parse_args(argv)
    ns = vars(args)

    serverhost = ns.pop('server_host', None)
    clienthost = ns.pop('host', None)
    if serverhost:
        args.address = Address.as_server(serverhost, ns.pop('port'))
    elif not clienthost:
        if args.nodebug:
            args.address = Address.as_client(clienthost, ns.pop('port'))
        else:
            args.address = Address.as_server(clienthost, ns.pop('port'))
    else:
        args.address = Address.as_client(clienthost, ns.pop('port'))

    module = ns.pop('module')
    filename = ns.pop('filename')
    if module is None:
        args.name = filename
        args.kind = 'script'
    else:
        args.name = module
        args.kind = 'module'
    #if argv[-1] != args.name or (module and argv[-1] != '-m'):
    #    parser.error('script/module must be last arg')

    return args


def main(addr, name, kind, extra=(), nodebug=False, **kwargs):
    if nodebug:
        run_main(addr, name, kind, *extra, **kwargs)
    else:
        debug_main(addr, name, kind, *extra, **kwargs)


if __name__ == '__main__':
    args, extra = parse_args()
    main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,
         singlesession=args.single_session, wait=args.wait)
/n/n/nptvsd/_local.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import sys
import time

import pydevd
from _pydevd_bundle.pydevd_comm import get_global_debugger

from ptvsd.pydevd_hooks import install
from ptvsd.runner import run as no_debug_runner
from ptvsd.socket import Address
from ptvsd._util import new_hidden_thread


PYDEVD_DEFAULTS = {
    '--qt-support=auto',
}


def _set_pydevd_defaults(pydevd_args):
    args_to_append = []
    for arg in PYDEVD_DEFAULTS:
        if arg not in pydevd_args:
            args_to_append.append(arg)
    return pydevd_args + args_to_append


########################
# high-level functions

def debug_main(address, name, kind, *extra, **kwargs):
    if not kwargs.pop('wait', False) and address.isserver:
        def unblock_debugger():
            debugger = get_global_debugger()
            while debugger is None:
                time.sleep(0.1)
                debugger = get_global_debugger()
            debugger.ready_to_run = True
        new_hidden_thread('ptvsd.unblock_debugger', unblock_debugger).start()
    if kind == 'module':
        run_module(address, name, *extra, **kwargs)
    else:
        run_file(address, name, *extra, **kwargs)


def run_main(address, name, kind, *extra, **kwargs):
    addr = Address.from_raw(address)
    sys.argv[:] = _run_main_argv(name, extra)
    runner = kwargs.pop('_runner', no_debug_runner)
    runner(addr, name, kind == 'module', *extra, **kwargs)


########################
# low-level functions

def run_module(address, modname, *extra, **kwargs):
    """"""Run pydevd for the given module.""""""
    addr = Address.from_raw(address)
    if not addr.isserver:
        kwargs['singlesession'] = True
    run = kwargs.pop('_run', _run)
    prog = kwargs.pop('_prog', sys.argv[0])
    filename = modname + ':'
    argv = _run_argv(addr, filename, extra, _prog=prog)
    argv.insert(argv.index('--file'), '--module')
    run(argv, addr, **kwargs)


def run_file(address, filename, *extra, **kwargs):
    """"""Run pydevd for the given Python file.""""""
    addr = Address.from_raw(address)
    if not addr.isserver:
        kwargs['singlesession'] = True
    run = kwargs.pop('_run', _run)
    prog = kwargs.pop('_prog', sys.argv[0])
    argv = _run_argv(addr, filename, extra, _prog=prog)
    run(argv, addr, **kwargs)


def _run_argv(address, filename, extra, _prog=sys.argv[0]):
    """"""Convert the given values to an argv that pydevd.main() supports.""""""
    if '--' in extra:
        pydevd = list(extra[:extra.index('--')])
        extra = list(extra[len(pydevd) + 1:])
    else:
        pydevd = []
        extra = list(extra)

    pydevd = _set_pydevd_defaults(pydevd)
    host, port = address
    argv = [
        _prog,
        '--port', str(port),
    ]
    if not address.isserver:
        argv.extend([
            '--client', host or 'localhost',
        ])
    return argv + pydevd + [
        '--file', filename,
    ] + extra


def _run_main_argv(filename, extra):
    if '--' in extra:
        pydevd = list(extra[:extra.index('--')])
        extra = list(extra[len(pydevd) + 1:])
    else:
        extra = list(extra)
    return [filename] + extra


def _run(argv, addr, _pydevd=pydevd, _install=install, **kwargs):
    """"""Start pydevd with the given commandline args.""""""
    #print(' '.join(argv))

    # Pydevd assumes that the ""__main__"" module is the ""pydevd"" module
    # and does some tricky stuff under that assumption.  For example,
    # when the debugger starts up it calls save_main_module()
    # (in pydevd_bundle/pydevd_utils.py).  That function explicitly sets
    # sys.modules[""pydevd""] to sys.modules[""__main__""] and then sets
    # the __main__ module to a new one.  This makes some sense since
    # it gives the debugged script a fresh __main__ module.
    #
    # This complicates things for us since we are running a different
    # file (i.e. this one) as the __main__ module.  Consequently,
    # sys.modules[""pydevd""] gets set to ptvsd/__main__.py.  Subsequent
    # imports of the ""pydevd"" module then return the wrong module.  We
    # work around this by avoiding lazy imports of the ""pydevd"" module.
    # We also replace the __main__ module with the ""pydevd"" module here.
    if sys.modules['__main__'].__file__ != _pydevd.__file__:
        sys.modules['__main___orig'] = sys.modules['__main__']
        sys.modules['__main__'] = _pydevd

    daemon = _install(_pydevd, addr, **kwargs)
    sys.argv[:] = argv
    try:
        _pydevd.main()
    except SystemExit as ex:
        daemon.exitcode = int(ex.code)
        raise
/n/n/nptvsd/_remote.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import pydevd
import time

from _pydevd_bundle.pydevd_comm import get_global_debugger

from ptvsd._util import new_hidden_thread
from ptvsd.pydevd_hooks import install
from ptvsd.daemon import session_not_bound, DaemonClosedError


def _pydevd_settrace(redirect_output=None, _pydevd=pydevd, **kwargs):
    if redirect_output is not None:
        kwargs.setdefault('stdoutToServer', redirect_output)
        kwargs.setdefault('stderrToServer', redirect_output)
    # pydevd.settrace() only enables debugging of the current
    # thread and all future threads.  PyDevd is not enabled for
    # existing threads (other than the current one).  Consequently,
    # pydevd.settrace() must be called ASAP in the current thread.
    # See issue #509.
    #
    # This is tricky, however, because settrace() will block until
    # it receives a CMD_RUN message.  You can't just call it in a
    # thread to avoid blocking; doing so would prevent the current
    # thread from being debugged.
    _pydevd.settrace(**kwargs)


# TODO: Split up enable_attach() to align with module organization.
# This should including making better use of Daemon (e,g, the
# start_server() method).
# Then move at least some parts to the appropriate modules.  This module
# is focused on running the debugger.

global_next_session = None


def enable_attach(address, redirect_output=True,
                  _pydevd=pydevd, _install=install,
                  on_attach=lambda: None, **kwargs):
    host, port = address

    def wait_for_connection(daemon, host, port, next_session=None):
        debugger = get_global_debugger()
        while debugger is None:
            time.sleep(0.1)
            debugger = get_global_debugger()

        debugger.ready_to_run = True

        while True:
            session_not_bound.wait()
            try:
                global_next_session()
                on_attach()
            except DaemonClosedError:
                return

    def start_daemon():
        daemon._sock = daemon._start()
        _, next_session = daemon.start_server(addr=(host, port))
        global global_next_session
        global_next_session = next_session
        return daemon._sock

    daemon = _install(_pydevd,
                      address,
                      start_server=None,
                      start_client=(lambda daemon, h, port: start_daemon()),
                      singlesession=False,
                      **kwargs)

    connection_thread = new_hidden_thread('ptvsd.listen_for_connection',
                                          wait_for_connection,
                                          args=(daemon, host, port))
    connection_thread.start()

    _pydevd.settrace(host=host,
                     stdoutToServer=redirect_output,
                     stderrToServer=redirect_output,
                     port=port,
                     suspend=False)
/n/n/nptvsd/_util.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

from __future__ import print_function

import contextlib
import os
import threading
import time
import sys


DEBUG = False
if os.environ.get('PTVSD_DEBUG', ''):
    DEBUG = True


def debug(*msg, **kwargs):
    if not DEBUG:
        return
    tb = kwargs.pop('tb', False)
    assert not kwargs
    if tb:
        import traceback
        traceback.print_exc()
    print(*msg, file=sys.stderr)
    sys.stderr.flush()


@contextlib.contextmanager
def ignore_errors(log=None):
    """"""A context manager that masks any raised exceptions.""""""
    try:
        yield
    except Exception as exc:
        if log is not None:
            log('ignoring error', exc)


def call_all(callables, *args, **kwargs):
    """"""Return the result of calling every given object.""""""
    results = []
    for call in callables:
        try:
            call(*args, **kwargs)
        except Exception as exc:
            results.append((call, exc))
        else:
            results.append((call, None))
    return results


########################
# pydevd stuff

from _pydevd_bundle import pydevd_comm  # noqa


def log_pydevd_msg(cmdid, seq, args, inbound,
                   log=debug, prefix=None, verbose=False):
    """"""Log a representation of the given pydevd msg.""""""
    if log is None or (log is debug and not DEBUG):
        return
    if not verbose and cmdid == pydevd_comm.CMD_WRITE_TO_CONSOLE:
        return

    if prefix is None:
        prefix = '-> ' if inbound else '<- '
    try:
        cmdname = pydevd_comm.ID_TO_MEANING[str(cmdid)]
    except KeyError:
        for cmdname, value in vars(pydevd_comm).items():
            if cmdid == value:
                break
        else:
            cmdname = '???'
    cmd = '{} ({})'.format(cmdid, cmdname)
    args = args.replace('\n', '\\n')
    msg = '{}{:28} [{:>10}]: |{}|'.format(prefix, cmd, seq, args)
    log(msg)


########################
# threading stuff

try:
    ThreadError = threading.ThreadError
except AttributeError:
    ThreadError = RuntimeError


try:
    base = __builtins__.TimeoutError
except AttributeError:
    base = OSError
class TimeoutError(base):  # noqa
    """"""Timeout expired.""""""
    timeout = None
    reason = None

    @classmethod
    def from_timeout(cls, timeout, reason=None):
        """"""Return a TimeoutError with the given timeout.""""""
        msg = 'timed out (after {} seconds)'.format(timeout)
        if reason is not None:
            msg += ' ' + reason
        self = cls(msg)
        self.timeout = timeout
        self.reason = reason
        return self
del base  # noqa


def wait(check, timeout=None, reason=None):
    """"""Wait for the given func to return True.

    If a timeout is given and reached then raise TimeoutError.
    """"""
    if timeout is None or timeout <= 0:
        while not check():
            time.sleep(0.01)
    else:
        if not _wait(check, timeout):
            raise TimeoutError.from_timeout(timeout, reason)


def is_locked(lock):
    """"""Return True if the lock is locked.""""""
    if lock is None:
        return False
    if not lock.acquire(False):
        return True
    lock_release(lock)
    return False


def lock_release(lock):
    """"""Ensure that the lock is released.""""""
    if lock is None:
        return
    try:
        lock.release()
    except ThreadError:  # already unlocked
        pass


def lock_wait(lock, timeout=None, reason='waiting for lock'):
    """"""Wait until the lock is not locked.""""""
    if not _lock_acquire(lock, timeout):
        raise TimeoutError.from_timeout(timeout, reason)
    lock_release(lock)


if sys.version_info >= (3,):
    def _lock_acquire(lock, timeout):
        if timeout is None:
            timeout = -1
        return lock.acquire(timeout=timeout)
else:
    def _lock_acquire(lock, timeout):
        if timeout is None or timeout <= 0:
            return lock.acquire()

        def check():
            return lock.acquire(False)
        return _wait(check, timeout)


def _wait(check, timeout):
    if check():
        return True
    for _ in range(int(timeout * 100)):
        time.sleep(0.01)
        if check():
            return True
    else:
        return False


def new_hidden_thread(name, target, prefix='ptvsd.', daemon=True, **kwargs):
    """"""Return a thread that will be ignored by pydevd.""""""
    if prefix is not None and not name.startswith(prefix):
        name = prefix + name
    t = threading.Thread(
        name=name,
        target=target,
        **kwargs
    )
    t.pydev_do_not_trace = True
    if daemon:
        t.is_pydev_daemon_thread = True
        t.daemon = True
    return t


########################
# closing stuff

class ClosedError(RuntimeError):
    """"""Indicates that the object is closed.""""""


def close_all(closeables):
    """"""Return the result of closing every given object.""""""
    results = []
    for obj in closeables:
        try:
            obj.close()
        except Exception as exc:
            results.append((obj, exc))
        else:
            results.append((obj, None))
    return results


class Closeable(object):
    """"""A base class for types that may be closed.""""""

    NAME = None
    FAIL_ON_ALREADY_CLOSED = True

    def __init__(self):
        super(Closeable, self).__init__()
        self._closed = False
        self._closedlock = threading.Lock()
        self._handlers = []

    def __del__(self):
        try:
            self.close()
        except ClosedError:
            pass

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()

    @property
    def closed(self):
        return self._closed

    def add_resource_to_close(self, resource, before=False):
        """"""Add a resource to be closed when closing.""""""
        close = resource.close
        if before:
            def handle_closing(before):
                if not before:
                    return
                close()
        else:
            def handle_closing(before):
                if before:
                    return
                close()
        self.add_close_handler(handle_closing)

    def add_close_handler(self, handle_closing, nodupe=True):
        """"""Add a func to be called when closing.

        The func takes one arg: True if it was called before the main
        close func and False if after.
        """"""
        with self._closedlock:
            if self._closed:
                if self.FAIL_ON_ALREADY_CLOSED:
                    raise ClosedError('already closed')
                return
            if nodupe and handle_closing in self._handlers:
                raise ValueError('close func already added')

            self._handlers.append(handle_closing)

    def check_closed(self):
        """"""Raise ClosedError if closed.""""""
        if self._closed:
            if self.NAME:
                raise ClosedError('{} closed'.format(self.NAME))
            else:
                raise ClosedError('closed')

    @contextlib.contextmanager
    def while_not_closed(self):
        """"""A context manager under which the object will not be closed.""""""
        with self._closedlock:
            self.check_closed()
            yield

    def close(self):
        """"""Release any owned resources and clean up.""""""
        with self._closedlock:
            if self._closed:
                if self.FAIL_ON_ALREADY_CLOSED:
                    raise ClosedError('already closed')
                return
            self._closed = True
            handlers = list(self._handlers)

        results = call_all(handlers, True)
        self._log_results(results)
        self._close()
        results = call_all(handlers, False)
        self._log_results(results)

    # implemented by subclasses

    def _close(self):
        pass

    # internal methods

    def _log_results(self, results, log=None):
        if log is None:
            return
        for obj, exc in results:
            if exc is None:
                continue
            log('failed to close {!r} ({!r})'.format(obj, exc))


########################
# running stuff

class NotRunningError(RuntimeError):
    """"""Something isn't currently running.""""""


class AlreadyStartedError(RuntimeError):
    """"""Something was already started.""""""


class AlreadyRunningError(AlreadyStartedError):
    """"""Something is already running.""""""


class Startable(object):
    """"""A base class for types that may be started.""""""

    RESTARTABLE = False
    FAIL_ON_ALREADY_STOPPED = True

    def __init__(self):
        super(Startable, self).__init__()
        self._is_running = None
        self._startlock = threading.Lock()
        self._numstarts = 0

    def is_running(self, checkclosed=True):
        """"""Return True if currently running.""""""
        if checkclosed and hasattr(self, 'check_closed'):
            self.check_closed()
        is_running = self._is_running
        if is_running is None:
            return False
        return is_running()

    def start(self, *args, **kwargs):
        """"""Begin internal execution.""""""
        with self._startlock:
            if self.is_running():
                raise AlreadyRunningError()
            if not self.RESTARTABLE and self._numstarts > 0:
                raise AlreadyStartedError()

            self._is_running = self._start(*args, **kwargs)
            self._numstarts += 1

    def stop(self, *args, **kwargs):
        """"""Stop execution and wait until done.""""""
        with self._startlock:
            # TODO: Call self.check_closed() here?
            if not self.is_running(checkclosed=False):
                if not self.FAIL_ON_ALREADY_STOPPED:
                    return
                raise NotRunningError()
            self._is_running = None

        self._stop(*args, **kwargs)

    # implemented by subclasses

    def _start(self, *args, **kwargs):
        """"""Return an ""is_running()"" func after starting.""""""
        raise NotImplementedError

    def _stop(self):
        raise NotImplementedError


def is_py34():
    return sys.version_info >= (3, 4,) and sys.version_info < (3, 5,)


def get_line_for_traceback(file_path, line_no):
    try:
        with open(file_path, 'r') as f:
            return f.readlines()[line_no - 1]
    except Exception:
        return None


_enable_debug_break = False


def _allow_debug_break(enabled=True):
    """"""Enable breaking into debugger feature.
    """"""
    global _enable_debug_break
    _enable_debug_break = enabled


def _is_debug_break_allowed():
    return _enable_debug_break
/n/n/nptvsd/daemon.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import contextlib
import sys
import threading

from ptvsd import wrapper
from ptvsd.socket import (
    close_socket, create_server, create_client, connect, Address)
from .exit_handlers import (
    ExitHandlers, UnsupportedSignalError,
    kill_current_proc)
from .session import PyDevdDebugSession
from ._util import (
    ClosedError, NotRunningError, ignore_errors, debug, lock_wait)


session_not_bound = threading.Event()
session_not_bound.set()


def _wait_for_user():
    if sys.__stdout__ is not None:
        try:
            import msvcrt
        except ImportError:
            sys.__stdout__.write('Press Enter to continue . . . ')
            sys.__stdout__.flush()
            sys.__stdin__.read(1)
        else:
            sys.__stdout__.write('Press any key to continue . . . ')
            sys.__stdout__.flush()
            msvcrt.getch()


class DaemonError(RuntimeError):
    """"""Indicates that a Daemon had a problem.""""""
    MSG = 'error'

    def __init__(self, msg=None):
        if msg is None:
            msg = self.MSG
        super(DaemonError, self).__init__(msg)


class DaemonClosedError(DaemonError):
    """"""Indicates that a Daemon was unexpectedly closed.""""""
    MSG = 'closed'


class DaemonStoppedError(DaemonError):
    """"""Indicates that a Daemon was unexpectedly stopped.""""""
    MSG = 'stopped'


# TODO: Inherit from Closeable.
# TODO: Inherit from Startable?

class DaemonBase(object):
    """"""The base class for DAP daemons.""""""

    SESSION = None

    exitcode = None

    def __init__(self, wait_for_user=_wait_for_user,
                 addhandlers=True, killonclose=True,
                 singlesession=False):

        self._lock = threading.Lock()
        self._started = False
        self._stopped = False
        self._closed = False

        # socket-related

        self._sock = None  # set when started
        self._server = None

        # session-related

        self._singlesession = singlesession

        self._session = None
        self._numsessions = 0
        self._sessionlock = None

        # proc-related

        self._wait_for_user = wait_for_user
        self._killonclose = killonclose

        self._exiting_via_atexit_handler = False

        self._exithandlers = ExitHandlers()
        if addhandlers:
            self._install_exit_handlers()

    @property
    def session(self):
        """"""The current session.""""""
        return self._session

    @contextlib.contextmanager
    def started(self):
        """"""A context manager that starts the daemon and stops it for errors.""""""
        self.start()
        try:
            yield self
        except Exception:
            self._stop_quietly()
            raise

    @contextlib.contextmanager
    def running(self):
        """"""A context manager that starts the daemon.

        If there's a failure then the daemon is stopped.  It is also
        stopped at the end of the with block.
        """"""
        self.start()
        try:
            yield self
        finally:
            self._stop_quietly()

    def is_running(self):
        """"""Return True if the daemon is running.""""""
        with self._lock:
            if self._closed:
                return False
            if self._sock is None:
                return False
            return self._started and not self._stopped

    def start(self):
        """"""Return the ""socket"" to use for pydevd after setting it up.""""""
        with self._lock:
            if self._closed:
                raise DaemonClosedError()
            if self._started:
                raise RuntimeError('already started')
            self._started = True

        sock = self._start()
        self._sock = sock
        return sock

    def start_server(self, addr, hidebadsessions=True):
        """"""Return (""socket"", next_session) with a new server socket.""""""
        addr = Address.from_raw(addr)
        with self.started():
            assert self._sessionlock is None
            assert self.session is None
            self._server = create_server(addr.host, addr.port)
            debug('server socket created')
            self._sessionlock = threading.Lock()
        sock = self._sock

        def check_ready(**kwargs):
            self._check_ready_for_session(**kwargs)
            if self._server is None:
                raise DaemonStoppedError()

        def next_session(timeout=None, **kwargs):
            server = self._server
            sessionlock = self._sessionlock
            check_ready(checksession=False)

            debug('getting next session')
            sessionlock.acquire()  # Released in _finish_session().
            debug('session lock acquired')
            # It may have closed or stopped while we waited.
            check_ready()

            timeout = kwargs.pop('timeout', None)
            try:
                debug('getting session socket')
                client = connect(server, None, **kwargs)
                self._bind_session(client)
                debug('starting session')
                self._start_session_safely('ptvsd.Server', timeout=timeout)
                debug('session started')
                return self._session
            except Exception as exc:
                debug('session exc:', exc, tb=True)
                with ignore_errors():
                    self._finish_session()
                if hidebadsessions:
                    debug('hiding bad session')
                    # TODO: Log the error?
                    return None
                self._stop_quietly()
                raise

        return sock, next_session

    def start_client(self, addr):
        """"""Return (""socket"", start_session) with a new client socket.""""""
        addr = Address.from_raw(addr)
        self._singlesession = True
        with self.started():
            assert self.session is None
            client = create_client()
            connect(client, addr)
        sock = self._sock

        def start_session(**kwargs):
            self._check_ready_for_session()
            if self._server is not None:
                raise RuntimeError('running as server')
            if self._numsessions:
                raise RuntimeError('session stopped')

            try:
                self._bind_session(client)
                self._start_session_safely('ptvsd.Client', **kwargs)
                return self._session
            except Exception:
                self._stop_quietly()
                raise

        return sock, start_session

    def start_session(self, session, threadname, **kwargs):
        """"""Start the debug session and remember it.

        If ""session"" is a client socket then a session is created
        from it.
        """"""
        self._check_ready_for_session()
        if self._server is not None:
            raise RuntimeError('running as server')

        self._bind_session(session)
        self._start_session_safely(threadname, **kwargs)
        return self.session

    def close(self):
        """"""Stop all loops and release all resources.""""""
        with self._lock:
            if self._closed:
                raise DaemonClosedError('already closed')
            self._closed = True

        self._close()

    # internal methods

    def _check_ready_for_session(self, checksession=True):
        with self._lock:
            if self._closed:
                raise DaemonClosedError()
            if not self._started:
                raise DaemonStoppedError('never started')
            if self._stopped or self._sock is None:
                raise DaemonStoppedError()
            if checksession and self.session is not None:
                raise RuntimeError('session already started')

    def _close(self):
        self._stop()

        self._sock = None

    def _stop(self):
        with self._lock:
            if self._stopped:
                return
            self._stopped = True

        server = self._server
        self._server = None

        with ignore_errors():
            self._finish_session()

        self._sessionlock = None  # TODO: Call self._clear_sessionlock?

        # TODO: Close the server socket *before* finish the session?
        if server is not None:
            with ignore_errors():
                close_socket(server)

        # TODO: Close self._sock *before* finishing the session?
        if self._sock is not None:
            with ignore_errors():
                close_socket(self._sock)

    def _stop_quietly(self):
        with ignore_errors():
            self._stop()

    def _handle_session_disconnecting(self, session):
        debug('handling disconnecting session')
        if self._singlesession:
            if self._killonclose:
                with self._lock:
                    if not self._exiting_via_atexit_handler:
                        # Ensure the proc is exiting before closing
                        # socket.  Note that we kill the proc instead
                        # of calling sys.exit(0).
                        # Note that this will trigger either the atexit
                        # handler or the signal handler.
                        kill_current_proc()
            else:
                try:
                    self.close()
                except DaemonClosedError:
                    pass

    def _handle_session_closing(self, session):
        debug('handling closing session')

        if self._singlesession:
            if self._killonclose:
                with self._lock:
                    if not self._exiting_via_atexit_handler:
                        # Ensure the proc is exiting before closing
                        # socket.  Note that we kill the proc instead
                        # of calling sys.exit(0).
                        # Note that this will trigger either the atexit
                        # handler or the signal handler.
                        kill_current_proc()
            else:
                try:
                    self.close()
                except DaemonClosedError:
                    pass
        else:
            self._finish_session()

    def _clear_sessionlock(self, done=False):
        sessionlock = self._sessionlock
        if done:
            self._sessionlock = None
        if sessionlock is not None:
            try:
                sessionlock.release()
            except Exception:  # TODO: Make it more specific?
                debug('session lock not released')
            else:
                debug('session lock released')

    # internal session-related methods

    def _bind_session(self, session):
        session_not_bound.clear()
        # TODO: Pass notify_* to session.start() instead.
        session = self.SESSION.from_raw(
            session,
            notify_closing=self._handle_session_closing,
            notify_disconnecting=self._handle_session_disconnecting,
            ownsock=True,
            **self._session_kwargs() or {}
        )
        self._session = session
        self._numsessions += 1

    def _start_session_safely(self, threadname, **kwargs):
        try:
            self._start_session(threadname, **kwargs)
        except Exception:
            with ignore_errors():
                self._finish_session()
            raise

    def _finish_session(self):
        self._numsessions -= 1
        session_not_bound.set()
        try:
            session = self._release_session()
            debug('session stopped')
        finally:
            self._clear_sessionlock()

            if self._singlesession:
                debug('closing daemon after single session')
                try:
                    self.close()
                except DaemonClosedError:
                    pass
        return session

    def _release_session(self):
        session = self.session
        if not self._singlesession:
            # TODO: This shouldn't happen if we are exiting?
            self._session = None

        try:
            session.stop()
        except NotRunningError:
            pass
        try:
            session.close()
        except ClosedError:
            pass

        return session

    # internal proc-related methods

    def _install_exit_handlers(self):
        """"""Set the placeholder handlers.""""""
        self._exithandlers.install()

        try:
            self._exithandlers.add_atexit_handler(self._handle_atexit)
        except ValueError:
            pass
        for signum in self._exithandlers.SIGNALS:
            try:
                self._exithandlers.add_signal_handler(signum,
                                                      self._handle_signal)
            except ValueError:
                # Already added.
                pass
            except UnsupportedSignalError:
                # TODO: This shouldn't happen.
                pass

    def _handle_atexit(self):
        debug('handling atexit')
        with self._lock:
            self._exiting_via_atexit_handler = True
        session = self.session

        if session is not None:
            lock = threading.Lock()
            lock.acquire()

            def wait_debugger(timeout=None):
                lock_wait(lock, timeout)

            def wait_exiting(cfg):
                if cfg:
                    self._wait_for_user()
                lock.release()
            # TODO: Rely on self._stop_debugger().
            session.handle_debugger_stopped(wait_debugger)
            session.handle_exiting(self.exitcode, wait_exiting)

        try:
            self.close()
        except DaemonClosedError:
            pass
        if session is not None:
            session.wait_until_stopped()

    def _handle_signal(self, signum, frame):
        debug('handling signal')
        try:
            self.close()
        except DaemonClosedError:
            pass
        if not self._exiting_via_atexit_handler:
            sys.exit(0)

    # methods for subclasses to override

    def _start(self):
        """"""Return the debugger client socket after starting the daemon.""""""
        raise NotImplementedError

    def _start_session(self, threadname, **kwargs):
        self.session.start(
            threadname,
            **kwargs
        )

    def _session_kwargs(self):
        return None


class Daemon(DaemonBase):
    """"""The process-level manager for the VSC protocol debug adapter.""""""

    SESSION = PyDevdDebugSession

    def __init__(self, wait_for_user=_wait_for_user,
                 notify_session_debugger_ready=None,
                 **kwargs):
        super(Daemon, self).__init__(wait_for_user, **kwargs)

        self._notify_session_debugger_ready = notify_session_debugger_ready

    @property
    def pydevd(self):
        return self._sock

    # internal methods

    def _start(self):
        return wrapper.PydevdSocket(
            self._handle_pydevd_message,
            self._handle_pydevd_close,
            self._getpeername,
            self._getsockname,
        )

    def _start_session(self, threadname, **kwargs):
        super(Daemon, self)._start_session(
            threadname,
            pydevd_notify=self.pydevd.pydevd_notify,
            pydevd_request=self.pydevd.pydevd_request,
            **kwargs
        )

    def _session_kwargs(self):
        def debugger_ready(session):
            if self._notify_session_debugger_ready is not None:
                self._notify_session_debugger_ready(session)

        return dict(
            notify_debugger_ready=debugger_ready,
        )

    # internal methods for PyDevdSocket().

    def _handle_pydevd_message(self, cmdid, seq, text):
        if self.session is None or self.session.closed:
            # TODO: Do more than ignore?
            return
        self.session.handle_pydevd_message(cmdid, seq, text)

    def _handle_pydevd_close(self):
        try:
            self.close()
        except DaemonClosedError:
            pass

    def _getpeername(self):
        if self.session is None or self.session.closed:
            raise NotImplementedError
        return self.session.socket.getpeername()

    def _getsockname(self):
        if self.session is None or self.session.closed:
            raise NotImplementedError
        return self.session.socket.getsockname()
/n/n/nptvsd/exit_handlers.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import atexit
import os
import platform
import signal


class AlreadyInstalledError(RuntimeError):
    """"""Exit handlers were already installed.""""""


class UnsupportedSignalError(RuntimeError):
    """"""A signal is not supported.""""""


def kill_current_proc(signum=signal.SIGTERM):
    """"""Kill the current process.

    Note that this directly kills the process (with SIGTERM, by default)
    rather than using sys.exit().
    """"""
    os.kill(os.getpid(), signum)


class ExitHandlers(object):
    """"""Manages signal and atexit handlers.""""""

    if platform.system() == 'Windows':
        # TODO: Windows *does* support these signals:
        #  SIGABRT, SIGFPE, SIGILL, SIGINT, SIGSEGV, SIGTERM, SIGBREAK
        SIGNALS = []
    else:
        SIGNALS = [
            signal.SIGHUP,
        ]

    def __init__(self):
        self._signal_handlers = {sig: []
                                 for sig in self.SIGNALS}
        self._atexit_handlers = []
        self._installed = False

    @property
    def supported_signals(self):
        return set(self.SIGNALS)

    @property
    def installed(self):
        return self._installed

    def install(self):
        """"""Set the parent handlers.

        This must be called in the main thread.
        """"""
        if self._installed:
            raise AlreadyInstalledError('exit handlers already installed')
        self._installed = True
        self._install_signal_handler()
        self._install_atexit_handler()

    # TODO: Add uninstall()?

    def add_atexit_handler(self, handle_atexit, nodupe=True):
        """"""Add an atexit handler to the list managed here.""""""
        if nodupe and handle_atexit in self._atexit_handlers:
            raise ValueError('atexit handler alraedy added')
        self._atexit_handlers.append(handle_atexit)

    def add_signal_handler(self, signum, handle_signal, nodupe=True,
                           ignoreunsupported=False):
        """"""Add a signal handler to the list managed here.""""""
        # TODO: The initialization of self.SIGNALS should make this
        # special-casing unnecessary.
        if platform.system() == 'Windows':
            return

        try:
            handlers = self._signal_handlers[signum]
        except KeyError:
            if ignoreunsupported:
                return
            raise UnsupportedSignalError(signum)
        if nodupe and handle_signal in handlers:
            raise ValueError('signal handler alraedy added')
        handlers.append(handle_signal)

    # internal methods

    def _install_signal_handler(self):
        # TODO: The initialization of self.SIGNALS should make this
        # special-casing unnecessary.
        if platform.system() == 'Windows':
            return

        orig = {}
        try:
            for sig in self._signal_handlers:
                # TODO: Skip or fail if signal.getsignal() returns None?
                orig[sig] = signal.signal(sig, self._signal_handler)
        except ValueError:
            # Wasn't called in main thread!
            raise

    def _signal_handler(self, signum, frame):
        for handle_signal in self._signal_handlers.get(signum, ()):
            handle_signal(signum, frame)

    def _install_atexit_handler(self):
        self._atexit_handlers = []
        atexit.register(self._atexit_handler)

    def _atexit_handler(self):
        for handle_atexit in self._atexit_handlers:
            handle_atexit()
/n/n/nptvsd/pydevd_hooks.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import sys

from _pydevd_bundle import pydevd_comm

from ptvsd.socket import Address
from ptvsd.daemon import Daemon, DaemonStoppedError, DaemonClosedError
from ptvsd._util import debug, new_hidden_thread


def start_server(daemon, host, port, **kwargs):
    """"""Return a socket to a (new) local pydevd-handling daemon.

    The daemon supports the pydevd client wire protocol, sending
    requests and handling responses (and events).

    This is a replacement for _pydevd_bundle.pydevd_comm.start_server.
    """"""
    sock, next_session = daemon.start_server((host, port))

    def handle_next():
        try:
            session = next_session(**kwargs)
            debug('done waiting')
            return session
        except (DaemonClosedError, DaemonStoppedError):
            # Typically won't happen.
            debug('stopped')
            raise
        except Exception as exc:
            # TODO: log this?
            debug('failed:', exc, tb=True)
            return None

    def serve_forever():
        debug('waiting on initial connection')
        handle_next()
        while True:
            debug('waiting on next connection')
            try:
                handle_next()
            except (DaemonClosedError, DaemonStoppedError):
                break
        debug('done')

    t = new_hidden_thread(
        target=serve_forever,
        name='sessions',
    )
    t.start()
    return sock


def start_client(daemon, host, port, **kwargs):
    """"""Return a socket to an existing ""remote"" pydevd-handling daemon.

    The daemon supports the pydevd client wire protocol, sending
    requests and handling responses (and events).

    This is a replacement for _pydevd_bundle.pydevd_comm.start_client.
    """"""
    sock, start_session = daemon.start_client((host, port))
    start_session(**kwargs)
    return sock


def install(pydevd, address,
            start_server=start_server, start_client=start_client,
            **kwargs):
    """"""Configure pydevd to use our wrapper.

    This is a bit of a hack to allow us to run our VSC debug adapter
    in the same process as pydevd.  Note that, as with most hacks,
    this is somewhat fragile (since the monkeypatching sites may
    change).
    """"""
    addr = Address.from_raw(address)
    daemon = Daemon(**kwargs)

    _start_server = (lambda p: start_server(daemon, addr.host, p))
    _start_server.orig = start_server
    _start_client = (lambda h, p: start_client(daemon, h, p))
    _start_client.orig = start_client

    # These are the functions pydevd invokes to get a socket to the client.
    pydevd_comm.start_server = _start_server
    pydevd_comm.start_client = _start_client

    # Ensure that pydevd is using our functions.
    pydevd.start_server = _start_server
    pydevd.start_client = _start_client
    __main__ = sys.modules['__main__']
    if __main__ is not pydevd:
        if getattr(__main__, '__file__', None) == pydevd.__file__:
            __main__.start_server = _start_server
            __main__.start_client = _start_client
    return daemon
/n/n/nptvsd/session.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

from .socket import is_socket, close_socket
from .wrapper import VSCodeMessageProcessor
from ._util import TimeoutError, ClosedError, Closeable, Startable, debug


class DebugSession(Startable, Closeable):
    """"""A single DAP session for a network client socket.""""""

    MESSAGE_PROCESSOR = None

    NAME = 'debug session'
    FAIL_ON_ALREADY_CLOSED = False
    FAIL_ON_ALREADY_STOPPED = False

    @classmethod
    def from_raw(cls, raw, **kwargs):
        """"""Return a session for the given data.""""""
        if isinstance(raw, cls):
            return raw
        if not is_socket(raw):
            # TODO: Create a new client socket from a remote address?
            #addr = Address.from_raw(raw)
            raise NotImplementedError
        client = raw
        return cls(client, **kwargs)

    @classmethod
    def from_server_socket(cls, server, **kwargs):
        """"""Return a session for the next connection to the given socket.""""""
        client, _ = server.accept()
        return cls(client, ownsock=True, **kwargs)

    def __init__(self, sock,
                 notify_closing=None,
                 notify_disconnecting=None,
                 ownsock=False):
        super(DebugSession, self).__init__()

        if notify_closing is not None:
            def handle_closing(before):
                if before:
                    notify_closing(self)
            self.add_close_handler(handle_closing)

        if notify_disconnecting is None:
            notify_disconnecting = (lambda _: None)
        self._notify_disconnecting = notify_disconnecting

        self._sock = sock
        self._pre_socket_close = None
        if ownsock:
            # Close the socket *after* calling sys.exit() (via notify_closing).
            def handle_closing(before):
                if before:
                    return
                debug('closing session socket')
                proc = self._msgprocessor
                if self._pre_socket_close is not None:
                    self._pre_socket_close()
                if proc is not None:
                    try:
                        proc.wait_while_connected(10)  # seconds
                    except TimeoutError:
                        debug('timed out waiting for disconnect')
                close_socket(self._sock)
            self.add_close_handler(handle_closing)

        self._msgprocessor = None

    @property
    def socket(self):
        return self._sock

    @property
    def msgprocessor(self):
        return self._msgprocessor

    def handle_debugger_stopped(self, wait=None):
        """"""Deal with the debugger exiting.""""""
        proc = self._msgprocessor
        if proc is None:
            return
        proc.handle_debugger_stopped(wait)

    def handle_exiting(self, exitcode=None, wait=None):
        """"""Deal with the debuggee exiting.""""""
        proc = self._msgprocessor
        if proc is None:
            return
        proc.handle_exiting(exitcode, wait)

    def wait_until_stopped(self):
        """"""Block until all resources (e.g. message processor) have stopped.""""""
        proc = self._msgprocessor
        if proc is None:
            return
        # TODO: Do this in VSCodeMessageProcessor.close()?
        proc._wait_for_server_thread()

    # internal methods

    def _new_msg_processor(self, **kwargs):
        return self.MESSAGE_PROCESSOR(
            self._sock,
            notify_disconnecting=self._handle_vsc_disconnect,
            notify_closing=self._handle_vsc_close,
            **kwargs
        )

    def _start(self, threadname, **kwargs):
        """"""Start the message handling for the session.""""""
        self._msgprocessor = self._new_msg_processor(**kwargs)
        self.add_resource_to_close(self._msgprocessor)
        self._msgprocessor.start(threadname)
        return self._msgprocessor_running

    def _stop(self):
        proc = self._msgprocessor
        if proc is None:
            return

        debug('proc stopping')
        # TODO: We should not need to wait if not exiting.
        # The editor will send a ""disconnect"" request at this point.
        proc._wait_for_disconnect()
        proc.close()
        self._msgprocessor = None

    def _close(self):
        debug('session closing')
        pass

    def _msgprocessor_running(self):
        if self._msgprocessor is None:
            return False
        # TODO: Return self._msgprocessor.is_running().
        return True

    # internal methods for VSCodeMessageProcessor

    def _handle_vsc_disconnect(self, pre_socket_close=None):
        debug('disconnecting')
        self._pre_socket_close = pre_socket_close  # TODO: Fail if already set?
        self._notify_disconnecting(self)

    def _handle_vsc_close(self):
        debug('processor closing')
        try:
            self.close()
        except ClosedError:
            pass


class PyDevdDebugSession(DebugSession):
    """"""A single DAP session for a network client socket.""""""

    MESSAGE_PROCESSOR = VSCodeMessageProcessor

    def __init__(self, sock,
                 notify_debugger_ready=None,
                 **kwargs):
        super(PyDevdDebugSession, self).__init__(sock, **kwargs)

        def notify_debugger_ready(session, _notify=notify_debugger_ready):
            if self._notified_debugger_ready:
                return
            self._notified_debugger_ready = True
            if _notify is not None:
                _notify(session)
        self._notified_debugger_ready = False
        self._notify_debugger_ready = notify_debugger_ready

    def handle_pydevd_message(self, cmdid, seq, text):
        if self._msgprocessor is None:
            # TODO: Do more than ignore?
            return
        return self._msgprocessor.on_pydevd_event(cmdid, seq, text)

    # internal methods

    def _new_msg_processor(self, **kwargs):
        return super(PyDevdDebugSession, self)._new_msg_processor(
            notify_debugger_ready=self._handle_vsc_debugger_ready,
            **kwargs
        )

    # internal methods for VSCodeMessageProcessor

    def _handle_vsc_debugger_ready(self):
        debug('ready to debug')
        self._notify_debugger_ready(self)
/n/n/nptvsd/socket.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

from __future__ import absolute_import

from collections import namedtuple
import contextlib
import errno
import socket
try:
    from urllib.parse import urlparse
except ImportError:
    from urlparse import urlparse


try:
    ConnectionError  # noqa
    BrokenPipeError  # noqa
    ConnectionResetError  # noqa
except NameError:
    class BrokenPipeError(Exception):
        # EPIPE and ESHUTDOWN
        pass

    class ConnectionResetError(Exception):
        # ECONNRESET
        pass


NOT_CONNECTED = (
    errno.ENOTCONN,
    errno.EBADF,
)

CLOSED = (
    errno.EPIPE,
    errno.ESHUTDOWN,
    errno.ECONNRESET,
    # Windows
    10038,  # ""An operation was attempted on something that is not a socket""
    10058,
)

EOF = NOT_CONNECTED + CLOSED


@contextlib.contextmanager
def convert_eof():
    """"""A context manager to convert some socket errors into EOFError.""""""
    try:
        yield
    except ConnectionResetError:
        raise EOFError
    except BrokenPipeError:
        raise EOFError
    except OSError as exc:
        if exc.errno in EOF:
            raise EOFError
        raise


class TimeoutError(socket.timeout):
    """"""A socket timeout happened.""""""


def is_socket(sock):
    """"""Return True if the object can be used as a socket.""""""
    return isinstance(sock, socket.socket)


def create_server(host, port):
    """"""Return a local server socket listening on the given port.""""""
    if host is None:
        host = 'localhost'
    server = _new_sock()
    server.bind((host, port))
    server.listen(0)
    return server


def create_client():
    """"""Return a client socket that may be connected to a remote address.""""""
    return _new_sock()


def _new_sock():
    sock = socket.socket(socket.AF_INET,
                         socket.SOCK_STREAM,
                         socket.IPPROTO_TCP)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    return sock


@contextlib.contextmanager
def ignored_errno(*ignored):
    """"""A context manager that ignores the given errnos.""""""
    try:
        yield
    except OSError as exc:
        if exc.errno not in ignored:
            raise


class KeepAlive(namedtuple('KeepAlive', 'interval idle maxfails')):
    """"""TCP keep-alive settings.""""""

    INTERVAL = 3  # seconds
    IDLE = 1  # seconds after idle
    MAX_FAILS = 5

    @classmethod
    def from_raw(cls, raw):
        """"""Return the corresponding KeepAlive.""""""
        if raw is None:
            return None
        elif isinstance(raw, cls):
            return raw
        elif isinstance(raw, (str, int, float)):
            return cls(raw)
        else:
            try:
                raw = dict(raw)
            except TypeError:
                return cls(*raw)
            else:
                return cls(**raw)

    def __new__(cls, interval=None, idle=None, maxfails=None):
        self = super(KeepAlive, cls).__new__(
            cls,
            float(interval) if interval or interval == 0 else cls.INTERVAL,
            float(idle) if idle or idle == 0 else cls.IDLE,
            float(maxfails) if maxfails or maxfails == 0 else cls.MAX_FAILS,
        )
        return self

    def apply(self, sock):
        """"""Set the keepalive values on the socket.""""""
        sock.setsockopt(socket.SOL_SOCKET,
                        socket.SO_KEEPALIVE,
                        1)
        interval = self.interval
        idle = self.idle
        maxfails = self.maxfails
        try:
            if interval > 0:
                sock.setsockopt(socket.IPPROTO_TCP,
                                socket.TCP_KEEPINTVL,
                                interval)
            if idle > 0:
                sock.setsockopt(socket.IPPROTO_TCP,
                                socket.TCP_KEEPIDLE,
                                idle)
            if maxfails >= 0:
                sock.setsockopt(socket.IPPROTO_TCP,
                                socket.TCP_KEEPCNT,
                                maxfails)
        except AttributeError:
            # mostly linux-only
            pass


def connect(sock, addr, keepalive=None):
    """"""Return the client socket for the next connection.""""""
    if addr is None:
        if keepalive is None or keepalive is True:
            keepalive = KeepAlive()
        elif keepalive:
            keepalive = KeepAlive.from_raw(keepalive)
        client, _ = sock.accept()
        if keepalive:
            keepalive.apply(client)
        return client
    else:
        if keepalive:
            raise NotImplementedError
        sock.connect(addr)
        return sock


def shut_down(sock, how=socket.SHUT_RDWR, ignored=NOT_CONNECTED):
    """"""Shut down the given socket.""""""
    with ignored_errno(*ignored or ()):
        sock.shutdown(how)


def close_socket(sock):
    """"""Shutdown and close the socket.""""""
    try:
        shut_down(sock)
    except Exception:
        # TODO: Log errors?
        pass
    sock.close()


class Address(namedtuple('Address', 'host port')):
    """"""An IP address to use for sockets.""""""

    @classmethod
    def from_raw(cls, raw, defaultport=None):
        """"""Return an address corresponding to the given data.""""""
        if isinstance(raw, cls):
            return raw
        elif isinstance(raw, int):
            return cls(None, raw)
        elif isinstance(raw, str):
            if raw == '':
                return cls('', defaultport)
            parsed = urlparse(raw)
            if not parsed.netloc:
                if parsed.scheme:
                    raise ValueError('invalid address {!r}'.format(raw))
                return cls.from_raw('x://' + raw, defaultport=defaultport)
            return cls(
                parsed.hostname or '',
                parsed.port if parsed.port else defaultport,
            )
        elif not raw:
            return cls(None, defaultport)
        else:
            try:
                kwargs = dict(**raw)
            except TypeError:
                return cls(*raw)
            else:
                kwargs.setdefault('host', None)
                kwargs.setdefault('port', defaultport)
                return cls(**kwargs)

    @classmethod
    def as_server(cls, host, port):
        """"""Return an address to use as a server address.""""""
        return cls(host, port, isserver=True)

    @classmethod
    def as_client(cls, host, port):
        """"""Return an address to use as a server address.""""""
        return cls(host, port, isserver=False)

    def __new__(cls, host, port, **kwargs):
        if host == '*':
            host = ''
        isserver = kwargs.pop('isserver', None)
        if isserver is None:
            isserver = (host is None or host == '')
        else:
            isserver = bool(isserver)
        if host is None:
            host = 'localhost'
        self = super(Address, cls).__new__(
            cls,
            str(host),
            int(port) if port is not None else None,
            **kwargs
        )
        self._isserver = isserver
        return self

    def __init__(self, *args, **kwargs):
        if self.port is None:
            raise TypeError('missing port')
        if self.port <= 0 or self.port > 65535:
            raise ValueError('port must be positive int < 65535')

    def __repr__(self):
        orig = super(Address, self).__repr__()
        return '{}, isserver={})'.format(orig[:-1], self._isserver)

    def __eq__(self, other):
        if not super(Address, self).__eq__(other):
            return False
        try:
            other = self.from_raw(other)
        except Exception:
            return False
        return self._isserver == other._isserver

    @property
    def isserver(self):
        return self._isserver
/n/n/ntests/helpers/debugadapter.py/n/nimport os
import os.path
import socket
import time

from ptvsd.socket import Address
from ptvsd._util import Closeable, ClosedError
from .proc import Proc
from .. import PROJECT_ROOT


COPIED_ENV = [
    'PYTHONHASHSEED',

    # Windows
    #'ALLUSERSPROFILE',
    #'APPDATA',
    #'CLIENTNAME',
    #'COMMONPROGRAMFILES',
    #'COMMONPROGRAMFILES(X86)',
    #'COMMONPROGRAMW6432',
    #'COMPUTERNAME',
    #'COMSPEC',
    #'DRIVERDATA',
    #'HOMEDRIVE',
    #'HOMEPATH',
    #'LOCALAPPDATA',
    #'LOGONSERVER',
    #'NUMBER_OF_PROCESSORS',
    #'OS',
    #'PATH',
    #'PATHEXT',
    #'PROCESSOR_ARCHITECTURE',
    #'PROCESSOR_IDENTIFIER',
    #'PROCESSOR_LEVEL',
    #'PROCESSOR_REVISION',
    #'PROGRAMDATA',
    #'PROGRAMFILES',
    #'PROGRAMFILES(X86)',
    #'PROGRAMW6432',
    #'PSMODULEPATH',
    #'PUBLIC',
    #'SESSIONNAME',
    'SYSTEMDRIVE',
    'SYSTEMROOT',
    #'TEMP',
    #'TMP',
    #'USERDOMAIN',
    #'USERDOMAIN_ROAMINGPROFILE',
    #'USERNAME',
    #'USERPROFILE',
    'WINDIR',
]

SERVER_READY_TIMEOUT = 3.0  # seconds

try:
    ConnectionRefusedError
except Exception:
    class ConnectionRefusedError(Exception):
        pass


def _copy_env(verbose=False, env=None):
    variables = {k: v for k, v in os.environ.items() if k in COPIED_ENV}
    # TODO: Be smarter about the seed?
    variables.setdefault('PYTHONHASHSEED', '1234')
    if verbose:
        variables.update({
            'PTVSD_DEBUG': '1',
            'PTVSD_SOCKET_TIMEOUT': '1',
        })
    if env is not None:
        variables.update(env)

    # Ensure Project root is always in current path.
    python_path = variables.get('PYTHONPATH', None)
    if python_path is None:
        variables['PYTHONPATH'] = PROJECT_ROOT
    else:
        variables['PYTHONPATH'] = os.pathsep.join([PROJECT_ROOT, python_path])

    return variables


def wait_for_socket_server(addr, timeout=SERVER_READY_TIMEOUT):
    start_time = time.time()
    while True:
        try:
            sock = socket.create_connection((addr.host, addr.port))
            sock.close()
            time.sleep(0.1)  # wait for daemon to detect to socket close.
            return
        except Exception:
            pass
        time.sleep(0.1)
        if time.time() - start_time > timeout:
            raise ConnectionRefusedError('Timeout waiting for connection')


def wait_for_port_to_free(port, timeout=3.0):
    start_time = time.time()
    while True:
        try:
            time.sleep(0.5)
            sock = socket.create_connection(('localhost', port))
            sock.close()
        except Exception:
            return
        time.sleep(0.1)
        if time.time() - start_time > timeout:
            raise ConnectionRefusedError('Timeout waiting for port to be free')


class DebugAdapter(Closeable):

    VERBOSE = False
    #VERBOSE = True

    PORT = 8888

    # generic factories

    @classmethod
    def start(cls, argv, env=None, cwd=None, **kwargs):
        def new_proc(argv, addr, **kwds):
            env_vars = _copy_env(verbose=cls.VERBOSE, env=env)
            argv = list(argv)
            cls._ensure_addr(argv, addr)
            return Proc.start_python_module(
                'ptvsd', argv, env=env_vars, cwd=cwd, **kwds)

        return cls._start(new_proc, argv, **kwargs)

    @classmethod
    def start_wrapper_script(cls, filename, argv, env=None, cwd=None,
                             **kwargs):  # noqa
        def new_proc(argv, addr, **kwds):
            env_vars = _copy_env(verbose=cls.VERBOSE, env=env)
            return Proc.start_python_script(
                filename, argv, env=env_vars, cwd=cwd, **kwds)

        return cls._start(new_proc, argv, **kwargs)

    @classmethod
    def start_wrapper_module(cls,
                             modulename,
                             argv,
                             env=None,
                             cwd=None,
                             **kwargs):  # noqa
        def new_proc(argv, addr, **kwds):
            env_vars = _copy_env(verbose=cls.VERBOSE, env=env)
            return Proc.start_python_module(
                modulename, argv, env=env_vars, cwd=cwd, **kwds)

        return cls._start(new_proc, argv, **kwargs)

    # specific factory cases

    @classmethod
    def start_nodebug(cls, addr, name, kind='script', **kwargs):
        if kind == 'script':
            argv = ['--nodebug', name]
        elif kind == 'module':
            argv = ['--nodebug', '-m', name]
        else:
            raise NotImplementedError
        return cls.start(argv, addr=addr, **kwargs)

    @classmethod
    def start_as_server(cls, addr, *args, **kwargs):
        addr = Address.as_server(*addr)
        return cls._start_as(addr, *args, server=False, **kwargs)

    @classmethod
    def start_as_client(cls, addr, *args, **kwargs):
        addr = Address.as_client(*addr)
        return cls._start_as(addr, *args, server=False, **kwargs)

    @classmethod
    def start_for_attach(cls, addr, *args, **kwargs):
        srvtimeout = kwargs.pop('srvtimeout', SERVER_READY_TIMEOUT)
        addr = Address.as_server(*addr)
        adapter = cls._start_as(addr, *args, server=True, **kwargs)
        if srvtimeout is not None:
            wait_for_socket_server(addr, timeout=srvtimeout)
        return adapter

    @classmethod
    def _start_as(cls,
                  addr,
                  name,
                  kind='script',
                  extra=None,
                  server=False,
                  **kwargs):
        argv = []
        if server:
            argv += ['--server']
            if kwargs.pop('wait', True):
                argv += ['--wait']
        if kind == 'script':
            argv += [name]
        elif kind == 'module':
            argv += ['-m', name]
        else:
            raise NotImplementedError
        if extra:
            argv += list(extra)
        return cls.start(argv, addr=addr, **kwargs)

    @classmethod
    def start_embedded(cls, addr, filename, argv=[], **kwargs):
        # ptvsd.enable_attach() slows things down, so we must wait longer.
        srvtimeout = kwargs.pop('srvtimeout', SERVER_READY_TIMEOUT + 2)
        addr = Address.as_server(*addr)
        with open(filename, 'r+') as scriptfile:
            content = scriptfile.read()
            # TODO: Handle this case somehow?
            assert 'ptvsd.enable_attach' in content
        adapter = cls.start_wrapper_script(
            filename, argv=argv, addr=addr, **kwargs)
        if srvtimeout is not None:
            wait_for_socket_server(addr, timeout=srvtimeout)
        return adapter

    @classmethod
    def _start(cls, new_proc, argv, addr=None, **kwargs):
        addr = Address.from_raw(addr, defaultport=cls.PORT)
        proc = new_proc(argv, addr, **kwargs)
        return cls(proc, addr, owned=True)

    @classmethod
    def _ensure_addr(cls, argv, addr):
        if '--host' in argv:
            raise ValueError(""unexpected '--host' in argv"")
        if '--server-host' in argv:
            raise ValueError(""unexpected '--server-host' in argv"")
        if '--port' in argv:
            raise ValueError(""unexpected '--port' in argv"")
        host, port = addr

        argv.insert(0, str(port))
        argv.insert(0, '--port')

        argv.insert(0, host)
        if addr.isserver:
            argv.insert(0, '--server-host')
        else:
            argv.insert(0, '--host')

    def __init__(self, proc, addr, owned=False):
        super(DebugAdapter, self).__init__()
        assert isinstance(proc, Proc)
        self._proc = proc
        self._addr = addr

    @property
    def address(self):
        return self._addr

    @property
    def pid(self):
        return self._proc.pid

    @property
    def output(self):
        # TODO: Decode here?
        return self._proc.output

    @property
    def exitcode(self):
        return self._proc.exitcode

    def wait(self, *argv):
        self._proc.wait(*argv)

    # internal methods

    def _close(self):
        if self._proc is not None:
            try:
                self._proc.close()
            except ClosedError:
                pass
        if self.VERBOSE:
            lines = self.output.decode('utf-8').splitlines()
            print(' + ' + '\n + '.join(lines))
/n/n/ntests/helpers/debugclient.py/n/nfrom __future__ import absolute_import

import os
import traceback
import warnings

from ptvsd.socket import Address
from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .debugadapter import DebugAdapter, wait_for_socket_server
from .debugsession import DebugSession

# TODO: Add a helper function to start a remote debugger for testing
# remote debugging?


class _LifecycleClient(Closeable):

    SESSION = DebugSession

    def __init__(
            self,
            addr=None,
            port=8888,
            breakpoints=None,
            connecttimeout=1.0,
    ):
        super(_LifecycleClient, self).__init__()
        self._addr = Address.from_raw(addr, defaultport=port)
        self._connecttimeout = connecttimeout
        self._adapter = None
        self._session = None

        self._breakpoints = breakpoints

    @property
    def adapter(self):
        return self._adapter

    @property
    def session(self):
        return self._session

    def start_debugging(self, launchcfg):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        raise NotImplementedError

    def stop_debugging(self):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')

        if self._session is not None:
            self._detach()

        try:
            self._adapter.close()
        except ClosedError:
            pass
        self._adapter = None

    def attach_pid(self, pid, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        raise NotImplementedError

    def attach_socket(self, addr=None, adapter=None, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if adapter is None:
            adapter = self._adapter
        elif self._adapter is not None:
            raise RuntimeError('already using managed adapter')
        if adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        if addr is None:
            addr = adapter.address
        self._attach(addr, **kwargs)
        return self._session

    def detach(self, adapter=None):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._session is None:
            raise RuntimeError('not attached')
        if adapter is None:
            adapter = self._adapter
        assert adapter is not None
        if not self._session.is_client:
            raise RuntimeError('detach not supported')

        self._detach()

    # internal methods

    def _close(self):
        if self._session is not None:
            try:
                self._session.close()
            except ClosedError:
                pass
        if self._adapter is not None:
            try:
                self._adapter.close()
            except ClosedError:
                pass

    def _launch(self,
                argv,
                script=None,
                wait_for_connect=None,
                detachable=True,
                env=None,
                cwd=None,
                **kwargs):
        if script is not None:
            def start(*args, **kwargs):
                return DebugAdapter.start_wrapper_script(
                    script, *args, **kwargs)
        else:
            start = DebugAdapter.start
        new_addr = Address.as_server if detachable else Address.as_client
        addr = new_addr(None, self._addr.port)
        self._adapter = start(argv, addr=addr, env=env, cwd=cwd)

        if wait_for_connect:
            wait_for_connect()
        else:
            try:
                wait_for_socket_server(addr)
            except Exception:
                # If we fail to connect, print out the adapter output.
                self._adapter.VERBOSE = True
                raise
            self._attach(addr, **kwargs)

    def _attach(self, addr, **kwargs):
        if addr is None:
            addr = self._addr
        assert addr.host == 'localhost'
        self._session = self.SESSION.create_client(addr, **kwargs)

    def _detach(self):
        session = self._session
        if session is None:
            return
        self._session = None
        try:
            session.close()
        except ClosedError:
            pass


class DebugClient(_LifecycleClient):
    """"""A high-level abstraction of a debug client (i.e. editor).""""""

    # TODO: Manage breakpoints, etc.
    # TODO: Add debugger methods here (e.g. ""pause"").


class EasyDebugClient(DebugClient):
    def start_detached(self, argv):
        """"""Start an adapter in a background process.""""""
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        # TODO: Launch, handshake and detach?
        self._adapter = DebugAdapter.start(argv, port=self._port)
        return self._adapter

    def host_local_debugger(self,
                            argv,
                            script=None,
                            env=None,
                            cwd=None,
                            **kwargs):  # noqa
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None
        addr = ('localhost', self._addr.port)

        self._run_server_ex = None

        def run():
            try:
                self._session = self.SESSION.create_server(addr, **kwargs)
            except Exception as ex:
                self._run_server_ex = traceback.format_exc()

        t = new_hidden_thread(
            target=run,
            name='test.client',
        )
        t.start()

        def wait():
            t.join(timeout=self._connecttimeout)
            if t.is_alive():
                warnings.warn('timed out waiting for connection')
            if self._session is None:
                message = 'unable to connect after {} secs'.format(  # noqa
                    self._connecttimeout)
                if self._run_server_ex is None:
                    raise Exception(message)
                else:
                    message = message + os.linesep + self._run_server_ex # noqa
                    raise Exception(message)

            # The adapter will close when the connection does.

        self._launch(
            argv,
            script=script,
            wait_for_connect=wait,
            detachable=False,
            env=env,
            cwd=cwd)

        return self._adapter, self._session

    def launch_script(self, filename, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            filename,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        if kwargs.pop('wait', True):
            argv.insert(0, '--wait')
        self._launch(argv, **kwargs)
        return self._adapter, self._session

    def launch_module(self, module, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            '-m',
            module,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session
/n/n/ntests/helpers/debugsession.py/n/nfrom __future__ import absolute_import, print_function

import contextlib
import json
import socket
import sys
import time
import threading
import warnings

from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .message import (
    raw_read_all as read_messages,
    raw_write_one as write_message
)
from .socket import (
    Connection, create_server, create_client, close,
    recv_as_read, send_as_write,
    timeout as socket_timeout)
from .threading import get_locked_and_waiter
from .vsc import parse_message


class DebugSessionConnection(Closeable):

    VERBOSE = False
    #VERBOSE = True

    TIMEOUT = 5.0

    @classmethod
    def create_client(cls, addr, **kwargs):
        def connect(addr, timeout):
            sock = create_client()
            for _ in range(int(timeout * 10)):
                try:
                    sock.connect(addr)
                except (OSError, socket.error):
                    if cls.VERBOSE:
                        print('+', end='')
                        sys.stdout.flush()
                    time.sleep(0.1)
                else:
                    break
            else:
                raise RuntimeError('could not connect')
            return sock
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def create_server(cls, addr, **kwargs):
        def connect(addr, timeout):
            server = create_server(addr)
            with socket_timeout(server, timeout):
                client, _ = server.accept()
            return Connection(client, server)
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def _create(cls, connect, addr, timeout=None):
        if timeout is None:
            timeout = cls.TIMEOUT
        sock = connect(addr, timeout)
        if cls.VERBOSE:
            print('connected')
        self = cls(sock, ownsock=True)
        self._addr = addr
        return self

    def __init__(self, sock, ownsock=False):
        super(DebugSessionConnection, self).__init__()
        self._sock = sock
        self._ownsock = ownsock

    @property
    def is_client(self):
        try:
            return self._sock.server is None
        except AttributeError:
            return True

    def iter_messages(self):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        read = recv_as_read(self._sock)
        for msg, _, _ in read_messages(read, stop=stop):
            if self.VERBOSE:
                print(repr(msg))
            yield parse_message(msg)

    def send(self, req):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        write = send_as_write(self._sock)
        body = json.dumps(req)
        write_message(write, body, stop=stop)

    # internal methods

    def _close(self):
        if self._ownsock:
            close(self._sock)


class DebugSession(Closeable):

    VERBOSE = False
    #VERBOSE = True

    HOST = 'localhost'
    PORT = 8888

    TIMEOUT = None

    @classmethod
    def create_client(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_client(
            addr,
            timeout=kwargs.get('timeout'),
        )
        return cls(conn, owned=True, **kwargs)

    @classmethod
    def create_server(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_server(addr, **kwargs)
        return cls(conn, owned=True, **kwargs)

    def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False):
        super(DebugSession, self).__init__()
        self._conn = conn
        self._seq = seq
        self._timeout = timeout
        self._owned = owned

        self._handlers = []
        for handler in handlers:
            if callable(handler):
                self._add_handler(handler)
            else:
                self._add_handler(*handler)
        self._received = []
        self._listenerthread = new_hidden_thread(
            target=self._listen,
            name='test.session',
        )
        self._listenerthread.start()

    @property
    def is_client(self):
        return self._conn.is_client

    @property
    def received(self):
        return list(self._received)

    def _create_request(self, command, **args):
        seq = self._seq
        self._seq += 1
        return {
            'type': 'request',
            'seq': seq,
            'command': command,
            'arguments': args,
        }

    def send_request(self, command, **args):
        if self.closed:
            raise RuntimeError('session closed')

        wait = args.pop('wait', False)
        req = self._create_request(command, **args)
        if self.VERBOSE:
            msg = parse_message(req)
            print(' <-', msg)

        if wait:
            with self.wait_for_response(req) as resp:
                self._conn.send(req)
            resp_awaiter = AwaitableResponse(req, lambda: resp[""msg""])
        else:
            resp_awaiter = self._get_awaiter_for_request(req, **args)
            self._conn.send(req)
        return resp_awaiter

    def add_handler(self, handler, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        self._add_handler(handler, **kwargs)

    @contextlib.contextmanager
    def wait_for_event(self, event, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event
        handlername = 'event {!r}'.format(event)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): # noqa
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event and condition(msg) # noqa
        handlername = 'event {!r}'.format(event)
        evt = self._get_message_handle(match, handlername)

        return AwaitableEvent(event, lambda: result[""msg""], evt)

    def _get_awaiter_for_request(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        evt = self._get_message_handle(match, handlername)

        return AwaitableResponse(req, lambda: result[""msg""], evt)

    @contextlib.contextmanager
    def wait_for_response(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    # internal methods

    def _close(self):
        if self._owned:
            try:
                self._conn.close()
            except ClosedError:
                pass
        if self._listenerthread != threading.current_thread():
            self._listenerthread.join(timeout=1.0)
            if self._listenerthread.is_alive():
                warnings.warn('session listener still running')
        self._check_handlers()

    def _listen(self):
        eof = None
        try:
            for msg in self._conn.iter_messages():
                if self.VERBOSE:
                    print(' ->', msg)
                self._receive_message(msg)
        except EOFError as ex:
            # Handle EOF outside of except to avoid unnecessary chaining.
            eof = ex
        if eof:
            remainder = getattr(eof, 'remainder', b'')
            if remainder:
                self._receive_message(remainder)
            try:
                self.close()
            except ClosedError:
                pass

    def _receive_message(self, msg):
        for i, handler in enumerate(list(self._handlers)):
            handle_message, _, _ = handler
            handled = handle_message(msg)
            try:
                msg, handled = handled
            except TypeError:
                pass
            if handled:
                self._handlers.remove(handler)
                break
        self._received.append(msg)

    def _add_handler(self, handle_msg, handlername=None, required=True):
        self._handlers.append(
            (handle_msg, handlername, required))

    def _check_handlers(self):
        unhandled = []
        for handle_msg, name, required in self._handlers:
            if not required:
                continue
            unhandled.append(name or repr(handle_msg))
        if unhandled:
            raise RuntimeError('unhandled: {}'.format(unhandled))

    @contextlib.contextmanager
    def _wait_for_message(self, match, handlername, timeout=None):
        if timeout is None:
            timeout = self.TIMEOUT
        lock, wait = get_locked_and_waiter()

        def handler(msg):
            if not match(msg):
                return msg, False
            lock.release()
            return msg, True
        self._add_handler(handler, handlername)
        try:
            yield
        finally:
            wait(timeout or self._timeout, handlername, fail=True)

    def _get_message_handle(self, match, handlername):
        event = threading.Event()

        def handler(msg):
            if not match(msg):
                return msg, False
            event.set()
            return msg, True
        self._add_handler(handler, handlername, False)
        return event


class Awaitable(object):

    @classmethod
    def wait_all(cls, *awaitables):
        timeout = 3.0
        messages = []
        for _ in range(int(timeout * 10)):
            time.sleep(0.1)
            messages = []
            not_ready = (a for a in awaitables if a._event is not None and not a._event.is_set()) # noqa
            for awaitable in not_ready:
                if isinstance(awaitable, AwaitableEvent):
                    messages.append('Event {}'.format(awaitable.name))
                else:
                    messages.append('Response {}'.format(awaitable.name))
            if len(messages) == 0:
                return
        else:
            raise TimeoutError('Timeout waiting for {}'.format(','.join(messages))) # noqa

    def __init__(self, name, event=None):
        self._event = event
        self.name = name

    def wait(self, timeout=1.0):
        if self._event is None:
            return
        if not self._event.wait(timeout):
            message = 'Timeout waiting for '
            if isinstance(self, AwaitableEvent):
                message += 'Event {}'.format(self.name)
            else:
                message += 'Response {}'.format(self.name)
            raise TimeoutError(message)


class AwaitableResponse(Awaitable):

    def __init__(self, req, result_getter, event=None):
        super(AwaitableResponse, self).__init__(req[""command""], event)
        self.req = req
        self._result_getter = result_getter

    @property
    def resp(self):
        return self._result_getter()


class AwaitableEvent(Awaitable):

    def __init__(self, name, result_getter, event=None):
        super(AwaitableEvent, self).__init__(name, event)
        self._result_getter = result_getter

    @property
    def event(self):
        return self._result_getter()
/n/n/ntests/ptvsd/test___main__.py/n/nimport unittest

from ptvsd.socket import Address
from ptvsd.__main__ import parse_args
from tests.helpers._io import captured_stdio


class ParseArgsTests(unittest.TestCase):

    EXPECTED_EXTRA = ['--']

    def test_module(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_module_server(self):
        args, extra = parse_args([
            'eggs',
            '--server-host', '10.0.1.1',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_server('10.0.1.1', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_module_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_script(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_script_server(self):
        args, extra = parse_args([
            'eggs',
            '--server-host', '10.0.1.1',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('10.0.1.1', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_script_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote(self):
        args, extra = parse_args([
            'eggs',
            '--host', '1.2.3.4',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_localhost(self):
        args, extra = parse_args([
            'eggs',
            '--host', 'localhost',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('localhost', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--host', '1.2.3.4',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_single_session(self):
        args, extra = parse_args([
            'eggs',
            '--single-session',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('localhost', 8888),
            'nodebug': False,
            'single_session': True,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_local_single_session(self):
        args, extra = parse_args([
            'eggs',
            '--single-session',
            '--server-host', '1.2.3.4',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': True,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_wait(self):
        args, extra = parse_args([
            'eggs',
            '--host', '1.2.3.4',
            '--port', '8888',
            '--wait',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': True,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_extra(self):
        args, extra = parse_args([
            'eggs',
            '--DEBUG',
            '--port', '8888',
            '--vm_type', '???',
            'spam.py',
            '--xyz', '123',
            'abc',
            '--cmd-line',
            '--',
            'foo',
            '--server',
            '--bar'
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, [
            '--DEBUG',
            '--vm_type', '???',
            '--',  # Expected pydevd defaults separator
            '--xyz', '123',
            'abc',
            '--cmd-line',
            'foo',
            '--server',
            '--bar',
        ])

    def test_extra_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--DEBUG',
            '--nodebug',
            '--port', '8888',
            '--vm_type', '???',
            'spam.py',
            '--xyz', '123',
            'abc',
            '--cmd-line',
            '--',
            'foo',
            '--server',
            '--bar'
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, [
            '--DEBUG',
            '--vm_type', '???',
            '--',  # Expected pydevd defaults separator
            '--xyz', '123',
            'abc',
            '--cmd-line',
            'foo',
            '--server',
            '--bar',
        ])

    def test_empty_host(self):
        args, extra = parse_args([
            'eggs',
            '--host', '',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_unsupported_arg(self):
        with self.assertRaises(SystemExit):
            with captured_stdio():
                parse_args([
                    'eggs',
                    '--port', '8888',
                    '--xyz', '123',
                    'spam.py',
                ])

    def test_backward_compatibility_host(self):
        args, extra = parse_args([
            'eggs',
            '--client', '1.2.3.4',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_host_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--client', '1.2.3.4',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_module(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '--module',
            '--file', 'spam:',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_module_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '--module',
            '--file', 'spam:',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_script(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '--file', 'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_script_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '--file', 'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_pseudo_backward_compatibility(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '--module',
            '--file', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, ['--module'] + self.EXPECTED_EXTRA)

    def test_pseudo_backward_compatibility_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '--module',
            '--file', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, ['--module'] + self.EXPECTED_EXTRA)
/n/n/ntests/system_tests/test_connection.py/n/nfrom __future__ import print_function

import contextlib
import os
import time
import sys
import unittest

import ptvsd._util
from ptvsd.socket import create_client, close_socket
from tests.helpers.proc import Proc
from tests.helpers.workspace import Workspace


@contextlib.contextmanager
def _retrier(timeout=1, persec=10, max=None, verbose=False):
    steps = int(timeout * persec) + 1
    delay = 1.0 / persec

    @contextlib.contextmanager
    def attempt(num):
        if verbose:
            print('*', end='')
            sys.stdout.flush()
        yield
        if verbose:
            if num % persec == 0:
                print()
            elif (num * 2) % persec == 0:
                print(' ', end='')

    def attempts():
        # The first attempt always happens.
        num = 1
        with attempt(num):
            yield num
        for num in range(2, steps):
            if max is not None and num > max:
                raise RuntimeError('too many attempts (max {})'.format(max))
            time.sleep(delay)
            with attempt(num):
                yield num
        else:
            raise RuntimeError('timed out')
    yield attempts()
    if verbose:
        print()


class RawConnectionTests(unittest.TestCase):

    VERBOSE = False
    #VERBOSE = True

    def setUp(self):
        super(RawConnectionTests, self).setUp()
        self.workspace = Workspace()
        self.addCleanup(self.workspace.cleanup)

    def _propagate_verbose(self):
        if not self.VERBOSE:
            return

        def unset():
            Proc.VERBOSE = False
            ptvsd._util.DEBUG = False
        self.addCleanup(unset)
        Proc.VERBOSE = True
        ptvsd._util.DEBUG = True

    def _wait_for_ready(self, rpipe):
        if self.VERBOSE:
            print('waiting for ready')
        line = b''
        while True:
            c = os.read(rpipe, 1)
            line += c
            if c == b'\n':
                if self.VERBOSE:
                    print(line.decode('utf-8'), end='')
                if b'getting session socket' in line:
                    break
                line = b''

    @unittest.skip('there is a race here under travis')
    def test_repeated(self):
        def debug(msg):
            if not self.VERBOSE:
                return
            print(msg)

        def connect(addr, wait=None, closeonly=False):
            sock = create_client()
            try:
                sock.settimeout(1)
                sock.connect(addr)
                debug('>connected')
                if wait is not None:
                    debug('>waiting')
                    time.sleep(wait)
            finally:
                debug('>closing')
                if closeonly:
                    sock.close()
                else:
                    close_socket(sock)
        filename = self.workspace.write('spam.py', content=""""""
            raise Exception('should never run')
            """""")
        addr = ('localhost', 5678)
        self._propagate_verbose()
        rpipe, wpipe = os.pipe()
        self.addCleanup(lambda: os.close(rpipe))
        self.addCleanup(lambda: os.close(wpipe))
        proc = Proc.start_python_module('ptvsd', [
            '--server',
            '--wait',
            '--port', '5678',
            '--file', filename,
        ], env={
            'PTVSD_DEBUG': '1',
            'PTVSD_SOCKET_TIMEOUT': '1',
        }, stdout=wpipe)
        with proc:
            # Wait for the server to spin up.
            debug('>a')
            with _retrier(timeout=3, verbose=self.VERBOSE) as attempts:
                for _ in attempts:
                    try:
                        connect(addr)
                        break
                    except Exception:
                        pass
            self._wait_for_ready(rpipe)
            debug('>b')
            connect(addr)
            self._wait_for_ready(rpipe)
            # We should be able to handle more connections.
            debug('>c')
            connect(addr)
            self._wait_for_ready(rpipe)
            # Give ptvsd long enough to try sending something.
            debug('>d')
            connect(addr, wait=0.2)
            self._wait_for_ready(rpipe)
            debug('>e')
            connect(addr)
            self._wait_for_ready(rpipe)
            debug('>f')
            connect(addr, closeonly=True)
            self._wait_for_ready(rpipe)
            debug('>g')
            connect(addr)
            self._wait_for_ready(rpipe)
            debug('>h')
            connect(addr)
            self._wait_for_ready(rpipe)
/n/n/n",0
33,f453ed1c417993eab4fc7b3c5288208d97270d13,"/ptvsd/__main__.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import argparse
import os.path
import sys

from ptvsd._local import debug_main, run_main
from ptvsd.socket import Address
from ptvsd.version import __version__, __author__  # noqa


##################################
# the script

""""""
For the PyDevd CLI handling see:

  https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py
  https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py#L1450  (main func)
""""""  # noqa

PYDEVD_OPTS = {
    '--file',
    '--client',
    #'--port',
    '--vm_type',
}

PYDEVD_FLAGS = {
    '--DEBUG',
    '--DEBUG_RECORD_SOCKET_READS',
    '--cmd-line',
    '--module',
    '--multiproc',
    '--multiprocess',
    '--print-in-debugger-startup',
    '--save-signatures',
    '--save-threading',
    '--save-asyncio',
    '--server',
    '--qt-support=auto',
}

USAGE = """"""
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT -m MODULE [arg ...]
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT FILENAME [arg ...]
""""""  # noqa


def parse_args(argv=None):
    """"""Return the parsed args to use in main().""""""
    if argv is None:
        argv = sys.argv
        prog = argv[0]
        if prog == __file__:
            prog = '{} -m ptvsd'.format(os.path.basename(sys.executable))
    else:
        prog = argv[0]
    argv = argv[1:]

    supported, pydevd, script = _group_args(argv)
    args = _parse_args(prog, supported)
    # '--' is used in _run_args to extract pydevd specific args
    extra = pydevd + ['--']
    if script:
        extra += script
    return args, extra


def _group_args(argv):
    supported = []
    pydevd = []
    script = []

    try:
        pos = argv.index('--')
    except ValueError:
        script = []
    else:
        script = argv[pos + 1:]
        argv = argv[:pos]

    for arg in argv:
        if arg == '-h' or arg == '--help':
            return argv, [], script

    gottarget = False
    skip = 0
    for i in range(len(argv)):
        if skip:
            skip -= 1
            continue

        arg = argv[i]
        try:
            nextarg = argv[i + 1]
        except IndexError:
            nextarg = None

        # TODO: Deprecate the PyDevd arg support.
        # PyDevd support
        if gottarget:
            script = argv[i:] + script
            break
        if arg == '--client':
            arg = '--host'
        elif arg == '--file':
            if nextarg is None:  # The filename is missing...
                pydevd.append(arg)
                continue  # This will get handled later.
            if nextarg.endswith(':') and '--module' in pydevd:
                pydevd.remove('--module')
                arg = '-m'
                argv[i + 1] = nextarg = nextarg[:-1]
            else:
                arg = nextarg
                skip += 1

        if arg in PYDEVD_OPTS:
            pydevd.append(arg)
            if nextarg is not None:
                pydevd.append(nextarg)
            skip += 1
        elif arg in PYDEVD_FLAGS:
            pydevd.append(arg)
        elif arg == '--nodebug':
            supported.append(arg)

        # ptvsd support
        elif arg in ('--host', '--server-host', '--port', '-m'):
            if arg == '-m':
                gottarget = True
            supported.append(arg)
            if nextarg is not None:
                supported.append(nextarg)
            skip += 1
        elif arg in ('--single-session',):
            supported.append(arg)
        elif not arg.startswith('-'):
            supported.append(arg)
            gottarget = True

        # unsupported arg
        else:
            supported.append(arg)
            break

    return supported, pydevd, script


def _parse_args(prog, argv):
    parser = argparse.ArgumentParser(
        prog=prog,
        usage=USAGE.format(prog),
    )
    parser.add_argument('--nodebug', action='store_true')
    host = parser.add_mutually_exclusive_group()
    host.add_argument('--host')
    host.add_argument('--server-host')
    parser.add_argument('--port', type=int, required=True)

    target = parser.add_mutually_exclusive_group(required=True)
    target.add_argument('-m', dest='module')
    target.add_argument('filename', nargs='?')

    parser.add_argument('--single-session', action='store_true')
    parser.add_argument('-V', '--version', action='version')
    parser.version = __version__

    args = parser.parse_args(argv)
    ns = vars(args)

    serverhost = ns.pop('server_host', None)
    clienthost = ns.pop('host', None)
    if serverhost:
        args.address = Address.as_server(serverhost, ns.pop('port'))
    elif not clienthost:
        if args.nodebug:
            args.address = Address.as_client(clienthost, ns.pop('port'))
        else:
            args.address = Address.as_server(clienthost, ns.pop('port'))
    else:
        args.address = Address.as_client(clienthost, ns.pop('port'))

    module = ns.pop('module')
    filename = ns.pop('filename')
    if module is None:
        args.name = filename
        args.kind = 'script'
    else:
        args.name = module
        args.kind = 'module'
    #if argv[-1] != args.name or (module and argv[-1] != '-m'):
    #    parser.error('script/module must be last arg')

    return args


def main(addr, name, kind, extra=(), nodebug=False, **kwargs):
    if nodebug:
        run_main(addr, name, kind, *extra, **kwargs)
    else:
        debug_main(addr, name, kind, *extra, **kwargs)


if __name__ == '__main__':
    args, extra = parse_args()
    main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,
         singlesession=args.single_session)
/n/n/n/tests/helpers/debugclient.py/n/nfrom __future__ import absolute_import

import os
import traceback
import warnings

from ptvsd.socket import Address
from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .debugadapter import DebugAdapter, wait_for_socket_server
from .debugsession import DebugSession

# TODO: Add a helper function to start a remote debugger for testing
# remote debugging?


class _LifecycleClient(Closeable):

    SESSION = DebugSession

    def __init__(
            self,
            addr=None,
            port=8888,
            breakpoints=None,
            connecttimeout=1.0,
    ):
        super(_LifecycleClient, self).__init__()
        self._addr = Address.from_raw(addr, defaultport=port)
        self._connecttimeout = connecttimeout
        self._adapter = None
        self._session = None

        self._breakpoints = breakpoints

    @property
    def adapter(self):
        return self._adapter

    @property
    def session(self):
        return self._session

    def start_debugging(self, launchcfg):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        raise NotImplementedError

    def stop_debugging(self):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')

        if self._session is not None:
            self._detach()

        try:
            self._adapter.close()
        except ClosedError:
            pass
        self._adapter = None

    def attach_pid(self, pid, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        raise NotImplementedError

    def attach_socket(self, addr=None, adapter=None, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if adapter is None:
            adapter = self._adapter
        elif self._adapter is not None:
            raise RuntimeError('already using managed adapter')
        if adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        if addr is None:
            addr = adapter.address
        self._attach(addr, **kwargs)
        return self._session

    def detach(self, adapter=None):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._session is None:
            raise RuntimeError('not attached')
        if adapter is None:
            adapter = self._adapter
        assert adapter is not None
        if not self._session.is_client:
            raise RuntimeError('detach not supported')

        self._detach()

    # internal methods

    def _close(self):
        if self._session is not None:
            try:
                self._session.close()
            except ClosedError:
                pass
        if self._adapter is not None:
            try:
                self._adapter.close()
            except ClosedError:
                pass

    def _launch(self,
                argv,
                script=None,
                wait_for_connect=None,
                detachable=True,
                env=None,
                cwd=None,
                **kwargs):
        if script is not None:
            def start(*args, **kwargs):
                return DebugAdapter.start_wrapper_script(
                    script, *args, **kwargs)
        else:
            start = DebugAdapter.start
        new_addr = Address.as_server if detachable else Address.as_client
        addr = new_addr(None, self._addr.port)
        self._adapter = start(argv, addr=addr, env=env, cwd=cwd)

        if wait_for_connect:
            wait_for_connect()
        else:
            wait_for_socket_server(addr)
            self._attach(addr, **kwargs)

    def _attach(self, addr, **kwargs):
        if addr is None:
            addr = self._addr
        assert addr.host == 'localhost'
        self._session = self.SESSION.create_client(addr, **kwargs)

    def _detach(self):
        session = self._session
        if session is None:
            return
        self._session = None
        try:
            session.close()
        except ClosedError:
            pass


class DebugClient(_LifecycleClient):
    """"""A high-level abstraction of a debug client (i.e. editor).""""""

    # TODO: Manage breakpoints, etc.
    # TODO: Add debugger methods here (e.g. ""pause"").


class EasyDebugClient(DebugClient):
    def start_detached(self, argv):
        """"""Start an adapter in a background process.""""""
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        # TODO: Launch, handshake and detach?
        self._adapter = DebugAdapter.start(argv, port=self._port)
        return self._adapter

    def host_local_debugger(self,
                            argv,
                            script=None,
                            env=None,
                            cwd=None,
                            **kwargs):  # noqa
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None
        addr = ('localhost', self._addr.port)

        self._run_server_ex = None

        def run():
            try:
                self._session = self.SESSION.create_server(addr, **kwargs)
            except Exception as ex:
                self._run_server_ex = traceback.format_exc()

        t = new_hidden_thread(
            target=run,
            name='test.client',
        )
        t.start()

        def wait():
            t.join(timeout=self._connecttimeout)
            if t.is_alive():
                warnings.warn('timed out waiting for connection')
            if self._session is None:
                message = 'unable to connect after {} secs'.format(  # noqa
                    self._connecttimeout)
                if self._run_server_ex is None:
                    raise Exception(message)
                else:
                    message = message + os.linesep + self._run_server_ex # noqa
                    raise Exception(message)

            # The adapter will close when the connection does.

        self._launch(
            argv,
            script=script,
            wait_for_connect=wait,
            detachable=False,
            env=env,
            cwd=cwd)

        return self._adapter, self._session

    def launch_script(self, filename, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            filename,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session

    def launch_module(self, module, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            '-m',
            module,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session
/n/n/n/tests/helpers/debugsession.py/n/nfrom __future__ import absolute_import, print_function

import contextlib
import json
import socket
import sys
import time
import threading
import warnings

from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .message import (
    raw_read_all as read_messages,
    raw_write_one as write_message
)
from .socket import (
    Connection, create_server, create_client, close,
    recv_as_read, send_as_write,
    timeout as socket_timeout)
from .threading import get_locked_and_waiter
from .vsc import parse_message


class DebugSessionConnection(Closeable):

    VERBOSE = False
    #VERBOSE = True

    TIMEOUT = 5.0

    @classmethod
    def create_client(cls, addr, **kwargs):
        def connect(addr, timeout):
            sock = create_client()
            for _ in range(int(timeout * 10)):
                try:
                    sock.connect(addr)
                except (OSError, socket.error):
                    if cls.VERBOSE:
                        print('+', end='')
                        sys.stdout.flush()
                    time.sleep(0.1)
                else:
                    break
            else:
                raise RuntimeError('could not connect')
            return sock
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def create_server(cls, addr, **kwargs):
        def connect(addr, timeout):
            server = create_server(addr)
            with socket_timeout(server, timeout):
                client, _ = server.accept()
            return Connection(client, server)
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def _create(cls, connect, addr, timeout=None):
        if timeout is None:
            timeout = cls.TIMEOUT
        sock = connect(addr, timeout)
        if cls.VERBOSE:
            print('connected')
        self = cls(sock, ownsock=True)
        self._addr = addr
        return self

    def __init__(self, sock, ownsock=False):
        super(DebugSessionConnection, self).__init__()
        self._sock = sock
        self._ownsock = ownsock

    @property
    def is_client(self):
        try:
            return self._sock.server is None
        except AttributeError:
            return True

    def iter_messages(self):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        read = recv_as_read(self._sock)
        for msg, _, _ in read_messages(read, stop=stop):
            if self.VERBOSE:
                print(repr(msg))
            yield parse_message(msg)

    def send(self, req):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        write = send_as_write(self._sock)
        body = json.dumps(req)
        write_message(write, body, stop=stop)

    # internal methods

    def _close(self):
        if self._ownsock:
            close(self._sock)


class DebugSession(Closeable):

    VERBOSE = False
    #VERBOSE = True

    HOST = 'localhost'
    PORT = 8888

    TIMEOUT = None

    @classmethod
    def create_client(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_client(
            addr,
            timeout=kwargs.get('timeout'),
        )
        return cls(conn, owned=True, **kwargs)

    @classmethod
    def create_server(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_server(addr, **kwargs)
        return cls(conn, owned=True, **kwargs)

    def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False):
        super(DebugSession, self).__init__()
        self._conn = conn
        self._seq = seq
        self._timeout = timeout
        self._owned = owned

        self._handlers = []
        for handler in handlers:
            if callable(handler):
                self._add_handler(handler)
            else:
                self._add_handler(*handler)
        self._received = []
        self._listenerthread = new_hidden_thread(
            target=self._listen,
            name='test.session',
        )
        self._listenerthread.start()

    @property
    def is_client(self):
        return self._conn.is_client

    @property
    def received(self):
        return list(self._received)

    def _create_request(self, command, **args):
        seq = self._seq
        self._seq += 1
        return {
            'type': 'request',
            'seq': seq,
            'command': command,
            'arguments': args,
        }

    def send_request(self, command, **args):
        if self.closed:
            raise RuntimeError('session closed')

        wait = args.pop('wait', False)
        req = self._create_request(command, **args)
        if self.VERBOSE:
            msg = parse_message(req)
            print(' <-', msg)

        if wait:
            with self.wait_for_response(req) as resp:
                self._conn.send(req)
            resp_awaiter = AwaitableResponse(req, lambda: resp[""msg""])
        else:
            resp_awaiter = self._get_awaiter_for_request(req, **args)
            self._conn.send(req)
        return resp_awaiter

    def add_handler(self, handler, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        self._add_handler(handler, **kwargs)

    @contextlib.contextmanager
    def wait_for_event(self, event, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event
        handlername = 'event {!r}'.format(event)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): # noqa
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event and condition(msg) # noqa
        handlername = 'event {!r}'.format(event)
        evt = self._get_message_handle(match, handlername)

        return AwaitableEvent(event, lambda: result[""msg""], evt)

    def _get_awaiter_for_request(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        evt = self._get_message_handle(match, handlername)

        return AwaitableResponse(req, lambda: result[""msg""], evt)

    @contextlib.contextmanager
    def wait_for_response(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    # internal methods

    def _close(self):
        if self._owned:
            try:
                self._conn.close()
            except ClosedError:
                pass
        if self._listenerthread != threading.current_thread():
            self._listenerthread.join(timeout=1.0)
            if self._listenerthread.is_alive():
                warnings.warn('session listener still running')
        self._check_handlers()

    def _listen(self):
        try:
            for msg in self._conn.iter_messages():
                if self.VERBOSE:
                    print(' ->', msg)
                self._receive_message(msg)
        except EOFError:
            try:
                self.close()
            except ClosedError:
                pass

    def _receive_message(self, msg):
        for i, handler in enumerate(list(self._handlers)):
            handle_message, _, _ = handler
            handled = handle_message(msg)
            try:
                msg, handled = handled
            except TypeError:
                pass
            if handled:
                self._handlers.remove(handler)
                break
        self._received.append(msg)

    def _add_handler(self, handle_msg, handlername=None, required=True):
        self._handlers.append(
            (handle_msg, handlername, required))

    def _check_handlers(self):
        unhandled = []
        for handle_msg, name, required in self._handlers:
            if not required:
                continue
            unhandled.append(name or repr(handle_msg))
        if unhandled:
            raise RuntimeError('unhandled: {}'.format(unhandled))

    @contextlib.contextmanager
    def _wait_for_message(self, match, handlername, timeout=None):
        if timeout is None:
            timeout = self.TIMEOUT
        lock, wait = get_locked_and_waiter()

        def handler(msg):
            if not match(msg):
                return msg, False
            lock.release()
            return msg, True
        self._add_handler(handler, handlername)
        try:
            yield
        finally:
            wait(timeout or self._timeout, handlername, fail=True)

    def _get_message_handle(self, match, handlername):
        event = threading.Event()

        def handler(msg):
            if not match(msg):
                return msg, False
            event.set()
            return msg, True
        self._add_handler(handler, handlername, False)
        return event


class Awaitable(object):

    @classmethod
    def wait_all(cls, *awaitables):
        timeout = 3.0
        messages = []
        for _ in range(int(timeout * 10)):
            time.sleep(0.1)
            messages = []
            not_ready = (a for a in awaitables if a._event is not None and not a._event.is_set()) # noqa
            for awaitable in not_ready:
                if isinstance(awaitable, AwaitableEvent):
                    messages.append('Event {}'.format(awaitable.name))
                else:
                    messages.append('Response {}'.format(awaitable.name))
            if len(messages) == 0:
                return
        else:
            raise TimeoutError('Timeout waiting for {}'.format(','.join(messages))) # noqa

    def __init__(self, name, event=None):
        self._event = event
        self.name = name

    def wait(self, timeout=1.0):
        if self._event is None:
            return
        if not self._event.wait(timeout):
            message = 'Timeout waiting for '
            if isinstance(self, AwaitableEvent):
                message += 'Event {}'.format(self.name)
            else:
                message += 'Response {}'.format(self.name)
            raise TimeoutError(message)


class AwaitableResponse(Awaitable):

    def __init__(self, req, result_getter, event=None):
        super(AwaitableResponse, self).__init__(req[""command""], event)
        self.req = req
        self._result_getter = result_getter

    @property
    def resp(self):
        return self._result_getter()


class AwaitableEvent(Awaitable):

    def __init__(self, name, result_getter, event=None):
        super(AwaitableEvent, self).__init__(name, event)
        self._result_getter = result_getter

    @property
    def event(self):
        return self._result_getter()
/n/n/n",1
34,bd037e882d675ea27b96d41faf0deeac6563695c,"reddytt.py/n/n#!/usr/bin/env python3

################
# Imports
################

import os
import pickle
from bs4 import BeautifulSoup
import urllib3
import certifi
import re
import subprocess
import sys
import argparse as ap
#from argparse import ArgumentParser, REMINDER

################
# Functions
################

# Function to flatten a list
flatten = lambda l: [item for sublist in l for item in sublist]
# cheers to https://stackoverflow.com/a/952952

# Get and parse out links
def getytlinks(link):
    pm = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())
    html_page = pm.request('GET', link)
    soup = BeautifulSoup(html_page.data, ""lxml"")
    links = [a.get('href') for a in soup('a') if a.get('href')]

    # Pick out youtube links
    new_links = [x for x in links if re.match(""^https://youtu\.be"", x)]
    newer_links = [x for x in links if re.match(""^https://www\.youtube\.com/watch"", x)]
    # the youtube.com links are not always well formatted for mpv, so we reformat them:
    for lk in newer_links:
        videolabel = re.search('v=([^&?]*)', lk)[1]
        if videolabel is None:
            print('Reddytt: skipping URL without video label:', lk)
            continue
        new_links.append('https://www.youtube.com/watch?v=' + videolabel)
    # in principal, add anything here you want. I guess all of these should work: https://rg3.github.io/youtube-dl/supportedsites.html
    return new_links, links

################
# Main
################

if __name__ == '__main__':

    parser = ap.ArgumentParser(usage='%(prog)s [options] <subreddit> [-- [mpv-arguments]]', description='Play the youtube links from your favourite subreddit.')

    parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.')
    parser.add_argument('subreddit', type=str, help='The subreddit you want to play.')
    parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.')

    args = parser.parse_args()

    subreddit = args.subreddit
    depth = args.depth

    subreddit_link = ""https://reddit.com/r/"" + subreddit

    # Setup working directory
    work_dir = os.environ['HOME'] + ""/.reddytt""
    sr_dir = work_dir + ""/%s"" % subreddit
    seen_file = sr_dir + ""/seen""
    seen_links = []
    unseen_file = sr_dir + ""/unseen""
    unseen_links = []
    print(""Reddytt: Checking for reddytt working directory (%s)."" % work_dir)

    if not os.path.isdir(work_dir):
        print(""Reddytt: Working directory not found. Creating %s, and files."" % work_dir)
        os.mkdir(work_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    elif not os.path.isdir(sr_dir):
        print(""Reddytt: Working directory found, but no subreddit directory. Creating %s, and files."" % sr_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    else:
        print(""Reddytt: Working directory found. Loading variables."")
        with open(seen_file, 'rb') as f:
            seen_links = pickle.load(f)
        with open(unseen_file, 'rb') as f:
            unseen_links = pickle.load(f)

    new_links, links = getytlinks(subreddit_link)

    # Go deeper
    if depth > 0:
        for d in range(depth):
            link = """"
            for l in links:
                if re.search(""after="", l):
                    link = l
            if link == """":
                print(""Reddytt: Could not identify 'after'-variable to progress deeper."")
            else:
                newer_links, links = getytlinks(link)
                new_links += newer_links
                new_links = list(set(new_links))

    # we also want to watch the stored ones
    new_links += unseen_links
    new_links = list(set(new_links))

    # Start watching
    save_links = new_links
    for link in new_links:
        if link in seen_links:
            print(""Reddytt: Link seen. Skipping."")
        else:
            p = subprocess.Popen(['mpv', link] + args.mpv, shell=False)
            p.communicate()
            print(""Reddytt: That was: %s"" % link)
            if p.returncode == 0:
                # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.
                # Store the video in seen_links.
                seen_links.append(link)
                save_links.remove(link)
            elif p.returncode == 4:
                # You made a hard exit, and want to stop. (Ctrl+C)
                # Store the links and exit the program.
                print(""Reddytt: Forced exit detected. Saving and exiting."")
                with open(seen_file, 'wb') as f:
                    pickle.dump(seen_links, f)
                with open(unseen_file, 'wb') as f:
                    pickle.dump(save_links, f)
                # Exit program.
                sys.exit()
            else:
                # Something else happened. Bad link perhaps.
                # Store in seen_links to avoid in the future.

                seen_links.append(link)
                save_links.remove(link)

    # The playlist is finished. Save everything.
    with open(seen_file, 'wb') as f:
        pickle.dump(seen_links, f)
    with open(unseen_file, 'wb') as f:
        pickle.dump(save_links, f)
/n/n/n",0
35,bd037e882d675ea27b96d41faf0deeac6563695c,"/reddytt.py/n/n#!/usr/bin/env python3

################
# Imports
################

import os
import pickle
from bs4 import BeautifulSoup
import urllib3
import certifi
import re
import sys
import argparse as ap
#from argparse import ArgumentParser, REMINDER

################
# Functions
################

# Function to flatten a list
flatten = lambda l: [item for sublist in l for item in sublist]
# cheers to https://stackoverflow.com/a/952952

# Get and parse out links
def getytlinks(link):
    pm = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())
    html_page = pm.request('GET', link)
    soup = BeautifulSoup(html_page.data, ""lxml"")
    links = [a.get('href') for a in soup('a') if a.get('href')]

    # Pick out youtube links
    new_links = [x for x in links if re.match(""^https://youtu\.be"", x)]
    newer_links = [x for x in links if re.match(""^https://www\.youtube\.com/watch"", x)]
    # the youtube.com links are not always well formatted for mpv, so we reformat them:
    for lk in newer_links:
        videolabel = re.search('v=([^&?]*)', lk)[1]
        if videolabel is None:
            print('Reddytt: skipping URL without video label:', lk)
            continue
        new_links.append('https://www.youtube.com/watch?v=' + videolabel)
    # in principal, add anything here you want. I guess all of these should work: https://rg3.github.io/youtube-dl/supportedsites.html
    return new_links, links

################
# Main
################

if __name__ == '__main__':

    parser = ap.ArgumentParser(usage='%(prog)s [options] <subreddit> [-- [mpv-arguments]]', description='Play the youtube links from your favourite subreddit.')

    parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.')
    parser.add_argument('subreddit', type=str, help='The subreddit you want to play.')
    parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.')

    args = parser.parse_args()

    subreddit = args.subreddit
    depth = args.depth
    mpv = "" "".join(args.mpv)

    subreddit_link = ""https://reddit.com/r/"" + subreddit

    # Setup working directory
    work_dir = os.environ['HOME'] + ""/.reddytt""
    sr_dir = work_dir + ""/%s"" % subreddit
    seen_file = sr_dir + ""/seen""
    seen_links = []
    unseen_file = sr_dir + ""/unseen""
    unseen_links = []
    print(""Reddytt: Checking for reddytt working directory (%s)."" % work_dir)

    if not os.path.isdir(work_dir):
        print(""Reddytt: Working directory not found. Creating %s, and files."" % work_dir)
        os.mkdir(work_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    elif not os.path.isdir(sr_dir):
        print(""Reddytt: Working directory found, but no subreddit directory. Creating %s, and files."" % sr_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    else:
        print(""Reddytt: Working directory found. Loading variables."")
        with open(seen_file, 'rb') as f:
            seen_links = pickle.load(f)
        with open(unseen_file, 'rb') as f:
            unseen_links = pickle.load(f)

    new_links, links = getytlinks(subreddit_link)

    # Go deeper
    if depth > 0:
        for d in range(depth):
            link = """"
            for l in links:
                if re.search(""after="", l):
                    link = l
            if link == """":
                print(""Reddytt: Could not identify 'after'-variable to progress deeper."")
            else:
                newer_links, links = getytlinks(link)
                new_links += newer_links
                new_links = list(set(new_links))

    # we also want to watch the stored ones
    new_links += unseen_links
    new_links = list(set(new_links))

    # Start watching
    save_links = new_links
    for link in new_links:
        if link in seen_links:
            print(""Reddytt: Link seen. Skipping."")
        else:
            x = os.system(""mpv %(args)s %(link)s"" % {""link"": link, ""args"": mpv})
            print(""Reddytt: That was: %s"" % link)
            if x == 0:
                # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.
                # Store the video in seen_links.
                seen_links.append(link)
                save_links.remove(link)
            elif x == 1024:
                # You made a hard exit, and want to stop. (Ctrl+C)
                # Store the links and exit the program.
                print(""Reddytt: Forced exit detected. Saving and exiting."")
                with open(seen_file, 'wb') as f:
                    pickle.dump(seen_links, f)
                with open(unseen_file, 'wb') as f:
                    pickle.dump(save_links, f)
                # Exit program.
                sys.exit()
            else:
                # Something else happened. Bad link perhaps.
                # Store in seen_links to avoid in the future.

                seen_links.append(link)
                save_links.remove(link)

    # The playlist is finished. Save everything.
    with open(seen_file, 'wb') as f:
        pickle.dump(seen_links, f)
    with open(unseen_file, 'wb') as f:
        pickle.dump(save_links, f)
/n/n/n",1
36,c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f,"Get_the_machine_learning_basics/Classification_Template/classification_template.py/n/n# Classification template

# Importing the libraries
import numpy as np
import matplotlib
matplotlib.use(""tkAgg"")
import matplotlib.pyplot as plt
import pandas as pd
import os

script_dir = os.path.dirname(__file__)
abs_file_path = os.path.join(script_dir, 'Social_Network_Ads.csv')

# Importing the dataset
dataset = pd.read_csv(abs_file_path)
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting classifier to the Training set
# Create your classifier here
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap

X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c=ListedColormap(('red', 'green'))(i), label=j)
plt.title('Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()/n/n/nGet_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py/n/n# Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os

script_dir = os.path.dirname(__file__)
abs_file_path = os.path.join(script_dir, 'Data.csv')

# Importing the dataset
dataset = pd.read_csv(abs_file_path)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values/n/n/nVolume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py/n/n# Artificial Neural Network

# Installing Theano
# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git

# Installing Tensorflow
# pip install tensorflow

# Installing Keras
# pip install --upgrade keras

# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os

script_dir = os.path.dirname(__file__)
abs_file_path = os.path.join(script_dir, 'Churn_Modelling.csv')

# Importing the dataset
dataset = pd.read_csv(abs_file_path)
X = dataset.iloc[:, 3:13].values
y = dataset.iloc[:, 13].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
onehotencoder = OneHotEncoder(categorical_features=[1])
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))

# Adding the second hidden layer
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))

# Adding the output layer
classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

# Compiling the ANN
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size=10, epochs=100)

# Part 3 - Making predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)/n/n/n",0
37,c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f,"/Get_the_machine_learning_basics/Classification_Template/classification_template.py/n/n# Classification template

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting classifier to the Training set
# Create your classifier here

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap

X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c=ListedColormap(('red', 'green'))(i), label=j)
plt.title('Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()/n/n/n/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py/n/n# Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Data.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values/n/n/n/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py/n/n# Artificial Neural Network

# Installing Theano
# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git

# Installing Tensorflow
# pip install tensorflow

# Installing Keras
# pip install --upgrade keras

# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13].values
y = dataset.iloc[:, 13].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
onehotencoder = OneHotEncoder(categorical_features = [1])
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)

# Part 3 - Making predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)/n/n/n",1
38,d7c7d42b2b3e4c024a624c2cf21b07dede26bce0,"dciagent/plugins/ansibleplugin.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016 Red Hat, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ansible import inventory
from ansible import vars
from ansible.executor import playbook_executor
from ansible.parsing import dataloader

from ansible.utils.display import Display

from dciclient.v1 import helper as dci_helper
from dciagent.plugins import plugin


import jinja2
import os
import subprocess


class Options(object):
    def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,
                 forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None,
                 output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None,
                 sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None,
                 ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None,
                 sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None,
                 syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None):
        self.verbosity = verbosity
        self.inventory = inventory
        self.listhosts = listhosts
        self.subset = subset
        self.module_paths = module_paths
        self.extra_vars = extra_vars
        self.forks = forks
        self.ask_vault_pass = ask_vault_pass
        self.vault_password_files = vault_password_files
        self.new_vault_password_file = new_vault_password_file
        self.output_file = output_file
        self.tags = tags
        self.skip_tags = skip_tags
        self.one_line = one_line
        self.tree = tree
        self.ask_sudo_pass = ask_sudo_pass
        self.ask_su_pass = ask_su_pass
        self.sudo = sudo
        self.sudo_user = sudo_user
        self.become = become
        self.become_method = become_method
        self.become_user = become_user
        self.become_ask_pass = become_ask_pass
        self.ask_pass = ask_pass
        self.private_key_file = private_key_file
        self.remote_user = remote_user
        self.connection = connection
        self.timeout = timeout
        self.ssh_common_args = ssh_common_args
        self.sftp_extra_args = sftp_extra_args
        self.scp_extra_args = scp_extra_args
        self.ssh_extra_args = ssh_extra_args
        self.poll_interval = poll_interval
        self.seconds = seconds
        self.check = check
        self.syntax = syntax
        self.diff = diff
        self.force_handlers = force_handlers
        self.flush_cache = flush_cache
        self.listtasks = listtasks
        self.listtags = listtags
        self.module_path = module_path


class Runner(object):

    def __init__(self, playbook, options=None, verbosity=0):

        if options is None:
            self.options = Options()
            self.options.verbosity = verbosity
            self.options.connection = 'ssh'
            self.options.become = True
            self.options.become_method = 'sudo'
            self.options.become_user = 'root'

        self.loader = dataloader.DataLoader()
        self.variable_manager = vars.VariableManager()

        self.inventory = inventory.Inventory(
            loader=self.loader,
            variable_manager=self.variable_manager,
            host_list='/etc/ansible/hosts'
        )
        self.variable_manager.set_inventory(self.inventory)

        # Playbook to run, from the current working directory.
        pb_dir = os.path.abspath('.')
        playbook_path = ""%s/%s"" % (pb_dir, playbook)

        self.pbex = playbook_executor.PlaybookExecutor(
            playbooks=[playbook],
            inventory=self.inventory,
            variable_manager=self.variable_manager,
            loader=self.loader,
            options=self.options,
            passwords={})

    def run(self, job_id):
        """"""Run the playbook and returns the playbook's stats.""""""

        self.variable_manager.extra_vars = {'job_id': job_id}
        self.pbex.run()
        return self.pbex._tqm._stats


class AnsiblePlugin(plugin.Plugin):

    def __init__(self, conf):
        super(AnsiblePlugin, self).__init__(conf)


    def generate_ansible_playbook_from_template(self, template_file, data):

        templateLoader = jinja2.FileSystemLoader( searchpath=""/"" )
        templateEnv = jinja2.Environment( loader=templateLoader )
        template = templateEnv.get_template( template_file )
        outputText = template.render( data )

        return outputText


    def run(self, state, data=None, context=None):
        """"""Run ansible-playbook on the specified playbook. """"""

        playbook = None
        log_file = None
        template = None

        if state in self.conf:
            if 'playbook' in self.conf[state]:
                playbook = self.conf[state]['playbook']
            if 'log_file' in self.conf[state]:
                log_file = self.conf[state]['log_file']
            if 'template' in self.conf[state]:
                template = self.conf[state]['template']

        if playbook is None:
            playbook = self.conf['playbook']
        if template is None and template in self.conf:
            template = self.conf['template']

        if log_file is None:
            if 'log_file' in self.conf:
                log_file = self.conf['log_file']
            else:
                log_file = open(os.devnull, 'w')

        if template:
            open(playbook, 'w').write(
                self.generate_ansible_playbook_from_template(template, data)
            )
            
        runner = Runner(playbook=playbook, verbosity=0)
        stats = runner.run(job_id=context.last_job_id)
/n/n/n",0
39,d7c7d42b2b3e4c024a624c2cf21b07dede26bce0,"/dciagent/plugins/ansibleplugin.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016 Red Hat, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ansible import inventory
from ansible import vars
from ansible.executor import playbook_executor
from ansible.parsing import dataloader

from ansible.utils.display import Display

from dciclient.v1 import helper as dci_helper
from dciagent.plugins import plugin


import jinja2
import os
import subprocess

display = Display()

class Options(object):
    def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,
                 forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None,
                 output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None,
                 sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None,
                 ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None,
                 sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None,
                 syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None):
        self.verbosity = verbosity
        self.inventory = inventory
        self.listhosts = listhosts
        self.subset = subset
        self.module_paths = module_paths
        self.extra_vars = extra_vars
        self.forks = forks
        self.ask_vault_pass = ask_vault_pass
        self.vault_password_files = vault_password_files
        self.new_vault_password_file = new_vault_password_file
        self.output_file = output_file
        self.tags = tags
        self.skip_tags = skip_tags
        self.one_line = one_line
        self.tree = tree
        self.ask_sudo_pass = ask_sudo_pass
        self.ask_su_pass = ask_su_pass
        self.sudo = sudo
        self.sudo_user = sudo_user
        self.become = become
        self.become_method = become_method
        self.become_user = become_user
        self.become_ask_pass = become_ask_pass
        self.ask_pass = ask_pass
        self.private_key_file = private_key_file
        self.remote_user = remote_user
        self.connection = connection
        self.timeout = timeout
        self.ssh_common_args = ssh_common_args
        self.sftp_extra_args = sftp_extra_args
        self.scp_extra_args = scp_extra_args
        self.ssh_extra_args = ssh_extra_args
        self.poll_interval = poll_interval
        self.seconds = seconds
        self.check = check
        self.syntax = syntax
        self.diff = diff
        self.force_handlers = force_handlers
        self.flush_cache = flush_cache
        self.listtasks = listtasks
        self.listtags = listtags
        self.module_path = module_path


class Runner(object):

    def __init__(self, playbook, options=None, verbosity=0):

        if options is None:
            self.options = Options()
            self.options.verbosity = verbosity

        self.loader = dataloader.DataLoader()
        self.variable_manager = vars.VariableManager()

        self.inventory = inventory.Inventory(
            loader=self.loader,
            variable_manager=self.variable_manager,
            host_list='/etc/ansible/hosts'
        )
        self.variable_manager.set_inventory(self.inventory)

        # Playbook to run, from the current working directory.
        pb_dir = os.path.abspath('.')
        playbook_path = ""%s/%s"" % (pb_dir, playbook)
        display.verbosity = self.options.verbosity

        self.pbex = playbook_executor.PlaybookExecutor(
            #playbooks=[playbook_path],
            playbooks=[playbook],
            inventory=self.inventory,
            variable_manager=self.variable_manager,
            loader=self.loader,
            options=self.options,
            passwords={})

    def run(self, job_id):
        """"""Run the playbook and returns the playbook's stats.""""""
        self.variable_manager.extra_vars = {'job_id': job_id}
        self.pbex.run()
        return self.pbex._tqm._stats


class AnsiblePlugin(plugin.Plugin):

    def __init__(self, conf):
        super(AnsiblePlugin, self).__init__(conf)


    def generate_ansible_playbook_from_template(self, template_file, data):

        templateLoader = jinja2.FileSystemLoader( searchpath=""/"" )
        templateEnv = jinja2.Environment( loader=templateLoader )
        template = templateEnv.get_template( template_file )
        outputText = template.render( data )

        return outputText





    def run(self, state, data=None, context=None):
        """"""Run ansible-playbook on the specified playbook. """"""

        playbook = None
        log_file = None
        template = None

        if state in self.conf:
            if 'playbook' in self.conf[state]:
                playbook = self.conf[state]['playbook']
            if 'log_file' in self.conf[state]:
                log_file = self.conf[state]['log_file']
            if 'template' in self.conf[state]:
                template = self.conf[state]['template']

        if playbook is None:
            playbook = self.conf['playbook']
        if template is None and template in self.conf:
            template = self.conf['template']

        if log_file is None:
            if 'log_file' in self.conf:
                log_file = self.conf['log_file']
            else:
                log_file = open(os.devnull, 'w')

        if template:
            open(playbook, 'w').write(
                self.generate_ansible_playbook_from_template(template, data)
            )
            
        runner = Runner(playbook=playbook, verbosity=0)
        stats = runner.run(job_id=context.last_job_id)
/n/n/n",1
40,ee20e7e1058d24191320e54f444a5f7c22adb1e8,"azurelinuxagent/common/event.py/n/n# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import atexit
import datetime
import json
import os
import sys
import time
import traceback

from datetime import datetime

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger

from azurelinuxagent.common.exception import EventError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.protocol.restapi import TelemetryEventParam, \
    TelemetryEvent, \
    get_properties
from azurelinuxagent.common.utils import textutil
from azurelinuxagent.common.version import CURRENT_VERSION

_EVENT_MSG = ""Event: name={0}, op={1}, message={2}, duration={3}""


class WALAEventOperation:
    ActivateResourceDisk = ""ActivateResourceDisk""
    AgentBlacklisted = ""AgentBlacklisted""
    AgentEnabled = ""AgentEnabled""
    AutoUpdate = ""AutoUpdate""
    CustomData = ""CustomData""
    Deploy = ""Deploy""
    Disable = ""Disable""
    Downgrade = ""Downgrade""
    Download = ""Download""
    Enable = ""Enable""
    ExtensionProcessing = ""ExtensionProcessing""
    Firewall = ""Firewall""
    GetArtifactExtended = ""GetArtifactExtended""
    HealthCheck = ""HealthCheck""
    HeartBeat = ""HeartBeat""
    HostPlugin = ""HostPlugin""
    HostPluginHeartbeat = ""HostPluginHeartbeat""
    HttpErrors = ""HttpErrors""
    ImdsHeartbeat = ""ImdsHeartbeat""
    Install = ""Install""
    InitializeCGroups = ""InitializeCGroups""
    InitializeHostPlugin = ""InitializeHostPlugin""
    Log = ""Log""
    Partition = ""Partition""
    ProcessGoalState = ""ProcessGoalState""
    Provision = ""Provision""
    ProvisionGuestAgent = ""ProvisionGuestAgent""
    RemoteAccessHandling = ""RemoteAccessHandling""
    ReportStatus = ""ReportStatus""
    ReportStatusExtended = ""ReportStatusExtended""
    Restart = ""Restart""
    SkipUpdate = ""SkipUpdate""
    UnhandledError = ""UnhandledError""
    UnInstall = ""UnInstall""
    Unknown = ""Unknown""
    Upgrade = ""Upgrade""
    Update = ""Update""


SHOULD_ENCODE_MESSAGE_LEN = 80
SHOULD_ENCODE_MESSAGE_OP = [
    WALAEventOperation.Disable,
    WALAEventOperation.Enable,
    WALAEventOperation.Install,
    WALAEventOperation.UnInstall,
]

class EventStatus(object):
    EVENT_STATUS_FILE = ""event_status.json""

    def __init__(self):
        self._path = None
        self._status = {}

    def clear(self):
        self._status = {}
        self._save()

    def event_marked(self, name, version, op):
        return self._event_name(name, version, op) in self._status

    def event_succeeded(self, name, version, op):
        event = self._event_name(name, version, op)
        if event not in self._status:
            return True
        return self._status[event] is True

    def initialize(self, status_dir=conf.get_lib_dir()):
        self._path = os.path.join(status_dir, EventStatus.EVENT_STATUS_FILE)
        self._load()

    def mark_event_status(self, name, version, op, status):
        event = self._event_name(name, version, op)
        self._status[event] = (status is True)
        self._save()

    def _event_name(self, name, version, op):
        return ""{0}-{1}-{2}"".format(name, version, op)

    def _load(self):
        try:
            self._status = {}
            if os.path.isfile(self._path):
                with open(self._path, 'r') as f:
                    self._status = json.load(f)
        except Exception as e:
            logger.warn(""Exception occurred loading event status: {0}"".format(e))
            self._status = {}

    def _save(self):
        try:
            with open(self._path, 'w') as f:
                json.dump(self._status, f)
        except Exception as e:
            logger.warn(""Exception occurred saving event status: {0}"".format(e))


__event_status__ = EventStatus()
__event_status_operations__ = [
        WALAEventOperation.AutoUpdate,
        WALAEventOperation.ReportStatus
    ]


def _encode_message(op, message):
    """"""
    Gzip and base64 encode a message based on the operation.

    The intent of this message is to make the logs human readable and include the
    stdout/stderr from extension operations.  Extension operations tend to generate
    a lot of noise, which makes it difficult to parse the line-oriented waagent.log.
    The compromise is to encode the stdout/stderr so we preserve the data and do
    not destroy the line oriented nature.

    The data can be recovered using the following command:

      $ echo '<encoded data>' | base64 -d | pigz -zd

    You may need to install the pigz command.

    :param op: Operation, e.g. Enable or Install
    :param message: Message to encode
    :return: gzip'ed and base64 encoded message, or the original message
    """"""

    if len(message) == 0:
        return message

    if op not in SHOULD_ENCODE_MESSAGE_OP:
        return message

    try:
        return textutil.compress(message)
    except Exception:
        # If the message could not be encoded a dummy message ('<>') is returned.
        # The original message was still sent via telemetry, so all is not lost.
        return ""<>""


def _log_event(name, op, message, duration, is_success=True):
    global _EVENT_MSG

    message = _encode_message(op, message)
    if not is_success:
        logger.error(_EVENT_MSG, name, op, message, duration)
    else:
        logger.info(_EVENT_MSG, name, op, message, duration)


class EventLogger(object):
    def __init__(self):
        self.event_dir = None
        self.periodic_events = {}

    def save_event(self, data):
        if self.event_dir is None:
            logger.warn(""Cannot save event -- Event reporter is not initialized."")
            return

        if not os.path.exists(self.event_dir):
            os.mkdir(self.event_dir)
            os.chmod(self.event_dir, 0o700)

        existing_events = os.listdir(self.event_dir)
        if len(existing_events) >= 1000:
            existing_events.sort()
            oldest_files = existing_events[:-999]
            logger.warn(""Too many files under: {0}, removing oldest"".format(self.event_dir))
            try:
                for f in oldest_files:
                    os.remove(os.path.join(self.event_dir, f))
            except IOError as e:
                raise EventError(e)

        filename = os.path.join(self.event_dir,
                                ustr(int(time.time() * 1000000)))
        try:
            with open(filename + "".tmp"", 'wb+') as hfile:
                hfile.write(data.encode(""utf-8""))
            os.rename(filename + "".tmp"", filename + "".tld"")
        except IOError as e:
            raise EventError(""Failed to write events to file:{0}"", e)

    def reset_periodic(self):
        self.periodic_events = {}

    def is_period_elapsed(self, delta, h):
        return h not in self.periodic_events or \
            (self.periodic_events[h] + delta) <= datetime.now()

    def add_periodic(self,
        delta, name, op=WALAEventOperation.Unknown, is_success=True, duration=0,
        version=CURRENT_VERSION, message="""", evt_type="""",
        is_internal=False, log_event=True, force=False):

        h = hash(name+op+ustr(is_success)+message)
        
        if force or self.is_period_elapsed(delta, h):
            self.add_event(name,
                op=op, is_success=is_success, duration=duration,
                version=version, message=message, evt_type=evt_type,
                is_internal=is_internal, log_event=log_event)
            self.periodic_events[h] = datetime.now()

    def add_event(self,
                  name,
                  op=WALAEventOperation.Unknown,
                  is_success=True,
                  duration=0,
                  version=CURRENT_VERSION,
                  message="""",
                  evt_type="""",
                  is_internal=False,
                  log_event=True):

        if not is_success or log_event:
            _log_event(name, op, message, duration, is_success=is_success)

        self._add_event(duration, evt_type, is_internal, is_success, message, name, op, version, eventId=1)
        self._add_event(duration, evt_type, is_internal, is_success, message, name, op, version, eventId=6)

    def _add_event(self, duration, evt_type, is_internal, is_success, message, name, op, version, eventId):
        event = TelemetryEvent(eventId, ""69B669B9-4AF8-4C50-BDC4-6006FA76E975"")
        event.parameters.append(TelemetryEventParam('Name', name))
        event.parameters.append(TelemetryEventParam('Version', str(version)))
        event.parameters.append(TelemetryEventParam('IsInternal', is_internal))
        event.parameters.append(TelemetryEventParam('Operation', op))
        event.parameters.append(TelemetryEventParam('OperationSuccess',
                                                    is_success))
        event.parameters.append(TelemetryEventParam('Message', message))
        event.parameters.append(TelemetryEventParam('Duration', duration))
        event.parameters.append(TelemetryEventParam('ExtensionType', evt_type))

        data = get_properties(event)
        try:
            self.save_event(json.dumps(data))
        except EventError as e:
            logger.error(""{0}"", e)

    def add_log_event(self, level, message):
        # By the time the message has gotten to this point it is formatted as
        #
        #   YYYY/MM/DD HH:mm:ss.fffffff LEVEL <text>.
        #
        # The timestamp and the level are redundant, and should be stripped.
        # The logging library does not schematize this data, so I am forced
        # to parse the message.  The format is regular, so the burden is low.

        parts = message.split(' ', 3)
        msg = parts[3] if len(parts) == 4 \
            else message

        event = TelemetryEvent(7, ""FFF0196F-EE4C-4EAF-9AA5-776F622DEB4F"")
        event.parameters.append(TelemetryEventParam('EventName', WALAEventOperation.Log))
        event.parameters.append(TelemetryEventParam('CapabilityUsed', logger.LogLevel.STRINGS[level]))
        event.parameters.append(TelemetryEventParam('Context1', msg))
        event.parameters.append(TelemetryEventParam('Context2', ''))
        event.parameters.append(TelemetryEventParam('Context3', ''))

        data = get_properties(event)
        try:
            self.save_event(json.dumps(data))
        except EventError:
            pass

    def add_metric(self, category, counter, instance, value, log_event=False):
        """"""
        Create and save an event which contains a telemetry event.

        :param str category: The category of metric (e.g. ""cpu"", ""memory"")
        :param str counter: The specific metric within the category (e.g. ""%idle"")
        :param str instance: For instanced metrics, the instance identifier (filesystem name, cpu core#, etc.)
        :param value: Value of the metric
        :param bool log_event: If true, log the collected metric in the agent log
        """"""
        if log_event:
            from azurelinuxagent.common.version import AGENT_NAME
            message = ""Metric {0}/{1} [{2}] = {3}"".format(category, counter, instance, value)
            _log_event(AGENT_NAME, ""METRIC"", message, 0)

        event = TelemetryEvent(4, ""69B669B9-4AF8-4C50-BDC4-6006FA76E975"")
        event.parameters.append(TelemetryEventParam('Category', category))
        event.parameters.append(TelemetryEventParam('Counter', counter))
        event.parameters.append(TelemetryEventParam('Instance', instance))
        event.parameters.append(TelemetryEventParam('Value', value))

        data = get_properties(event)
        try:
            self.save_event(json.dumps(data))
        except EventError as e:
            logger.error(""{0}"", e)


__event_logger__ = EventLogger()


def elapsed_milliseconds(utc_start):
    now = datetime.utcnow()
    if now < utc_start:
        return 0

    d = now - utc_start
    return int(((d.days * 24 * 60 * 60 + d.seconds) * 1000) + \
                    (d.microseconds / 1000.0))


def report_event(op, is_success=True, message=''):
    from azurelinuxagent.common.version import AGENT_NAME, CURRENT_VERSION
    add_event(AGENT_NAME,
              version=CURRENT_VERSION,
              is_success=is_success,
              message=message,
              op=op)


def report_periodic(delta, op, is_success=True, message=''):
    from azurelinuxagent.common.version import AGENT_NAME, CURRENT_VERSION
    add_periodic(delta, AGENT_NAME,
              version=CURRENT_VERSION,
              is_success=is_success,
              message=message,
              op=op)


def report_metric(category, counter, instance, value, log_event=False, reporter=__event_logger__):
    """"""
    Send a telemetry event reporting a single instance of a performance counter.
    :param str category: The category of the metric (cpu, memory, etc)
    :param str counter: The name of the metric (""%idle"", etc)
    :param str instance: For instanced metrics, the identifier of the instance. E.g. a disk drive name, a cpu core#
    :param     value: The value of the metric
    :param bool log_event: If True, log the metric in the agent log as well
    :param EventLogger reporter: The EventLogger instance to which metric events should be sent
    """"""
    if reporter.event_dir is None:
        from azurelinuxagent.common.version import AGENT_NAME
        logger.warn(""Cannot report metric event -- Event reporter is not initialized."")
        message = ""Metric {0}/{1} [{2}] = {3}"".format(category, counter, instance, value)
        _log_event(AGENT_NAME, ""METRIC"", message, 0)
        return
    reporter.add_metric(category, counter, instance, value, log_event)


def add_event(name, op=WALAEventOperation.Unknown, is_success=True, duration=0,
              version=CURRENT_VERSION,
              message="""", evt_type="""", is_internal=False, log_event=True,
              reporter=__event_logger__):
    if reporter.event_dir is None:
        logger.warn(""Cannot add event -- Event reporter is not initialized."")
        _log_event(name, op, message, duration, is_success=is_success)
        return

    if should_emit_event(name, version, op, is_success):
        mark_event_status(name, version, op, is_success)
        reporter.add_event(
            name, op=op, is_success=is_success, duration=duration,
            version=str(version), message=message, evt_type=evt_type,
            is_internal=is_internal, log_event=log_event)


def add_log_event(level, message, reporter=__event_logger__):
    if reporter.event_dir is None:
        return

    reporter.add_log_event(level, message)


def add_periodic(
    delta, name, op=WALAEventOperation.Unknown, is_success=True, duration=0,
    version=CURRENT_VERSION,
    message="""", evt_type="""", is_internal=False, log_event=True, force=False,
    reporter=__event_logger__):
    if reporter.event_dir is None:
        logger.warn(""Cannot add periodic event -- Event reporter is not initialized."")
        _log_event(name, op, message, duration, is_success=is_success)
        return

    reporter.add_periodic(
        delta, name, op=op, is_success=is_success, duration=duration,
        version=str(version), message=message, evt_type=evt_type,
        is_internal=is_internal, log_event=log_event, force=force)


def mark_event_status(name, version, op, status):
    if op in __event_status_operations__:
        __event_status__.mark_event_status(name, version, op, status)


def should_emit_event(name, version, op, status):
    return \
        op not in __event_status_operations__ or \
        __event_status__ is None or \
        not __event_status__.event_marked(name, version, op) or \
        __event_status__.event_succeeded(name, version, op) != status


def init_event_logger(event_dir):
    __event_logger__.event_dir = event_dir


def init_event_status(status_dir):
    __event_status__.initialize(status_dir)


def dump_unhandled_err(name):
    if hasattr(sys, 'last_type') and hasattr(sys, 'last_value') and \
            hasattr(sys, 'last_traceback'):
        last_type = getattr(sys, 'last_type')
        last_value = getattr(sys, 'last_value')
        last_traceback = getattr(sys, 'last_traceback')
        error = traceback.format_exception(last_type, last_value,
                                           last_traceback)
        message = """".join(error)
        add_event(name, is_success=False, message=message,
                  op=WALAEventOperation.UnhandledError)


def enable_unhandled_err_dump(name):
    atexit.register(dump_unhandled_err, name)
/n/n/nazurelinuxagent/common/osutil/alpine.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class AlpineOSUtil(DefaultOSUtil):

    def __init__(self):
        super(AlpineOSUtil, self).__init__()
        self.agent_conf_file_path = '/etc/waagent.conf'
        self.jit_enabled = True

    def is_dhcp_enabled(self):
        return True

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output('pidof dhcpcd', chk_err=False)
        if ret[0] == 0:
            logger.info('dhcpcd is pid {}'.format(ret[1]))
            return ret[1].strip()
        return None

    def restart_if(self, ifname):
        logger.info('restarting {} (sort of, actually SIGHUPing dhcpcd)'.format(ifname))
        pid = self.get_dhcp_pid()
        if pid != None:
            ret = shellutil.run_get_output('kill -HUP {}'.format(pid))

    def set_ssh_client_alive_interval(self):
        # Alpine will handle this.
        pass

    def conf_sshd(self, disable_password):
        # Alpine will handle this.
        pass
/n/n/nazurelinuxagent/common/osutil/arch.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class ArchUtil(DefaultOSUtil):
    def __init__(self):
        super(ArchUtil, self).__init__()
        self.jit_enabled = True
    
    def is_dhcp_enabled(self):
        return True

    def start_network(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def restart_if(self, iface):
        shellutil.run(""systemctl restart systemd-networkd"")

    def restart_ssh_service(self):
        # SSH is socket activated on CoreOS.  No need to restart it.
        pass

    def stop_dhcp_service(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret= shellutil.run_get_output(""pidof systemd-networkd"")
        return ret[1] if ret[0] == 0 else None

    def conf_sshd(self, disable_password):
        # Don't whack the system default sshd conf
        pass/n/n/nazurelinuxagent/common/osutil/bigip.py/n/n# Copyright 2016 F5 Networks Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import array
import fcntl
import os
import platform
import re
import socket
import struct
import time

try:
    # WAAgent > 2.1.3
    import azurelinuxagent.common.logger as logger
    import azurelinuxagent.common.utils.shellutil as shellutil

    from azurelinuxagent.common.exception import OSUtilError
    from azurelinuxagent.common.osutil.default import DefaultOSUtil
except ImportError:
    # WAAgent <= 2.1.3
    import azurelinuxagent.logger as logger
    import azurelinuxagent.utils.shellutil as shellutil

    from azurelinuxagent.exception import OSUtilError
    from azurelinuxagent.distro.default.osutil import DefaultOSUtil


class BigIpOSUtil(DefaultOSUtil):

    def __init__(self):
        super(BigIpOSUtil, self).__init__()

    def _wait_until_mcpd_is_initialized(self):
        """"""Wait for mcpd to become available

        All configuration happens in mcpd so we need to wait that this is
        available before we go provisioning the system. I call this method
        at the first opportunity I have (during the DVD mounting call).
        This ensures that the rest of the provisioning does not need to wait
        for mcpd to be available unless it absolutely wants to.

        :return bool: Returns True upon success
        :raises OSUtilError: Raises exception if mcpd does not come up within
                             roughly 50 minutes (100 * 30 seconds)
        """"""
        for retries in range(1, 100):
            # Retry until mcpd completes startup:
            logger.info(""Checking to see if mcpd is up"")
            rc = shellutil.run(""/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running"", chk_err=False)
            if rc == 0:
                logger.info(""mcpd is up!"")
                break
            time.sleep(30)

        if rc is 0:
            return True

        raise OSUtilError(
            ""mcpd hasn't completed initialization! Cannot proceed!""
        )

    def _save_sys_config(self):
        cmd = ""/usr/bin/tmsh save sys config""
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""WARNING: Cannot save sys config on 1st boot."")
        return rc

    def restart_ssh_service(self):
        return shellutil.run(""/usr/bin/bigstart restart sshd"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --add waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --del waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""/sbin/pidof dhclient"")
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        """"""Set the static hostname of the device

        Normally, tmsh is used to set the hostname for the system. For our
        purposes at this time though, I would hesitate to trust this function.

        Azure(Stack) uses the name that you provide in the Web UI or ARM (for
        example) as the value of the hostname argument to this method. The
        problem is that there is nowhere in the UI that specifies the
        restrictions and checks that tmsh has for the hostname.

        For example, if you set the name ""bigip1"" in the Web UI, Azure(Stack)
        considers that a perfectly valid name. When WAAgent gets around to
        running though, tmsh will reject that value because it is not a fully
        qualified domain name. The proper value should have been bigip.xxx.yyy

        WAAgent will not fail if this command fails, but the hostname will not
        be what the user set either. Currently we do not set the hostname when
        WAAgent starts up, so I am passing on setting it here too.

        :param hostname: The hostname to set on the device
        """"""
        return None

    def set_dhcp_hostname(self, hostname):
        """"""Sets the DHCP hostname

        See `set_hostname` for an explanation of why I pass here

        :param hostname: The hostname to set on the device
        """"""
        return None

    def useradd(self, username, expiration=None, comment=None):
        """"""Create user account using tmsh

        Our policy is to create two accounts when booting a BIG-IP instance.
        The first account is the one that the user specified when they did
        the instance creation. The second one is the admin account that is,
        or should be, built in to the system.

        :param username: The username that you want to add to the system
        :param expiration: The expiration date to use. We do not use this
                           value.
        :param comment: description of the account.  We do not use this value.
        """"""
        if self.get_userentry(username):
            logger.info(""User {0} already exists, skip useradd"", username)
            return None

        cmd = ""/usr/bin/tmsh create auth user %s partition-access add { all-partitions { role admin } } shell bash"" % (username)
        retcode, out = shellutil.run_get_output(cmd, log_cmd=True, chk_err=True)
        if retcode != 0:
            raise OSUtilError(
                ""Failed to create user account:{0}, retcode:{1}, output:{2}"".format(username, retcode, out)
            )
        self._save_sys_config()
        return retcode

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        """"""Change a user's password with tmsh

        Since we are creating the user specified account and additionally
        changing the password of the built-in 'admin' account, both must
        be modified in this method.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        :param username: The username whose password to change
        :param password: The unencrypted password to set for the user
        :param crypt_id: If encrypting the password, the crypt_id that was used
        :param salt_len: If encrypting the password, the length of the salt
                         value used to do it.
        """"""

        # Start by setting the password of the user provided account
        cmd = ""/usr/bin/tmsh modify auth user {0} password '{1}'"".format(username, password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for {0}: {1}"".format(username, output)
            )

        # Next, set the password of the built-in 'admin' account to be have
        # the same password as the user provided account
        userentry = self.get_userentry('admin')
        if userentry is None:
            raise OSUtilError(""The 'admin' user account was not found!"")

        cmd = ""/usr/bin/tmsh modify auth user 'admin' password '{0}'"".format(password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for 'admin': {0}"".format(output)
            )
        self._save_sys_config()
        return ret

    def del_account(self, username):
        """"""Deletes a user account.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        We also don't use sudo, so we remove that method call as well.

        :param username:
        :return:
        """"""
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""/usr/bin/tmsh delete auth user "" + username)

    def get_dvd_device(self, dev_dir='/dev'):
        """"""Find BIG-IP's CD/DVD device

        This device is almost certainly /dev/cdrom so I added the ? to this pattern.
        Note that this method will return upon the first device found, but in my
        tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the
        correct CD/DVD device though.

        :todo: Consider just always returning ""/dev/cdrom"" here if that device device
               exists on all platforms that are supported on Azure(Stack)
        :param dev_dir: The root directory from which to look for devices
        """"""
        patten = r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)'
        for dvd in [re.match(patten, dev) for dev in os.listdir(dev_dir)]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        raise OSUtilError(""Failed to get dvd device"")

    def mount_dvd(self, **kwargs):
        """"""Mount the DVD containing the provisioningiso.iso file

        This is the _first_ hook that WAAgent provides for us, so this is the
        point where we should wait for mcpd to load. I am just overloading
        this method to add the mcpd wait. Then I proceed with the stock code.

        :param max_retry: Maximum number of retries waagent will make when
                          mounting the provisioningiso.iso DVD
        :param chk_err: Whether to check for errors or not in the mounting
                        commands
        """"""
        self._wait_until_mcpd_is_initialized()
        return super(BigIpOSUtil, self).mount_dvd(**kwargs)

    def eject_dvd(self, chk_err=True):
        """"""Runs the eject command to eject the provisioning DVD

        BIG-IP does not include an eject command. It is sufficient to just
        umount the DVD disk. But I will log that we do not support this for
        future reference.

        :param chk_err: Whether or not to check for errors raised by the eject
                        command
        """"""
        logger.warn(""Eject is not supported on this platform"")

    def get_first_if(self):
        """"""Return the interface name, and ip addr of the management interface.

        We need to add a struct_size check here because, curiously, our 64bit
        platform is identified by python in Azure(Stack) as 32 bit and without
        adjusting the struct_size, we can't get the information we need.

        I believe this may be caused by only python i686 being shipped with
        BIG-IP instead of python x86_64??
        """"""
        iface = ''
        expected = 16  # how many devices should I expect...

        python_arc = platform.architecture()[0]
        if python_arc == '64bit':
            struct_size = 40  # for 64bit the size is 40 bytes
        else:
            struct_size = 32  # for 32bit the size is 32 bytes
        sock = socket.socket(socket.AF_INET,
                             socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        buff = array.array('B', b'\0' * (expected * struct_size))
        param = struct.pack('iL',
                            expected*struct_size,
                            buff.buffer_info()[0])
        ret = fcntl.ioctl(sock.fileno(), 0x8912, param)
        retsize = (struct.unpack('iL', ret)[0])
        if retsize == (expected * struct_size):
            logger.warn(('SIOCGIFCONF returned more than {0} up '
                         'network interfaces.'), expected)
        sock = buff.tostring()
        for i in range(0, struct_size * expected, struct_size):
            iface = self._format_single_interface_name(sock, i)

            # Azure public was returning ""lo:1"" when deploying WAF
            if b'lo' in iface:
                continue
            else:
                break
        return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24])

    def _format_single_interface_name(self, sock, offset):
        return sock[offset:offset+16].split(b'\0', 1)[0]

    def route_add(self, net, mask, gateway):
        """"""Add specified route using tmsh.

        :param net:
        :param mask:
        :param gateway:
        :return:
        """"""
        cmd = (""/usr/bin/tmsh create net route ""
               ""{0}/{1} gw {2}"").format(net, mask, gateway)
        return shellutil.run(cmd, chk_err=False)

    def device_for_ide_port(self, port_id):
        """"""Return device name attached to ide port 'n'.

        Include a wait in here because BIG-IP may not have yet initialized
        this list of devices.

        :param port_id:
        :return:
        """"""
        for retries in range(1, 100):
            # Retry until devices are ready
            if os.path.exists(""/sys/bus/vmbus/devices/""):
                break
            else:
                time.sleep(10)
        return super(BigIpOSUtil, self).device_for_ide_port(port_id)
/n/n/nazurelinuxagent/common/osutil/clearlinux.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import base64
import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class ClearLinuxUtil(DefaultOSUtil):

    def __init__(self):
        super(ClearLinuxUtil, self).__init__()
        self.agent_conf_file_path = '/usr/share/defaults/waagent/waagent.conf'
        self.jit_enabled = True

    def is_dhcp_enabled(self):
        return True

    def start_network(self) :
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def restart_if(self, iface):
        shellutil.run(""systemctl restart systemd-networkd"")

    def restart_ssh_service(self):
        # SSH is socket activated. No need to restart it.
        pass

    def stop_dhcp_service(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret= shellutil.run_get_output(""pidof systemd-networkd"")
        return ret[1] if ret[0] == 0 else None

    def conf_sshd(self, disable_password):
        # Don't whack the system default sshd conf
        pass

    def del_root_password(self):
        try:
            passwd_file_path = conf.get_passwd_file_path()
            try:
                passwd_content = fileutil.read_file(passwd_file_path)
                if not passwd_content:
                    # Empty file is no better than no file
                    raise FileNotFoundError
            except FileNotFoundError:
                new_passwd = [""root:*LOCK*:14600::::::""]
            else:
                passwd = passwd_content.split('\n')
                new_passwd = [x for x in passwd if not x.startswith(""root:"")]
                new_passwd.insert(0, ""root:*LOCK*:14600::::::"")
            fileutil.write_file(passwd_file_path, ""\n"".join(new_passwd))
        except IOError as e:
            raise OSUtilError(""Failed to delete root password:{0}"".format(e))
        pass
/n/n/nazurelinuxagent/common/osutil/coreos.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class CoreOSUtil(DefaultOSUtil):

    def __init__(self):
        super(CoreOSUtil, self).__init__()
        self.agent_conf_file_path = '/usr/share/oem/waagent.conf'
        self.waagent_path = '/usr/share/oem/bin/waagent'
        self.python_path = '/usr/share/oem/python/bin'
        self.jit_enabled = True
        if 'PATH' in os.environ:
            path = ""{0}:{1}"".format(os.environ['PATH'], self.python_path)
        else:
            path = self.python_path
        os.environ['PATH'] = path

        if 'PYTHONPATH' in os.environ:
            py_path = os.environ['PYTHONPATH']
            py_path = ""{0}:{1}"".format(py_path, self.waagent_path)
        else:
            py_path = self.waagent_path
        os.environ['PYTHONPATH'] = py_path

    def is_sys_user(self, username):
        # User 'core' is not a sysuser.
        if username == 'core':
            return False
        return super(CoreOSUtil, self).is_sys_user(username)

    def is_dhcp_enabled(self):
        return True

    def start_network(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def restart_if(self, *dummy, **_):
        shellutil.run(""systemctl restart systemd-networkd"")

    def restart_ssh_service(self):
        # SSH is socket activated on CoreOS.  No need to restart it.
        pass

    def stop_dhcp_service(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""systemctl show -p MainPID ""
                                       ""systemd-networkd"", chk_err=False)
        pid = ret[1].split('=', 1)[-1].strip() if ret[0] == 0 else None
        return pid if pid != '0' else None

    def conf_sshd(self, disable_password):
        # In CoreOS, /etc/sshd_config is mount readonly.  Skip the setting.
        pass
/n/n/nazurelinuxagent/common/osutil/debian.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import base64
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class DebianOSUtil(DefaultOSUtil):

    def __init__(self):
        super(DebianOSUtil, self).__init__()
        self.jit_enabled = True

    def restart_ssh_service(self):
        return shellutil.run(""systemctl --job-mode=ignore-dependencies try-reload-or-restart ssh"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""service azurelinuxagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""service azurelinuxagent start"", chk_err=False)

    def start_network(self):
        pass

    def remove_rules_files(self, rules_files=""""):
        pass

    def restore_rules_files(self, rules_files=""""):
        pass

    def get_dhcp_lease_endpoint(self):
        return self.get_endpoint_from_leases_path('/var/lib/dhcp/dhclient.*.leases')
/n/n/nazurelinuxagent/common/osutil/default.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import array
import base64
import datetime
import errno
import fcntl
import glob
import multiprocessing
import os
import platform
import pwd
import re
import shutil
import socket
import struct
import sys
import time

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil

from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.utils.flexible_version import FlexibleVersion

from pwd import getpwall

__RULES_FILES__ = [ ""/lib/udev/rules.d/75-persistent-net-generator.rules"",
                    ""/etc/udev/rules.d/70-persistent-net.rules"" ]

""""""
Define distro specific behavior. OSUtil class defines default behavior
for all distros. Each concrete distro classes could overwrite default behavior
if needed.
""""""

IPTABLES_VERSION_PATTERN = re.compile(""^[^\d\.]*([\d\.]+).*$"")
IPTABLES_VERSION = ""iptables --version""
IPTABLES_LOCKING_VERSION = FlexibleVersion('1.4.21')

FIREWALL_ACCEPT = ""iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m owner --uid-owner {3} -j ACCEPT""
# Note:
# -- Initially ""flight"" the change to ACCEPT packets and develop a metric baseline
#    A subsequent release will convert the ACCEPT to DROP
# FIREWALL_DROP = ""iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m conntrack --ctstate INVALID,NEW -j ACCEPT""
FIREWALL_DROP = ""iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m conntrack --ctstate INVALID,NEW -j DROP""
FIREWALL_LIST = ""iptables {0} -t security -L -nxv""
FIREWALL_PACKETS = ""iptables {0} -t security -L OUTPUT --zero OUTPUT -nxv""
FIREWALL_FLUSH = ""iptables {0} -t security --flush""

# Precisely delete the rules created by the agent.
# this rule was used <= 2.2.25.  This rule helped to validate our change, and determine impact.
FIREWALL_DELETE_CONNTRACK_ACCEPT = ""iptables {0} -t security -D OUTPUT -d {1} -p tcp -m conntrack --ctstate INVALID,NEW -j ACCEPT""
FIREWALL_DELETE_OWNER_ACCEPT = ""iptables {0} -t security -D OUTPUT -d {1} -p tcp -m owner --uid-owner {2} -j ACCEPT""
FIREWALL_DELETE_CONNTRACK_DROP = ""iptables {0} -t security -D OUTPUT -d {1} -p tcp -m conntrack --ctstate INVALID,NEW -j DROP""

PACKET_PATTERN = ""^\s*(\d+)\s+(\d+)\s+DROP\s+.*{0}[^\d]*$""
ALL_CPUS_REGEX = re.compile('^cpu .*')


_enable_firewall = True

DMIDECODE_CMD = 'dmidecode --string system-uuid'
PRODUCT_ID_FILE = '/sys/class/dmi/id/product_uuid'
UUID_PATTERN = re.compile(
    r'^\s*[A-F0-9]{8}(?:\-[A-F0-9]{4}){3}\-[A-F0-9]{12}\s*$',
    re.IGNORECASE)

IOCTL_SIOCGIFCONF = 0x8912
IOCTL_SIOCGIFFLAGS = 0x8913
IOCTL_SIOCGIFHWADDR = 0x8927
IFNAMSIZ = 16


class DefaultOSUtil(object):
    def __init__(self):
        self.agent_conf_file_path = '/etc/waagent.conf'
        self.selinux = None
        self.disable_route_warning = False
        self.jit_enabled = False

    def get_firewall_dropped_packets(self, dst_ip=None):
        # If a previous attempt failed, do not retry
        global _enable_firewall
        if not _enable_firewall:
            return 0

        try:
            wait = self.get_firewall_will_wait()

            rc, output = shellutil.run_get_output(FIREWALL_PACKETS.format(wait), log_cmd=False)
            if rc == 3:
                # Transient error  that we ignore.  This code fires every loop
                # of the daemon (60m), so we will get the value eventually.
                return 0

            if rc != 0:
                return -1

            pattern = re.compile(PACKET_PATTERN.format(dst_ip))
            for line in output.split('\n'):
                m = pattern.match(line)
                if m is not None:
                    return int(m.group(1))
            
            return 0

        except Exception as e:
            _enable_firewall = False
            logger.warn(""Unable to retrieve firewall packets dropped""
                        ""{0}"".format(ustr(e)))
            return -1

    def get_firewall_will_wait(self):
        # Determine if iptables will serialize access
        rc, output = shellutil.run_get_output(IPTABLES_VERSION)
        if rc != 0:
            msg = ""Unable to determine version of iptables""
            logger.warn(msg)
            raise Exception(msg)

        m = IPTABLES_VERSION_PATTERN.match(output)
        if m is None:
            msg = ""iptables did not return version information""
            logger.warn(msg)
            raise Exception(msg)

        wait = ""-w"" \
                if FlexibleVersion(m.group(1)) >= IPTABLES_LOCKING_VERSION \
                else """"
        return wait

    def _delete_rule(self, rule):
        """"""
        Continually execute the delete operation until the return
        code is non-zero or the limit has been reached.
        """"""
        for i in range(1, 100):
            rc = shellutil.run(rule, chk_err=False)
            if rc == 1:
                return
            elif rc == 2:
                raise Exception(""invalid firewall deletion rule '{0}'"".format(rule))

    def remove_firewall(self, dst_ip=None, uid=None):
        # If a previous attempt failed, do not retry
        global _enable_firewall
        if not _enable_firewall:
            return False

        try:
            if dst_ip is None or uid is None:
                msg = ""Missing arguments to enable_firewall""
                logger.warn(msg)
                raise Exception(msg)

            wait = self.get_firewall_will_wait()

            # This rule was <= 2.2.25 only, and may still exist on some VMs.  Until 2.2.25
            # has aged out, keep this cleanup in place.
            self._delete_rule(FIREWALL_DELETE_CONNTRACK_ACCEPT.format(wait, dst_ip))

            self._delete_rule(FIREWALL_DELETE_OWNER_ACCEPT.format(wait, dst_ip, uid))
            self._delete_rule(FIREWALL_DELETE_CONNTRACK_DROP.format(wait, dst_ip))

            return True

        except Exception as e:
            _enable_firewall = False
            logger.info(""Unable to remove firewall -- ""
                        ""no further attempts will be made: ""
                        ""{0}"".format(ustr(e)))
            return False

    def enable_firewall(self, dst_ip=None, uid=None):
        # If a previous attempt failed, do not retry
        global _enable_firewall
        if not _enable_firewall:
            return False

        try:
            if dst_ip is None or uid is None:
                msg = ""Missing arguments to enable_firewall""
                logger.warn(msg)
                raise Exception(msg)

            wait = self.get_firewall_will_wait()

            # If the DROP rule exists, make no changes
            drop_rule = FIREWALL_DROP.format(wait, ""C"", dst_ip)
            rc = shellutil.run(drop_rule, chk_err=False)
            if rc == 0:
                logger.verbose(""Firewall appears established"")
                return True
            elif rc == 2:
                self.remove_firewall(dst_ip, uid)
                msg = ""please upgrade iptables to a version that supports the -C option""
                logger.warn(msg)
                raise Exception(msg)

            # Otherwise, append both rules
            accept_rule = FIREWALL_ACCEPT.format(wait, ""A"", dst_ip, uid)
            drop_rule = FIREWALL_DROP.format(wait, ""A"", dst_ip)

            if shellutil.run(accept_rule) != 0:
                msg = ""Unable to add ACCEPT firewall rule '{0}'"".format(
                    accept_rule)
                logger.warn(msg)
                raise Exception(msg)

            if shellutil.run(drop_rule) != 0:
                msg = ""Unable to add DROP firewall rule '{0}'"".format(
                    drop_rule)
                logger.warn(msg)
                raise Exception(msg)

            logger.info(""Successfully added Azure fabric firewall rules"")

            rc, output = shellutil.run_get_output(FIREWALL_LIST.format(wait))
            if rc == 0:
                logger.info(""Firewall rules:\n{0}"".format(output))
            else:
                logger.warn(""Listing firewall rules failed: {0}"".format(output))

            return True

        except Exception as e:
            _enable_firewall = False
            logger.info(""Unable to establish firewall -- ""
                        ""no further attempts will be made: ""
                        ""{0}"".format(ustr(e)))
            return False

    def _correct_instance_id(self, id):
        '''
        Azure stores the instance ID with an incorrect byte ordering for the
        first parts. For example, the ID returned by the metadata service:

            D0DF4C54-4ECB-4A4B-9954-5BDF3ED5C3B8

        will be found as:

            544CDFD0-CB4E-4B4A-9954-5BDF3ED5C3B8

        This code corrects the byte order such that it is consistent with
        that returned by the metadata service.
        '''

        if not UUID_PATTERN.match(id):
            return id

        parts = id.split('-')
        return '-'.join([
                textutil.swap_hexstring(parts[0], width=2),
                textutil.swap_hexstring(parts[1], width=2),
                textutil.swap_hexstring(parts[2], width=2),
                parts[3],
                parts[4]
            ])

    def is_current_instance_id(self, id_that):
        '''
        Compare two instance IDs for equality, but allow that some IDs
        may have been persisted using the incorrect byte ordering.
        '''
        id_this = self.get_instance_id()
        return id_that == id_this or \
            id_that == self._correct_instance_id(id_this)

    def is_cgroups_supported(self):
        return False

    def mount_cgroups(self):
        pass

    def get_agent_conf_file_path(self):
        return self.agent_conf_file_path

    def get_instance_id(self):
        '''
        Azure records a UUID as the instance ID
        First check /sys/class/dmi/id/product_uuid.
        If that is missing, then extracts from dmidecode
        If nothing works (for old VMs), return the empty string
        '''
        if os.path.isfile(PRODUCT_ID_FILE):
            s = fileutil.read_file(PRODUCT_ID_FILE).strip()

        else:
            rc, s = shellutil.run_get_output(DMIDECODE_CMD)
            if rc != 0 or UUID_PATTERN.match(s) is None:
                return """"

        return self._correct_instance_id(s.strip())

    def get_userentry(self, username):
        try:
            return pwd.getpwnam(username)
        except KeyError:
            return None

    def is_sys_user(self, username):
        """"""
        Check whether use is a system user. 
        If reset sys user is allowed in conf, return False
        Otherwise, check whether UID is less than UID_MIN
        """"""
        if conf.get_allow_reset_sys_user():
            return False

        userentry = self.get_userentry(username)
        uidmin = None
        try:
            uidmin_def = fileutil.get_line_startingwith(""UID_MIN"",
                                                        ""/etc/login.defs"")
            if uidmin_def is not None:
                uidmin = int(uidmin_def.split()[1])
        except IOError as e:
            pass
        if uidmin == None:
            uidmin = 100
        if userentry != None and userentry[2] < uidmin:
            return True
        else:
            return False

    def useradd(self, username, expiration=None, comment=None):
        """"""
        Create user account with 'username'
        """"""
        userentry = self.get_userentry(username)
        if userentry is not None:
            logger.info(""User {0} already exists, skip useradd"", username)
            return

        if expiration is not None:
            cmd = ""useradd -m {0} -e {1}"".format(username, expiration)
        else:
            cmd = ""useradd -m {0}"".format(username)
        
        if comment is not None:
            cmd += "" -c {0}"".format(comment)
        retcode, out = shellutil.run_get_output(cmd)
        if retcode != 0:
            raise OSUtilError((""Failed to create user account:{0}, ""
                               ""retcode:{1}, ""
                               ""output:{2}"").format(username, retcode, out))

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user, ""
                               ""will not set password."").format(username))
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        cmd = ""usermod -p '{0}' {1}"".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))
    
    def get_users(self):
        return getpwall()

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        sudoers_dir = conf.get_sudoers_dir()
        sudoers_wagent = os.path.join(sudoers_dir, 'waagent')

        if not remove:
            # for older distros create sudoers.d
            if not os.path.isdir(sudoers_dir):
                sudoers_file = os.path.join(sudoers_dir, '../sudoers')
                # create the sudoers.d directory
                os.mkdir(sudoers_dir)
                # add the include of sudoers.d to the /etc/sudoers
                sudoers = '\n#includedir ' + sudoers_dir + '\n'
                fileutil.append_file(sudoers_file, sudoers)
            sudoer = None
            if nopasswd:
                sudoer = ""{0} ALL=(ALL) NOPASSWD: ALL"".format(username)
            else:
                sudoer = ""{0} ALL=(ALL) ALL"".format(username)
            if not os.path.isfile(sudoers_wagent) or \
                    fileutil.findstr_in_file(sudoers_wagent, sudoer) is False:
                fileutil.append_file(sudoers_wagent, ""{0}\n"".format(sudoer))
            fileutil.chmod(sudoers_wagent, 0o440)
        else:
            # remove user from sudoers
            if os.path.isfile(sudoers_wagent):
                try:
                    content = fileutil.read_file(sudoers_wagent)
                    sudoers = content.split(""\n"")
                    sudoers = [x for x in sudoers if username not in x]
                    fileutil.write_file(sudoers_wagent, ""\n"".join(sudoers))
                except IOError as e:
                    raise OSUtilError(""Failed to remove sudoer: {0}"".format(e))

    def del_root_password(self):
        try:
            passwd_file_path = conf.get_passwd_file_path()
            passwd_content = fileutil.read_file(passwd_file_path)
            passwd = passwd_content.split('\n')
            new_passwd = [x for x in passwd if not x.startswith(""root:"")]
            new_passwd.insert(0, ""root:*LOCK*:14600::::::"")
            fileutil.write_file(passwd_file_path, ""\n"".join(new_passwd))
        except IOError as e:
            raise OSUtilError(""Failed to delete root password:{0}"".format(e))

    def _norm_path(self, filepath):
        home = conf.get_home_dir()
        # Expand HOME variable if present in path
        path = os.path.normpath(filepath.replace(""$HOME"", home))
        return path

    def deploy_ssh_keypair(self, username, keypair):
        """"""
        Deploy id_rsa and id_rsa.pub
        """"""
        path, thumbprint = keypair
        path = self._norm_path(path)
        dir_path = os.path.dirname(path)
        fileutil.mkdir(dir_path, mode=0o700, owner=username)
        lib_dir = conf.get_lib_dir()
        prv_path = os.path.join(lib_dir, thumbprint + '.prv')
        if not os.path.isfile(prv_path):
            raise OSUtilError(""Can't find {0}.prv"".format(thumbprint))
        shutil.copyfile(prv_path, path)
        pub_path = path + '.pub'
        crytputil = CryptUtil(conf.get_openssl_cmd())
        pub = crytputil.get_pubkey_from_prv(prv_path)
        fileutil.write_file(pub_path, pub)
        self.set_selinux_context(pub_path, 'unconfined_u:object_r:ssh_home_t:s0')
        self.set_selinux_context(path, 'unconfined_u:object_r:ssh_home_t:s0')
        os.chmod(path, 0o644)
        os.chmod(pub_path, 0o600)

    def openssl_to_openssh(self, input_file, output_file):
        cryptutil = CryptUtil(conf.get_openssl_cmd())
        cryptutil.crt_to_ssh(input_file, output_file)

    def deploy_ssh_pubkey(self, username, pubkey):
        """"""
        Deploy authorized_key
        """"""
        path, thumbprint, value = pubkey
        if path is None:
            raise OSUtilError(""Public key path is None"")

        crytputil = CryptUtil(conf.get_openssl_cmd())

        path = self._norm_path(path)
        dir_path = os.path.dirname(path)
        fileutil.mkdir(dir_path, mode=0o700, owner=username)
        if value is not None:
            if not value.startswith(""ssh-""):
                raise OSUtilError(""Bad public key: {0}"".format(value))
            fileutil.write_file(path, value)
        elif thumbprint is not None:
            lib_dir = conf.get_lib_dir()
            crt_path = os.path.join(lib_dir, thumbprint + '.crt')
            if not os.path.isfile(crt_path):
                raise OSUtilError(""Can't find {0}.crt"".format(thumbprint))
            pub_path = os.path.join(lib_dir, thumbprint + '.pub')
            pub = crytputil.get_pubkey_from_crt(crt_path)
            fileutil.write_file(pub_path, pub)
            self.set_selinux_context(pub_path,
                                     'unconfined_u:object_r:ssh_home_t:s0')
            self.openssl_to_openssh(pub_path, path)
            fileutil.chmod(pub_path, 0o600)
        else:
            raise OSUtilError(""SSH public key Fingerprint and Value are None"")

        self.set_selinux_context(path, 'unconfined_u:object_r:ssh_home_t:s0')
        fileutil.chowner(path, username)
        fileutil.chmod(path, 0o644)

    def is_selinux_system(self):
        """"""
        Checks and sets self.selinux = True if SELinux is available on system.
        """"""
        if self.selinux == None:
            if shellutil.run(""which getenforce"", chk_err=False) == 0:
                self.selinux = True
            else:
                self.selinux = False
        return self.selinux

    def is_selinux_enforcing(self):
        """"""
        Calls shell command 'getenforce' and returns True if 'Enforcing'.
        """"""
        if self.is_selinux_system():
            output = shellutil.run_get_output(""getenforce"")[1]
            return output.startswith(""Enforcing"")
        else:
            return False

    def set_selinux_context(self, path, con):
        """"""
        Calls shell 'chcon' with 'path' and 'con' context.
        Returns exit result.
        """"""
        if self.is_selinux_system():
            if not os.path.exists(path):
                logger.error(""Path does not exist: {0}"".format(path))
                return 1
            return shellutil.run('chcon ' + con + ' ' + path)

    def conf_sshd(self, disable_password):
        option = ""no"" if disable_password else ""yes""
        conf_file_path = conf.get_sshd_conf_file_path()
        conf_file = fileutil.read_file(conf_file_path).split(""\n"")
        textutil.set_ssh_config(conf_file, ""PasswordAuthentication"", option)
        textutil.set_ssh_config(conf_file, ""ChallengeResponseAuthentication"", option)
        textutil.set_ssh_config(conf_file, ""ClientAliveInterval"", str(conf.get_ssh_client_alive_interval()))
        fileutil.write_file(conf_file_path, ""\n"".join(conf_file))
        logger.info(""{0} SSH password-based authentication methods.""
                    .format(""Disabled"" if disable_password else ""Enabled""))
        logger.info(""Configured SSH client probing to keep connections alive."")

    def get_dvd_device(self, dev_dir='/dev'):
        pattern = r'(sr[0-9]|hd[c-z]|cdrom[0-9]|cd[0-9])'
        device_list = os.listdir(dev_dir)
        for dvd in [re.match(pattern, dev) for dev in device_list]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        inner_detail = ""The following devices were found, but none matched "" \
                       ""the pattern [{0}]: {1}\n"".format(pattern, device_list)
        raise OSUtilError(msg=""Failed to get dvd device from {0}"".format(dev_dir),
                          inner=inner_detail)

    def mount_dvd(self,
                  max_retry=6,
                  chk_err=True,
                  dvd_device=None,
                  mount_point=None,
                  sleep_time=5):
        if dvd_device is None:
            dvd_device = self.get_dvd_device()
        if mount_point is None:
            mount_point = conf.get_dvd_mount_point()
        mount_list = shellutil.run_get_output(""mount"")[1]
        existing = self.get_mount_point(mount_list, dvd_device)

        if existing is not None:
            # already mounted
            logger.info(""{0} is already mounted at {1}"", dvd_device, existing)
            return

        if not os.path.isdir(mount_point):
            os.makedirs(mount_point)

        err = ''
        for retry in range(1, max_retry):
            return_code, err = self.mount(dvd_device,
                                          mount_point,
                                          option=""-o ro -t udf,iso9660"",
                                          chk_err=False)
            if return_code == 0:
                logger.info(""Successfully mounted dvd"")
                return
            else:
                logger.warn(
                    ""Mounting dvd failed [retry {0}/{1}, sleeping {2} sec]"",
                    retry,
                    max_retry - 1,
                    sleep_time)
                if retry < max_retry:
                    time.sleep(sleep_time)
        if chk_err:
            raise OSUtilError(""Failed to mount dvd device"", inner=err)

    def umount_dvd(self, chk_err=True, mount_point=None):
        if mount_point is None:
            mount_point = conf.get_dvd_mount_point()
        return_code = self.umount(mount_point, chk_err=chk_err)
        if chk_err and return_code != 0:
            raise OSUtilError(""Failed to unmount dvd device at {0}"",
                              mount_point)

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""eject {0}"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject dvd: ret={0}"".format(retcode))

    def try_load_atapiix_mod(self):
        try:
            self.load_atapiix_mod()
        except Exception as e:
            logger.warn(""Could not load ATAPI driver: {0}"".format(e))

    def load_atapiix_mod(self):
        if self.is_atapiix_mod_loaded():
            return
        ret, kern_version = shellutil.run_get_output(""uname -r"")
        if ret != 0:
            raise Exception(""Failed to call uname -r"")
        mod_path = os.path.join('/lib/modules',
                                kern_version.strip('\n'),
                                'kernel/drivers/ata/ata_piix.ko')
        if not os.path.isfile(mod_path):
            raise Exception(""Can't find module file:{0}"".format(mod_path))

        ret, output = shellutil.run_get_output(""insmod "" + mod_path)
        if ret != 0:
            raise Exception(""Error calling insmod for ATAPI CD-ROM driver"")
        if not self.is_atapiix_mod_loaded(max_retry=3):
            raise Exception(""Failed to load ATAPI CD-ROM driver"")

    def is_atapiix_mod_loaded(self, max_retry=1):
        for retry in range(0, max_retry):
            ret = shellutil.run(""lsmod | grep ata_piix"", chk_err=False)
            if ret == 0:
                logger.info(""Module driver for ATAPI CD-ROM is already present."")
                return True
            if retry < max_retry - 1:
                time.sleep(1)
        return False

    def mount(self, device, mount_point, option="""", chk_err=True):
        cmd = ""mount {0} {1} {2}"".format(option, device, mount_point)
        retcode, err = shellutil.run_get_output(cmd, chk_err)
        if retcode != 0:
            detail = ""[{0}] returned {1}: {2}"".format(cmd, retcode, err)
            err = detail
        return retcode, err

    def umount(self, mount_point, chk_err=True):
        return shellutil.run(""umount {0}"".format(mount_point), chk_err=chk_err)

    def allow_dhcp_broadcast(self):
        # Open DHCP port if iptables is enabled.
        # We supress error logging on error.
        shellutil.run(""iptables -D INPUT -p udp --dport 68 -j ACCEPT"",
                      chk_err=False)
        shellutil.run(""iptables -I INPUT -p udp --dport 68 -j ACCEPT"",
                      chk_err=False)


    def remove_rules_files(self, rules_files=__RULES_FILES__):
        lib_dir = conf.get_lib_dir()
        for src in rules_files:
            file_name = fileutil.base_name(src)
            dest = os.path.join(lib_dir, file_name)
            if os.path.isfile(dest):
                os.remove(dest)
            if os.path.isfile(src):
                logger.warn(""Move rules file {0} to {1}"", file_name, dest)
                shutil.move(src, dest)

    def restore_rules_files(self, rules_files=__RULES_FILES__):
        lib_dir = conf.get_lib_dir()
        for dest in rules_files:
            filename = fileutil.base_name(dest)
            src = os.path.join(lib_dir, filename)
            if os.path.isfile(dest):
                continue
            if os.path.isfile(src):
                logger.warn(""Move rules file {0} to {1}"", filename, dest)
                shutil.move(src, dest)

    def get_mac_addr(self):
        """"""
        Convenience function, returns mac addr bound to
        first non-loopback interface.
        """"""
        ifname = self.get_if_name()
        addr = self.get_if_mac(ifname)
        return textutil.hexstr_to_bytearray(addr)

    def get_if_mac(self, ifname):
        """"""
        Return the mac-address bound to the socket.
        """"""
        sock = socket.socket(socket.AF_INET,
                             socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        param = struct.pack('256s', (ifname[:15]+('\0'*241)).encode('latin-1'))
        info = fcntl.ioctl(sock.fileno(), IOCTL_SIOCGIFHWADDR, param)
        sock.close()
        return ''.join(['%02X' % textutil.str_to_ord(char) for char in info[18:24]])

    @staticmethod
    def _get_struct_ifconf_size():
        """"""
        Return the sizeof struct ifinfo. On 64-bit platforms the size is 40 bytes;
        on 32-bit platforms the size is 32 bytes.
        """"""
        python_arc = platform.architecture()[0]
        struct_size = 32 if python_arc == '32bit' else 40
        return struct_size

    def _get_all_interfaces(self):
        """"""
        Return a dictionary mapping from interface name to IPv4 address.
        Interfaces without a name are ignored.
        """"""
        expected=16 # how many devices should I expect...
        struct_size = DefaultOSUtil._get_struct_ifconf_size()
        array_size = expected * struct_size

        buff = array.array('B', b'\0' * array_size)
        param = struct.pack('iL', array_size, buff.buffer_info()[0])

        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        ret = fcntl.ioctl(sock.fileno(), IOCTL_SIOCGIFCONF, param)
        retsize = (struct.unpack('iL', ret)[0])
        sock.close()

        if retsize == array_size:
            logger.warn(('SIOCGIFCONF returned more than {0} up '
                         'network interfaces.'), expected)

        ifconf_buff = buff.tostring()

        ifaces = {}
        for i in range(0, array_size, struct_size):
            iface = ifconf_buff[i:i+IFNAMSIZ].split(b'\0', 1)[0]
            if len(iface) > 0:
                iface_name = iface.decode('latin-1')
                if iface_name not in ifaces:
                    ifaces[iface_name] = socket.inet_ntoa(ifconf_buff[i+20:i+24])
        return ifaces


    def get_first_if(self):
        """"""
        Return the interface name, and IPv4 addr of the ""primary"" interface or,
        failing that, any active non-loopback interface.
        """"""
        primary = self.get_primary_interface()
        ifaces = self._get_all_interfaces()

        if primary in ifaces:
            return primary, ifaces[primary]

        for iface_name in ifaces.keys():
            if not self.is_loopback(iface_name):
                logger.info(""Choosing non-primary [{0}]"".format(iface_name))
                return iface_name, ifaces[iface_name]

        return '', ''


    def get_primary_interface(self):
        """"""
        Get the name of the primary interface, which is the one with the
        default route attached to it; if there are multiple default routes,
        the primary has the lowest Metric.
        :return: the interface which has the default route
        """"""
        # from linux/route.h
        RTF_GATEWAY = 0x02
        DEFAULT_DEST = ""00000000""

        hdr_iface = ""Iface""
        hdr_dest = ""Destination""
        hdr_flags = ""Flags""
        hdr_metric = ""Metric""

        idx_iface = -1
        idx_dest = -1
        idx_flags = -1
        idx_metric = -1
        primary = None
        primary_metric = None

        if not self.disable_route_warning:
            logger.info(""Examine /proc/net/route for primary interface"")
        with open('/proc/net/route') as routing_table:
            idx = 0
            for header in filter(lambda h: len(h) > 0, routing_table.readline().strip("" \n"").split(""\t"")):
                if header == hdr_iface:
                    idx_iface = idx
                elif header == hdr_dest:
                    idx_dest = idx
                elif header == hdr_flags:
                    idx_flags = idx
                elif header == hdr_metric:
                    idx_metric = idx
                idx = idx + 1
            for entry in routing_table.readlines():
                route = entry.strip("" \n"").split(""\t"")
                if route[idx_dest] == DEFAULT_DEST and int(route[idx_flags]) & RTF_GATEWAY == RTF_GATEWAY:
                    metric = int(route[idx_metric])
                    iface = route[idx_iface]
                    if primary is None or metric < primary_metric:
                        primary = iface
                        primary_metric = metric

        if primary is None:
            primary = ''
            if not self.disable_route_warning:
                with open('/proc/net/route') as routing_table_fh:
                    routing_table_text = routing_table_fh.read()
                    logger.warn('Could not determine primary interface, '
                                'please ensure /proc/net/route is correct')
                    logger.warn('Contents of /proc/net/route:\n{0}'.format(routing_table_text))
                    logger.warn('Primary interface examination will retry silently')
                    self.disable_route_warning = True
        else:
            logger.info('Primary interface is [{0}]'.format(primary))
            self.disable_route_warning = False
        return primary

    def is_primary_interface(self, ifname):
        """"""
        Indicate whether the specified interface is the primary.
        :param ifname: the name of the interface - eth0, lo, etc.
        :return: True if this interface binds the default route
        """"""
        return self.get_primary_interface() == ifname

    def is_loopback(self, ifname):
        """"""
        Determine if a named interface is loopback.
        """"""
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        ifname_buff = ifname + ('\0'*256)
        result = fcntl.ioctl(s.fileno(), IOCTL_SIOCGIFFLAGS, ifname_buff)
        flags, = struct.unpack('H', result[16:18])
        isloopback = flags & 8 == 8
        if not self.disable_route_warning:
            logger.info('interface [{0}] has flags [{1}], '
                        'is loopback [{2}]'.format(ifname, flags, isloopback))
        s.close()
        return isloopback

    def get_dhcp_lease_endpoint(self):
        """"""
        OS specific, this should return the decoded endpoint of
        the wireserver from option 245 in the dhcp leases file
        if it exists on disk.
        :return: The endpoint if available, or None
        """"""
        return None

    @staticmethod
    def get_endpoint_from_leases_path(pathglob):
        """"""
        Try to discover and decode the wireserver endpoint in the
        specified dhcp leases path.
        :param pathglob: The path containing dhcp lease files
        :return: The endpoint if available, otherwise None
        """"""
        endpoint = None

        HEADER_LEASE = ""lease""
        HEADER_OPTION = ""option unknown-245""
        HEADER_DNS = ""option domain-name-servers""
        HEADER_EXPIRE = ""expire""
        FOOTER_LEASE = ""}""
        FORMAT_DATETIME = ""%Y/%m/%d %H:%M:%S""

        logger.info(""looking for leases in path [{0}]"".format(pathglob))
        for lease_file in glob.glob(pathglob):
            leases = open(lease_file).read()
            if HEADER_OPTION in leases:
                cached_endpoint = None
                has_option_245 = False
                expired = True  # assume expired
                for line in leases.splitlines():
                    if line.startswith(HEADER_LEASE):
                        cached_endpoint = None
                        has_option_245 = False
                        expired = True
                    elif HEADER_DNS in line:
                        cached_endpoint = line.replace(HEADER_DNS, '').strip("" ;"")
                    elif HEADER_OPTION in line:
                        has_option_245 = True
                    elif HEADER_EXPIRE in line:
                        if ""never"" in line:
                            expired = False
                        else:
                            try:
                                expire_string = line.split("" "", 4)[-1].strip("";"")
                                expire_date = datetime.datetime.strptime(expire_string, FORMAT_DATETIME)
                                if expire_date > datetime.datetime.utcnow():
                                    expired = False
                            except:
                                logger.error(""could not parse expiry token '{0}'"".format(line))
                    elif FOOTER_LEASE in line:
                        logger.info(""dhcp entry:{0}, 245:{1}, expired:{2}"".format(
                            cached_endpoint, has_option_245, expired))
                        if not expired and cached_endpoint is not None and has_option_245:
                            endpoint = cached_endpoint
                            logger.info(""found endpoint [{0}]"".format(endpoint))
                            # we want to return the last valid entry, so
                            # keep searching
        if endpoint is not None:
            logger.info(""cached endpoint found [{0}]"".format(endpoint))
        else:
            logger.info(""cached endpoint not found"")
        return endpoint

    def is_missing_default_route(self):
        routes = shellutil.run_get_output(""route -n"")[1]
        for route in routes.split(""\n""):
            if route.startswith(""0.0.0.0 "") or route.startswith(""default ""):
               return False
        return True

    def get_if_name(self):
        if_name = ''
        if_found = False
        while not if_found:
            if_name = self.get_first_if()[0]
            if_found = len(if_name) >= 2
            if not if_found:
                time.sleep(2)
        return if_name

    def get_ip4_addr(self):
        return self.get_first_if()[1]

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 dev {0}"".format(ifname),
                             chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route del 255.255.255.255 dev {0}"".format(ifname),
                      chk_err=False)

    def is_dhcp_enabled(self):
        return False

    def stop_dhcp_service(self):
        pass

    def start_dhcp_service(self):
        pass

    def start_network(self):
        pass

    def start_agent_service(self):
        pass

    def stop_agent_service(self):
        pass

    def register_agent_service(self):
        pass

    def unregister_agent_service(self):
        pass

    def restart_ssh_service(self):
        pass

    def route_add(self, net, mask, gateway):
        """"""
        Add specified route using /sbin/route add -net.
        """"""
        cmd = (""/sbin/route add -net ""
               ""{0} netmask {1} gw {2}"").format(net, mask, gateway)
        return shellutil.run(cmd, chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        fileutil.write_file('/etc/hostname', hostname)
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def set_dhcp_hostname(self, hostname):
        autosend = r'^[^#]*?send\s*host-name.*?(<hostname>|gethostname[(,)])'
        dhclient_files = ['/etc/dhcp/dhclient.conf', '/etc/dhcp3/dhclient.conf', '/etc/dhclient.conf']
        for conf_file in dhclient_files:
            if not os.path.isfile(conf_file):
                continue
            if fileutil.findre_in_file(conf_file, autosend):
                #Return if auto send host-name is configured
                return
            fileutil.update_conf_file(conf_file,
                                      'send host-name',
                                      'send host-name ""{0}"";'.format(hostname))

    def restart_if(self, ifname, retries=3, wait=5):
        retry_limit=retries+1
        for attempt in range(1, retry_limit):
            return_code=shellutil.run(""ifdown {0} && ifup {0}"".format(ifname))
            if return_code == 0:
                return
            logger.warn(""failed to restart {0}: return code {1}"".format(ifname, return_code))
            if attempt < retry_limit:
                logger.info(""retrying in {0} seconds"".format(wait))
                time.sleep(wait)
            else:
                logger.warn(""exceeded restart retries"")

    def publish_hostname(self, hostname):
        self.set_dhcp_hostname(hostname)
        self.set_hostname_record(hostname)
        ifname = self.get_if_name()
        self.restart_if(ifname)

    def set_scsi_disks_timeout(self, timeout):
        for dev in os.listdir(""/sys/block""):
            if dev.startswith('sd'):
                self.set_block_device_timeout(dev, timeout)

    def set_block_device_timeout(self, dev, timeout):
        if dev is not None and timeout is not None:
            file_path = ""/sys/block/{0}/device/timeout"".format(dev)
            content = fileutil.read_file(file_path)
            original = content.splitlines()[0].rstrip()
            if original != timeout:
                fileutil.write_file(file_path, timeout)
                logger.info(""Set block dev timeout: {0} with timeout: {1}"",
                            dev, timeout)

    def get_mount_point(self, mountlist, device):
        """"""
        Example of mountlist:
            /dev/sda1 on / type ext4 (rw)
            proc on /proc type proc (rw)
            sysfs on /sys type sysfs (rw)
            devpts on /dev/pts type devpts (rw,gid=5,mode=620)
            tmpfs on /dev/shm type tmpfs
            (rw,rootcontext=""system_u:object_r:tmpfs_t:s0"")
            none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
            /dev/sdb1 on /mnt/resource type ext4 (rw)
        """"""
        if (mountlist and device):
            for entry in mountlist.split('\n'):
                if(re.search(device, entry)):
                    tokens = entry.split()
                    #Return the 3rd column of this line
                    return tokens[2] if len(tokens) > 2 else None
        return None

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        if port_id > 3:
            return None
        g0 = ""00000000""
        if port_id > 1:
            g0 = ""00000001""
            port_id = port_id - 2
        device = None
        path = ""/sys/bus/vmbus/devices/""
        if os.path.exists(path):
            try:
                for vmbus in os.listdir(path):
                    deviceid = fileutil.read_file(os.path.join(path, vmbus, ""device_id""))
                    guid = deviceid.lstrip('{').split('-')
                    if guid[0] == g0 and guid[1] == ""000"" + ustr(port_id):
                        for root, dirs, files in os.walk(path + vmbus):
                            if root.endswith(""/block""):
                                device = dirs[0]
                                break
                            else:
                                # older distros
                                for d in dirs:
                                    if ':' in d and ""block"" == d.split(':')[0]:
                                        device = d.split(':')[1]
                                        break
                        break
            except OSError as oe:
                logger.warn('Could not obtain device for IDE port {0}: {1}', port_id, ustr(oe))
        return device

    def set_hostname_record(self, hostname):
        fileutil.write_file(conf.get_published_hostname(), contents=hostname)

    def get_hostname_record(self):
        hostname_record = conf.get_published_hostname()
        if not os.path.exists(hostname_record):
            # this file is created at provisioning time with agents >= 2.2.3
            hostname = socket.gethostname()
            logger.info('Hostname record does not exist, '
                        'creating [{0}] with hostname [{1}]',
                        hostname_record,
                        hostname)
            self.set_hostname_record(hostname)
        record = fileutil.read_file(hostname_record)
        return record

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."", username)
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""userdel -f -r "" + username)
        self.conf_sudoer(username, remove=True)

    def decode_customdata(self, data):
        return base64.b64decode(data).decode('utf-8')

    def get_total_mem(self):
        # Get total memory in bytes and divide by 1024**2 to get the value in MB.
        return os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024**2)

    def get_processor_cores(self):
        return multiprocessing.cpu_count()

    def check_pid_alive(self, pid):
        try:
            pid = int(pid)
            os.kill(pid, 0)
        except (ValueError, TypeError):
            return False
        except OSError as e:
            if e.errno == errno.EPERM:
                return True
            return False
        return True

    @property
    def is_64bit(self):
        return sys.maxsize > 2**32

    @staticmethod
    def _get_proc_stat():
        """"""
        Get the contents of /proc/stat.
        # cpu  813599 3940 909253 154538746 874851 0 6589 0 0 0
        # cpu0 401094 1516 453006 77276738 452939 0 3312 0 0 0
        # cpu1 412505 2423 456246 77262007 421912 0 3276 0 0 0

        :return: A single string with the contents of /proc/stat
        :rtype: str
        """"""
        results = None
        try:
            results = fileutil.read_file('/proc/stat')
        except (OSError, IOError) as ex:
            logger.warn(""Couldn't read /proc/stat: {0}"".format(ex.strerror))

        return results

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        """"""
        Compute the number of USER_HZ units of time that have elapsed in all categories, across all cores, since boot.

        :return: int
        """"""
        system_cpu = 0
        proc_stat = DefaultOSUtil._get_proc_stat()
        if proc_stat is not None:
            for line in proc_stat.splitlines():
                if ALL_CPUS_REGEX.match(line):
                    system_cpu = sum(int(i) for i in line.split()[1:7])
                    break
        return system_cpu
/n/n/nazurelinuxagent/common/osutil/freebsd.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+

import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.osutil.default import DefaultOSUtil
from azurelinuxagent.common.future import ustr

class FreeBSDOSUtil(DefaultOSUtil):

    def __init__(self):
        super(FreeBSDOSUtil, self).__init__()
        self._scsi_disks_timeout_set = False
        self.jit_enabled = True

    def set_hostname(self, hostname):
        rc_file_path = '/etc/rc.conf'
        conf_file = fileutil.read_file(rc_file_path).split(""\n"")
        textutil.set_ini_config(conf_file, ""hostname"", hostname)
        fileutil.write_file(rc_file_path, ""\n"".join(conf_file))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run('service sshd restart', chk_err=False)

    def useradd(self, username, expiration=None, comment=None):
        """"""
        Create user account with 'username'
        """"""
        userentry = self.get_userentry(username)
        if userentry is not None:
            logger.warn(""User {0} already exists, skip useradd"", username)
            return
        if expiration is not None:
            cmd = ""pw useradd {0} -e {1} -m"".format(username, expiration)
        else:
            cmd = ""pw useradd {0} -m"".format(username)
        if comment is not None:
            cmd += "" -c {0}"".format(comment)
        retcode, out = shellutil.run_get_output(cmd)
        if retcode != 0:
            raise OSUtilError((""Failed to create user account:{0}, ""
                               ""retcode:{1}, ""
                               ""output:{2}"").format(username, retcode, out))

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."", username)
        shellutil.run('> /var/run/utx.active')
        shellutil.run('rmuser -y ' + username)
        self.conf_sudoer(username, remove=True)

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user, ""
                               ""will not set password."").format(username))
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        cmd = ""echo '{0}'|pw usermod {1} -H 0 "".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))

    def del_root_password(self):
        err = shellutil.run('pw usermod root -h -')
        if err:
            raise OSUtilError(""Failed to delete root password: Failed to update password database."")

    def get_if_mac(self, ifname):
        data = self._get_net_info()
        if data[0] == ifname:
            return data[2].replace(':', '').upper()
        return None

    def get_first_if(self):
        return self._get_net_info()[:2]

    def route_add(self, net, mask, gateway):
        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)
        return shellutil.run(cmd, chk_err=False)

    def is_missing_default_route(self):
        """"""
        For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to
        specify the route manually to get it work in a VNET environment.
        SEE ALSO: man ip(4) IP_ONESBCAST,
        """"""
        return True

    def is_dhcp_enabled(self):
        return True

    def start_dhcp_service(self):
        shellutil.run(""/etc/rc.d/dhclient start {0}"".format(self.get_if_name()), chk_err=False)

    def allow_dhcp_broadcast(self):
        pass

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route delete 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pgrep -n dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""cdcontrol -f {0} eject"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject dvd: ret={0}"".format(retcode))

    def restart_if(self, ifname):
        # Restart dhclient only to publish hostname
        shellutil.run(""/etc/rc.d/dhclient restart {0}"".format(ifname), chk_err=False)

    def get_total_mem(self):
        cmd = ""sysctl hw.physmem |awk '{print $2}'""
        ret, output = shellutil.run_get_output(cmd)
        if ret:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))
        try:
            return int(output)/1024/1024
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def get_processor_cores(self):
        ret, output = shellutil.run_get_output(""sysctl hw.ncpu |awk '{print $2}'"")
        if ret:
            raise OSUtilError(""Failed to get processor cores."")

        try:
            return int(output)
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def set_scsi_disks_timeout(self, timeout):
        if self._scsi_disks_timeout_set:
            return

        ret, output = shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout))
        if ret:
            raise OSUtilError(""Failed set SCSI disks timeout: {0}"".format(output))
        self._scsi_disks_timeout_set = True

    def check_pid_alive(self, pid):
        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0

    @staticmethod
    def _get_net_info():
        """"""
        There is no SIOCGIFCONF
        on freeBSD - just parse ifconfig.
        Returns strings: iface, inet4_addr, and mac
        or 'None,None,None' if unable to parse.
        We will sleep and retry as the network must be up.
        """"""
        iface = ''
        inet = ''
        mac = ''

        err, output = shellutil.run_get_output('ifconfig -l ether', chk_err=False)
        if err:
            raise OSUtilError(""Can't find ether interface:{0}"".format(output))
        ifaces = output.split()
        if not ifaces:
            raise OSUtilError(""Can't find ether interface."")
        iface = ifaces[0]

        err, output = shellutil.run_get_output('ifconfig ' + iface, chk_err=False)
        if err:
            raise OSUtilError(""Can't get info for interface:{0}"".format(iface))

        for line in output.split('\n'):
            if line.find('inet ') != -1:
                inet = line.split()[1]
            elif line.find('ether ') != -1:
                mac = line.split()[1]
        logger.verbose(""Interface info: ({0},{1},{2})"", iface, inet, mac)

        return iface, inet, mac

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        if port_id > 3:
            return None
        g0 = ""00000000""
        if port_id > 1:
            g0 = ""00000001""
            port_id = port_id - 2
        err, output = shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=')
        if err:
            return None
        g1 = ""000"" + ustr(port_id)
        g0g1 = ""{0}-{1}"".format(g0, g1)
        """"""
        search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000'
        """"""
        cmd_search_ide = ""sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}"".format(g0g1)
        err, output = shellutil.run_get_output(cmd_search_ide)
        if err:
            return None
        cmd_extract_id = cmd_search_ide + ""|awk -F . '{print $3}'""
        err, output = shellutil.run_get_output(cmd_extract_id)
        """"""
        try to search 'blkvscX' and 'storvscX' to find device name
        """"""
        output = output.rstrip()
        cmd_search_blkvsc = ""camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_blkvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible

        cmd_search_storvsc = ""camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_storvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible
        return None

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        return 0
/n/n/nazurelinuxagent/common/osutil/gaia.py/n/n#
# Copyright 2017 Check Point Software Technologies
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import base64
import socket
import struct
import time

import azurelinuxagent.common.conf as conf
from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.future import ustr, bytebuffer
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.osutil.default import DefaultOSUtil
from azurelinuxagent.common.utils.cryptutil import CryptUtil
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil


class GaiaOSUtil(DefaultOSUtil):

    def __init__(self):
        super(GaiaOSUtil, self).__init__()

    def _run_clish(self, cmd, log_cmd=True):
        for i in xrange(10):
            ret, out = shellutil.run_get_output(
                ""/bin/clish -s -c '"" + cmd + ""'"", log_cmd=log_cmd)
            if not ret:
                break
            if 'NMSHST0025' in out:  # Entry for [hostname] already present
                ret = 0
                break
            time.sleep(2)
        return ret, out

    def useradd(self, username, expiration=None):
        logger.warn('useradd is not supported on GAiA')

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        logger.info('chpasswd')
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        ret, out = self._run_clish(
            'set user admin password-hash ' + passwd_hash, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format('admin', out))

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        logger.info('conf_sudoer is not supported on GAiA')

    def del_root_password(self):
        logger.info('del_root_password')
        ret, out = self._run_clish('set user admin password-hash *LOCK*')
        if ret != 0:
            raise OSUtilError(""Failed to delete root password"")

    def _replace_user(self, path, username):
        if path.startswith('$HOME'):
            path = '/home' + path[5:]
        parts = path.split('/')
        parts[2] = username
        return '/'.join(parts)

    def deploy_ssh_keypair(self, username, keypair):
        logger.info('deploy_ssh_keypair')
        username = 'admin'
        path, thumbprint = keypair
        path = self._replace_user(path, username)
        super(GaiaOSUtil, self).deploy_ssh_keypair(
            username, (path, thumbprint))

    def openssl_to_openssh(self, input_file, output_file):
        cryptutil = CryptUtil(conf.get_openssl_cmd())
        ret, out = shellutil.run_get_output(
            conf.get_openssl_cmd() +
            "" rsa -pubin -noout -text -in '"" + input_file + ""'"")
        if ret != 0:
            raise OSUtilError('openssl failed with {0}'.format(ret))

        modulus = []
        exponent = []
        buf = None
        for line in out.split('\n'):
            if line.startswith('Modulus:'):
                buf = modulus
                buf.append(line)
                continue
            if line.startswith('Exponent:'):
                buf = exponent
                buf.append(line)
                continue
            if buf and line:
                buf.append(line.strip().replace(':', ''))

        def text_to_num(buf):
            if len(buf) == 1:
                return int(buf[0].split()[1])
            return long(''.join(buf[1:]), 16)

        n = text_to_num(modulus)
        e = text_to_num(exponent)

        keydata = bytearray()
        keydata.extend(struct.pack('>I', len('ssh-rsa')))
        keydata.extend(b'ssh-rsa')
        keydata.extend(struct.pack('>I', len(cryptutil.num_to_bytes(e))))
        keydata.extend(cryptutil.num_to_bytes(e))
        keydata.extend(struct.pack('>I', len(cryptutil.num_to_bytes(n)) + 1))
        keydata.extend(b'\0')
        keydata.extend(cryptutil.num_to_bytes(n))
        keydata_base64 = base64.b64encode(bytebuffer(keydata))
        fileutil.write_file(output_file,
                            ustr(b'ssh-rsa ' + keydata_base64 + b'\n',
                                 encoding='utf-8'))

    def deploy_ssh_pubkey(self, username, pubkey):
        logger.info('deploy_ssh_pubkey')
        username = 'admin'
        path, thumbprint, value = pubkey
        path = self._replace_user(path, username)
        super(GaiaOSUtil, self).deploy_ssh_pubkey(
            username, (path, thumbprint, value))

    def eject_dvd(self, chk_err=True):
        logger.warn('eject is not supported on GAiA')

    def mount(self, device, mount_point, option="""", chk_err=True):
        logger.info('mount {0} {1} {2}', device, mount_point, option)
        if 'udf,iso9660' in option:
            ret, out = super(GaiaOSUtil, self).mount(
                device, mount_point, option=option.replace('udf,iso9660', 'udf'),
                chk_err=chk_err)
            if not ret:
                return ret, out
        return super(GaiaOSUtil, self).mount(
            device, mount_point, option=option, chk_err=chk_err)

    def allow_dhcp_broadcast(self):
        logger.info('allow_dhcp_broadcast is ignored on GAiA')

    def remove_rules_files(self, rules_files=''):
        pass

    def restore_rules_files(self, rules_files=''):
        logger.info('restore_rules_files is ignored on GAiA')

    def restart_ssh_service(self):
        return shellutil.run('/sbin/service sshd condrestart', chk_err=False)

    def _address_to_string(self, addr):
        return socket.inet_ntoa(struct.pack(""!I"", addr))

    def _get_prefix(self, mask):
        return str(sum([bin(int(x)).count('1') for x in mask.split('.')]))

    def route_add(self, net, mask, gateway):
        logger.info('route_add {0} {1} {2}', net, mask, gateway)

        if net == 0 and mask == 0:
            cidr = 'default'
        else:
            cidr = self._address_to_string(net) + '/' + self._get_prefix(
                self._address_to_string(mask))

        ret, out = self._run_clish(
            'set static-route ' + cidr +
            ' nexthop gateway address ' +
            self._address_to_string(gateway) + ' on')
        return ret

    def set_hostname(self, hostname):
        logger.warn('set_hostname is ignored on GAiA')

    def set_dhcp_hostname(self, hostname):
        logger.warn('set_dhcp_hostname is ignored on GAiA')

    def publish_hostname(self, hostname):
        logger.warn('publish_hostname is ignored on GAiA')

    def del_account(self, username):
        logger.warn('del_account is ignored on GAiA')
/n/n/nazurelinuxagent/common/osutil/openbsd.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
# Copyright 2017 Reyk Floeter <reyk@openbsd.org>
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and OpenSSL 1.0+

import os
import re
import time
import glob
import datetime

import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.conf as conf

from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.osutil.default import DefaultOSUtil

UUID_PATTERN = re.compile(
    r'^\s*[A-F0-9]{8}(?:\-[A-F0-9]{4}){3}\-[A-F0-9]{12}\s*$',
    re.IGNORECASE)

class OpenBSDOSUtil(DefaultOSUtil):

    def __init__(self):
        super(OpenBSDOSUtil, self).__init__()
        self.jit_enabled = True
        self._scsi_disks_timeout_set = False

    def get_instance_id(self):
        ret, output = shellutil.run_get_output(""sysctl -n hw.uuid"")
        if ret != 0 or UUID_PATTERN.match(output) is None:
            return """"
        return output.strip()

    def set_hostname(self, hostname):
        fileutil.write_file(""/etc/myname"", ""{}\n"".format(hostname))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run('rcctl restart sshd', chk_err=False)

    def start_agent_service(self):
        return shellutil.run('rcctl start waagent', chk_err=False)

    def stop_agent_service(self):
        return shellutil.run('rcctl stop waagent', chk_err=False)

    def register_agent_service(self):
        shellutil.run('chmod 0555 /etc/rc.d/waagent', chk_err=False)
        return shellutil.run('rcctl enable waagent', chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run('rcctl disable waagent', chk_err=False)

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."",
                         username)
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""userdel -r "" + username)
        self.conf_sudoer(username, remove=True)

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        doas_conf = ""/etc/doas.conf""
        doas = None
        if not remove:
            if not os.path.isfile(doas_conf):
                # always allow root to become root
                doas = ""permit keepenv nopass root\n""
                fileutil.append_file(doas_conf, doas)
            if nopasswd:
                doas = ""permit keepenv nopass {0}\n"".format(username)
            else:
                doas = ""permit keepenv persist {0}\n"".format(username)
            fileutil.append_file(doas_conf, doas)
            fileutil.chmod(doas_conf, 0o644)
        else:
            # Remove user from doas.conf
            if os.path.isfile(doas_conf):
                try:
                    content = fileutil.read_file(doas_conf)
                    doas = content.split(""\n"")
                    doas = [x for x in doas if username not in x]
                    fileutil.write_file(doas_conf, ""\n"".join(doas))
                except IOError as err:
                    raise OSUtilError(""Failed to remove sudoer: ""
                                      ""{0}"".format(err))

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user. ""
                               ""Will not set passwd."").format(username))
        cmd = ""echo -n {0}|encrypt"".format(password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to encrypt password for {0}: {1}""
                               """").format(username, output))
        passwd_hash = output.strip()
        cmd = ""usermod -p '{0}' {1}"".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))

    def del_root_password(self):
        ret, output = shellutil.run_get_output('usermod -p ""*"" root')
        if ret:
            raise OSUtilError(""Failed to delete root password: ""
                              ""{0}"".format(output))

    def get_if_mac(self, ifname):
        data = self._get_net_info()
        if data[0] == ifname:
            return data[2].replace(':', '').upper()
        return None

    def get_first_if(self):
        return self._get_net_info()[:2]

    def route_add(self, net, mask, gateway):
        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)
        return shellutil.run(cmd, chk_err=False)

    def is_missing_default_route(self):
        ret = shellutil.run(""route -n get default"", chk_err=False)
        if ret == 0:
            return False
        return True

    def is_dhcp_enabled(self):
        pass

    def start_dhcp_service(self):
        pass

    def stop_dhcp_service(self):
        pass

    def get_dhcp_lease_endpoint(self):
        """"""
        OpenBSD has a sligthly different lease file format.
        """"""
        endpoint = None
        pathglob = '/var/db/dhclient.leases.{}'.format(self.get_first_if()[0])

        HEADER_LEASE = ""lease""
        HEADER_OPTION = ""option option-245""
        HEADER_EXPIRE = ""expire""
        FOOTER_LEASE = ""}""
        FORMAT_DATETIME = ""%Y/%m/%d %H:%M:%S %Z""

        logger.info(""looking for leases in path [{0}]"".format(pathglob))
        for lease_file in glob.glob(pathglob):
            leases = open(lease_file).read()
            if HEADER_OPTION in leases:
                cached_endpoint = None
                has_option_245 = False
                expired = True  # assume expired
                for line in leases.splitlines():
                    if line.startswith(HEADER_LEASE):
                        cached_endpoint = None
                        has_option_245 = False
                        expired = True
                    elif HEADER_OPTION in line:
                        try:
                            ipaddr = line.split("" "")[-1].strip("";"").split("":"")
                            cached_endpoint = \
                               ""."".join(str(int(d, 16)) for d in ipaddr)
                            has_option_245 = True
                        except ValueError:
                            logger.error(""could not parse '{0}'"".format(line))
                    elif HEADER_EXPIRE in line:
                        if ""never"" in line:
                            expired = False
                        else:
                            try:
                                expire_string = line.split(
                                    "" "", 4)[-1].strip("";"")
                                expire_date = datetime.datetime.strptime(
                                    expire_string, FORMAT_DATETIME)
                                if expire_date > datetime.datetime.utcnow():
                                    expired = False
                            except ValueError:
                                logger.error(""could not parse expiry token ""
                                             ""'{0}'"".format(line))
                    elif FOOTER_LEASE in line:
                        logger.info(""dhcp entry:{0}, 245:{1}, expired: {2}""
                                    .format(cached_endpoint, has_option_245, expired))
                        if not expired and cached_endpoint is not None and has_option_245:
                            endpoint = cached_endpoint
                            logger.info(""found endpoint [{0}]"".format(endpoint))
                            # we want to return the last valid entry, so
                            # keep searching
        if endpoint is not None:
            logger.info(""cached endpoint found [{0}]"".format(endpoint))
        else:
            logger.info(""cached endpoint not found"")
        return endpoint

    def allow_dhcp_broadcast(self):
        pass

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 -iface ""
                             ""{0}"".format(ifname), chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route delete 255.255.255.255 -iface ""
                      ""{0}"".format(ifname), chk_err=False)

    def get_dhcp_pid(self):
        ret, output = shellutil.run_get_output(""pgrep -n dhclient"",
                                               chk_err=False)
        return output if ret == 0 else None

    def get_dvd_device(self, dev_dir='/dev'):
        pattern = r'cd[0-9]c'
        for dvd in [re.match(pattern, dev) for dev in os.listdir(dev_dir)]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        raise OSUtilError(""Failed to get DVD device"")

    def mount_dvd(self,
                  max_retry=6,
                  chk_err=True,
                  dvd_device=None,
                  mount_point=None,
                  sleep_time=5):
        if dvd_device is None:
            dvd_device = self.get_dvd_device()
        if mount_point is None:
            mount_point = conf.get_dvd_mount_point()
        if not os.path.isdir(mount_point):
            os.makedirs(mount_point)

        for retry in range(0, max_retry):
            retcode = self.mount(dvd_device,
                                mount_point,
                                option=""-o ro -t udf"",
                                chk_err=False)
            if retcode == 0:
                logger.info(""Successfully mounted DVD"")
                return
            if retry < max_retry - 1:
                mountlist = shellutil.run_get_output(""/sbin/mount"")[1]
                existing = self.get_mount_point(mountlist, dvd_device)
                if existing is not None:
                    logger.info(""{0} is mounted at {1}"", dvd_device, existing)
                    return
                logger.warn(""Mount DVD failed: retry={0}, ret={1}"", retry,
                            retcode)
                time.sleep(sleep_time)
        if chk_err:
            raise OSUtilError(""Failed to mount DVD."")

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""cdio eject {0}"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject DVD: ret={0}"".format(retcode))

    def restart_if(self, ifname, retries=3, wait=5):
        # Restart dhclient only to publish hostname
        shellutil.run(""/sbin/dhclient {0}"".format(ifname), chk_err=False)

    def get_total_mem(self):
        ret, output = shellutil.run_get_output(""sysctl -n hw.physmem"")
        if ret:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))
        try:
            return int(output)/1024/1024
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def get_processor_cores(self):
        ret, output = shellutil.run_get_output(""sysctl -n hw.ncpu"")
        if ret:
            raise OSUtilError(""Failed to get processor cores."")

        try:
            return int(output)
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def set_scsi_disks_timeout(self, timeout):
        pass

    def check_pid_alive(self, pid):
        if not pid:
            return
        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0

    @staticmethod
    def _get_net_info():
        """"""
        There is no SIOCGIFCONF
        on OpenBSD - just parse ifconfig.
        Returns strings: iface, inet4_addr, and mac
        or 'None,None,None' if unable to parse.
        We will sleep and retry as the network must be up.
        """"""
        iface = ''
        inet = ''
        mac = ''

        ret, output = shellutil.run_get_output(
            'ifconfig hvn | grep -E ""^hvn.:"" | sed ""s/:.*//g""', chk_err=False)
        if ret:
            raise OSUtilError(""Can't find ether interface:{0}"".format(output))
        ifaces = output.split()
        if not ifaces:
            raise OSUtilError(""Can't find ether interface."")
        iface = ifaces[0]

        ret, output = shellutil.run_get_output(
            'ifconfig ' + iface, chk_err=False)
        if ret:
            raise OSUtilError(""Can't get info for interface:{0}"".format(iface))

        for line in output.split('\n'):
            if line.find('inet ') != -1:
                inet = line.split()[1]
            elif line.find('lladdr ') != -1:
                mac = line.split()[1]
        logger.verbose(""Interface info: ({0},{1},{2})"", iface, inet, mac)

        return iface, inet, mac

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        return ""wd{0}"".format(port_id)

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        return 0
/n/n/nazurelinuxagent/common/osutil/redhat.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import base64
import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.future import ustr, bytebuffer
from azurelinuxagent.common.exception import OSUtilError, CryptError
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.osutil.default import DefaultOSUtil


class Redhat6xOSUtil(DefaultOSUtil):

    def __init__(self):
        super(Redhat6xOSUtil, self).__init__()
        self.jit_enabled = True

    def start_network(self):
        return shellutil.run(""/sbin/service networking start"", chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run(""/sbin/service sshd condrestart"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""chkconfig --add waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""chkconfig --del waagent"", chk_err=False)

    def openssl_to_openssh(self, input_file, output_file):
        pubkey = fileutil.read_file(input_file)
        try:
            cryptutil = CryptUtil(conf.get_openssl_cmd())
            ssh_rsa_pubkey = cryptutil.asn1_to_ssh(pubkey)
        except CryptError as e:
            raise OSUtilError(ustr(e))
        fileutil.write_file(output_file, ssh_rsa_pubkey)

    # Override
    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        """"""
        Set /etc/sysconfig/network
        """"""
        fileutil.update_conf_file('/etc/sysconfig/network',
                                  'HOSTNAME',
                                  'HOSTNAME={0}'.format(hostname))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def set_dhcp_hostname(self, hostname):
        ifname = self.get_if_name()
        filepath = ""/etc/sysconfig/network-scripts/ifcfg-{0}"".format(ifname)
        fileutil.update_conf_file(filepath,
                                  'DHCP_HOSTNAME',
                                  'DHCP_HOSTNAME={0}'.format(hostname))

    def get_dhcp_lease_endpoint(self):
        return self.get_endpoint_from_leases_path('/var/lib/dhclient/dhclient-*.leases')


class RedhatOSUtil(Redhat6xOSUtil):
    def __init__(self):
        super(RedhatOSUtil, self).__init__()

    def set_hostname(self, hostname):
        """"""
        Unlike redhat 6.x, redhat 7.x will set hostname via hostnamectl
        Due to a bug in systemd in Centos-7.0, if this call fails, fallback
        to hostname.
        """"""
        hostnamectl_cmd = ""hostnamectl set-hostname {0} --static"".format(hostname)
        if shellutil.run(hostnamectl_cmd, chk_err=False) != 0:
            logger.warn(""[{0}] failed, attempting fallback"".format(hostnamectl_cmd))
            DefaultOSUtil.set_hostname(self, hostname)

    def publish_hostname(self, hostname):
        """"""
        Restart NetworkManager first before publishing hostname
        """"""
        shellutil.run(""service NetworkManager restart"")
        super(RedhatOSUtil, self).publish_hostname(hostname)

    def register_agent_service(self):
        return shellutil.run(""systemctl enable waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""systemctl disable waagent"", chk_err=False)

    def openssl_to_openssh(self, input_file, output_file):
        DefaultOSUtil.openssl_to_openssh(self, input_file, output_file)

    def get_dhcp_lease_endpoint(self):
        # dhclient
        endpoint = self.get_endpoint_from_leases_path('/var/lib/dhclient/dhclient-*.lease')

        if endpoint is None:
            # NetworkManager
            endpoint = self.get_endpoint_from_leases_path('/var/lib/NetworkManager/dhclient-*.lease')

        return endpoint
/n/n/nazurelinuxagent/common/osutil/suse.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.version import DISTRO_NAME, DISTRO_VERSION, DISTRO_FULL_NAME
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class SUSE11OSUtil(DefaultOSUtil):

    def __init__(self):
        super(SUSE11OSUtil, self).__init__()
        self.jit_enabled = True
        self.dhclient_name='dhcpcd'

    def set_hostname(self, hostname):
        fileutil.write_file('/etc/HOSTNAME', hostname)
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof {0}"".format(self.dhclient_name),
                                       chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def is_dhcp_enabled(self):
        return True

    def stop_dhcp_service(self):
        cmd = ""/sbin/service {0} stop"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_dhcp_service(self):
        cmd = ""/sbin/service {0} start"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_network(self) :
        return shellutil.run(""/sbin/service start network"", chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run(""/sbin/service sshd restart"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""/sbin/insserv waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""/sbin/insserv -r waagent"", chk_err=False)

class SUSEOSUtil(SUSE11OSUtil):
    def __init__(self):
        super(SUSEOSUtil, self).__init__()
        self.dhclient_name = 'wickedd-dhcp4'

    def stop_dhcp_service(self):
        cmd = ""systemctl stop {0}"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_dhcp_service(self):
        cmd = ""systemctl start {0}"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_network(self) :
        return shellutil.run(""systemctl start network"", chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run(""systemctl restart sshd"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""systemctl enable waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""systemctl disable waagent"", chk_err=False)


/n/n/nazurelinuxagent/common/osutil/ubuntu.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import time

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil

from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.osutil.default import DefaultOSUtil


def _cgroup_path(tail=""""):
    return os.path.join('/sys/fs/cgroup/', tail).rstrip(os.path.sep)


class Ubuntu14OSUtil(DefaultOSUtil):

    def __init__(self):
        super(Ubuntu14OSUtil, self).__init__()
        self.jit_enabled = True

    def start_network(self):
        return shellutil.run(""service networking start"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""service walinuxagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""service walinuxagent start"", chk_err=False)

    def remove_rules_files(self, rules_files=""""):
        pass

    def restore_rules_files(self, rules_files=""""):
        pass

    def get_dhcp_lease_endpoint(self):
        return self.get_endpoint_from_leases_path('/var/lib/dhcp/dhclient.*.leases')

    def is_cgroups_supported(self):
        if 'TRAVIS' in os.environ and os.environ['TRAVIS'] == 'true':
            return False
        return True

    def mount_cgroups(self):
        try:
            if not os.path.exists(_cgroup_path()):
                fileutil.mkdir(_cgroup_path())
                self.mount(device='cgroup_root',
                           mount_point=_cgroup_path(),
                           option=""-t tmpfs"",
                           chk_err=False)
            elif not os.path.isdir(_cgroup_path()):
                logger.error(""Could not mount cgroups: ordinary file at {0}"".format(_cgroup_path()))
                return

            for metric_hierarchy in ['cpu,cpuacct', 'memory']:
                target_path = _cgroup_path(metric_hierarchy)
                if not os.path.exists(target_path):
                    fileutil.mkdir(target_path)
                self.mount(device=metric_hierarchy,
                           mount_point=target_path,
                           option=""-t cgroup -o {0}"".format(metric_hierarchy),
                           chk_err=False)

            for metric_hierarchy in ['cpu', 'cpuacct']:
                target_path = _cgroup_path(metric_hierarchy)
                if not os.path.exists(target_path):
                    os.symlink(_cgroup_path('cpu,cpuacct'), target_path)

        except Exception as e:
            logger.error(""Could not mount cgroups: {0}"", ustr(e))


class Ubuntu12OSUtil(Ubuntu14OSUtil):
    def __init__(self):
        super(Ubuntu12OSUtil, self).__init__()

    # Override
    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof dhclient3"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def mount_cgroups(self):
        pass

class Ubuntu16OSUtil(Ubuntu14OSUtil):
    """"""
    Ubuntu 16.04, 16.10, and 17.04.
    """"""
    def __init__(self):
        super(Ubuntu16OSUtil, self).__init__()

    def register_agent_service(self):
        return shellutil.run(""systemctl unmask walinuxagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""systemctl mask walinuxagent"", chk_err=False)

    def mount_cgroups(self):
        """"""
        Mounted by default in Ubuntu 16.04
        """"""
        pass


class Ubuntu18OSUtil(Ubuntu16OSUtil):
    """"""
    Ubuntu 18.04
    """"""
    def __init__(self):
        super(Ubuntu18OSUtil, self).__init__()

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof systemd-networkd"")
        return ret[1] if ret[0] == 0 else None

    def start_network(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def stop_network(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return self.start_network()

    def stop_dhcp_service(self):
        return self.stop_network()

    def start_agent_service(self):
        return shellutil.run(""systemctl start walinuxagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop walinuxagent"", chk_err=False)


class UbuntuOSUtil(Ubuntu16OSUtil):
    def __init__(self):
        super(UbuntuOSUtil, self).__init__()

    def restart_if(self, ifname, retries=3, wait=5):
        """"""
        Restart an interface by bouncing the link. systemd-networkd observes
        this event, and forces a renew of DHCP.
        """"""
        retry_limit=retries+1
        for attempt in range(1, retry_limit):
            return_code=shellutil.run(""ip link set {0} down && ip link set {0} up"".format(ifname))
            if return_code == 0:
                return
            logger.warn(""failed to restart {0}: return code {1}"".format(ifname, return_code))
            if attempt < retry_limit:
                logger.info(""retrying in {0} seconds"".format(wait))
                time.sleep(wait)
            else:
                logger.warn(""exceeded restart retries"")


class UbuntuSnappyOSUtil(Ubuntu14OSUtil):
    def __init__(self):
        super(UbuntuSnappyOSUtil, self).__init__()
        self.conf_file_path = '/apps/walinuxagent/current/waagent.conf'

    def mount_cgroups(self):
        """"""
        Already mounted in Snappy
        """"""
        pass
/n/n/nazurelinuxagent/common/protocol/restapi.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#
import socket
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.restutil as restutil
from azurelinuxagent.common.exception import ProtocolError, HttpError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.version import DISTRO_VERSION, DISTRO_NAME, CURRENT_VERSION


def validate_param(name, val, expected_type):
    if val is None:
        raise ProtocolError(""{0} is None"".format(name))
    if not isinstance(val, expected_type):
        raise ProtocolError((""{0} type should be {1} not {2}""
                             """").format(name, expected_type, type(val)))


def set_properties(name, obj, data):
    if isinstance(obj, DataContract):
        validate_param(""Property '{0}'"".format(name), data, dict)
        for prob_name, prob_val in data.items():
            prob_full_name = ""{0}.{1}"".format(name, prob_name)
            try:
                prob = getattr(obj, prob_name)
            except AttributeError:
                logger.warn(""Unknown property: {0}"", prob_full_name)
                continue
            prob = set_properties(prob_full_name, prob, prob_val)
            setattr(obj, prob_name, prob)
        return obj
    elif isinstance(obj, DataContractList):
        validate_param(""List '{0}'"".format(name), data, list)
        for item_data in data:
            item = obj.item_cls()
            item = set_properties(name, item, item_data)
            obj.append(item)
        return obj
    else:
        return data


def get_properties(obj):
    if isinstance(obj, DataContract):
        data = {}
        props = vars(obj)
        for prob_name, prob in list(props.items()):
            data[prob_name] = get_properties(prob)
        return data
    elif isinstance(obj, DataContractList):
        data = []
        for item in obj:
            item_data = get_properties(item)
            data.append(item_data)
        return data
    else:
        return obj


class DataContract(object):
    pass


class DataContractList(list):
    def __init__(self, item_cls):
        self.item_cls = item_cls


""""""
Data contract between guest and host
""""""


class VMInfo(DataContract):
    def __init__(self,
                 subscriptionId=None,
                 vmName=None,
                 containerId=None,
                 roleName=None,
                 roleInstanceName=None,
                 tenantName=None):
        self.subscriptionId = subscriptionId
        self.vmName = vmName
        self.containerId = containerId
        self.roleName = roleName
        self.roleInstanceName = roleInstanceName
        self.tenantName = tenantName


class CertificateData(DataContract):
    def __init__(self, certificateData=None):
        self.certificateData = certificateData


class Cert(DataContract):
    def __init__(self,
                 name=None,
                 thumbprint=None,
                 certificateDataUri=None,
                 storeName=None,
                 storeLocation=None):
        self.name = name
        self.thumbprint = thumbprint
        self.certificateDataUri = certificateDataUri
        self.storeLocation = storeLocation
        self.storeName = storeName


class CertList(DataContract):
    def __init__(self):
        self.certificates = DataContractList(Cert)


# TODO: confirm vmagent manifest schema
class VMAgentManifestUri(DataContract):
    def __init__(self, uri=None):
        self.uri = uri


class VMAgentManifest(DataContract):
    def __init__(self, family=None):
        self.family = family
        self.versionsManifestUris = DataContractList(VMAgentManifestUri)


class VMAgentManifestList(DataContract):
    def __init__(self):
        self.vmAgentManifests = DataContractList(VMAgentManifest)


class Extension(DataContract):
    def __init__(self,
                 name=None,
                 sequenceNumber=None,
                 publicSettings=None,
                 protectedSettings=None,
                 certificateThumbprint=None):
        self.name = name
        self.sequenceNumber = sequenceNumber
        self.publicSettings = publicSettings
        self.protectedSettings = protectedSettings
        self.certificateThumbprint = certificateThumbprint


class ExtHandlerProperties(DataContract):
    def __init__(self):
        self.version = None
        self.upgradePolicy = None
        self.upgradeGuid = None
        self.dependencyLevel = None
        self.state = None
        self.extensions = DataContractList(Extension)


class ExtHandlerVersionUri(DataContract):
    def __init__(self):
        self.uri = None


class ExtHandler(DataContract):
    def __init__(self, name=None):
        self.name = name
        self.properties = ExtHandlerProperties()
        self.versionUris = DataContractList(ExtHandlerVersionUri)

    def sort_key(self):
        level = self.properties.dependencyLevel
        if level is None:
            level = 0
        # Process uninstall or disabled before enabled, in reverse order
        # remap 0 to -1, 1 to -2, 2 to -3, etc
        if self.properties.state != u""enabled"":
            level = (0 - level) - 1
        return level


class ExtHandlerList(DataContract):
    def __init__(self):
        self.extHandlers = DataContractList(ExtHandler)


class ExtHandlerPackageUri(DataContract):
    def __init__(self, uri=None):
        self.uri = uri


class ExtHandlerPackage(DataContract):
    def __init__(self, version=None):
        self.version = version
        self.uris = DataContractList(ExtHandlerPackageUri)
        # TODO update the naming to align with metadata protocol
        self.isinternal = False
        self.disallow_major_upgrade = False


class ExtHandlerPackageList(DataContract):
    def __init__(self):
        self.versions = DataContractList(ExtHandlerPackage)


class VMProperties(DataContract):
    def __init__(self, certificateThumbprint=None):
        # TODO need to confirm the property name
        self.certificateThumbprint = certificateThumbprint


class ProvisionStatus(DataContract):
    def __init__(self, status=None, subStatus=None, description=None):
        self.status = status
        self.subStatus = subStatus
        self.description = description
        self.properties = VMProperties()


class ExtensionSubStatus(DataContract):
    def __init__(self, name=None, status=None, code=None, message=None):
        self.name = name
        self.status = status
        self.code = code
        self.message = message


class ExtensionStatus(DataContract):
    def __init__(self,
                 configurationAppliedTime=None,
                 operation=None,
                 status=None,
                 seq_no=None,
                 code=None,
                 message=None):
        self.configurationAppliedTime = configurationAppliedTime
        self.operation = operation
        self.status = status
        self.sequenceNumber = seq_no
        self.code = code
        self.message = message
        self.substatusList = DataContractList(ExtensionSubStatus)


class ExtHandlerStatus(DataContract):
    def __init__(self,
                 name=None,
                 version=None,
                 upgradeGuid=None,
                 status=None,
                 code=0,
                 message=None):
        self.name = name
        self.version = version
        self.upgradeGuid = upgradeGuid
        self.status = status
        self.code = code
        self.message = message
        self.extensions = DataContractList(ustr)


class VMAgentStatus(DataContract):
    def __init__(self, status=None, message=None):
        self.status = status
        self.message = message
        self.hostname = socket.gethostname()
        self.version = str(CURRENT_VERSION)
        self.osname = DISTRO_NAME
        self.osversion = DISTRO_VERSION
        self.extensionHandlers = DataContractList(ExtHandlerStatus)


class VMStatus(DataContract):
    def __init__(self, status, message):
        self.vmAgent = VMAgentStatus(status=status, message=message)


class TelemetryEventParam(DataContract):
    def __init__(self, name=None, value=None):
        self.name = name
        self.value = value


class TelemetryEvent(DataContract):
    def __init__(self, eventId=None, providerId=None):
        self.eventId = eventId
        self.providerId = providerId
        self.parameters = DataContractList(TelemetryEventParam)


class TelemetryEventList(DataContract):
    def __init__(self):
        self.events = DataContractList(TelemetryEvent)

class RemoteAccessUser(DataContract):
    def __init__(self, name, encrypted_password, expiration):
        self.name = name
        self.encrypted_password = encrypted_password
        self.expiration = expiration

class RemoteAccessUsersList(DataContract):
    def __init__(self):
        self.users = DataContractList(RemoteAccessUser)


class Protocol(DataContract):
    def detect(self):
        raise NotImplementedError()

    def get_vminfo(self):
        raise NotImplementedError()

    def get_certs(self):
        raise NotImplementedError()

    def get_incarnation(self):
        raise NotImplementedError()

    def get_vmagent_manifests(self):
        raise NotImplementedError()

    def get_vmagent_pkgs(self, manifest):
        raise NotImplementedError()

    def get_ext_handlers(self):
        raise NotImplementedError()

    def get_ext_handler_pkgs(self, extension):
        raise NotImplementedError()

    def get_artifacts_profile(self):
        raise NotImplementedError()

    def download_ext_handler_pkg(self, uri, headers=None, use_proxy=True):
        pkg = None
        try:
            resp = restutil.http_get(uri, headers=headers, use_proxy=use_proxy)
            if restutil.request_succeeded(resp):
                pkg = resp.read()
        except Exception as e:
            logger.warn(""Failed to download from: {0}"".format(uri), e)
        return pkg

    def report_provision_status(self, provision_status):
        raise NotImplementedError()

    def report_vm_status(self, vm_status):
        raise NotImplementedError()

    def report_ext_status(self, ext_handler_name, ext_name, ext_status):
        raise NotImplementedError()

    def report_event(self, event):
        raise NotImplementedError()
/n/n/nazurelinuxagent/common/protocol/wire.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+

from datetime import datetime

import json
import os
import random
import re
import sys
import time
import xml.sax.saxutils as saxutils

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.textutil as textutil

from azurelinuxagent.common.exception import ProtocolNotFoundError, \
                                            ResourceGoneError
from azurelinuxagent.common.future import httpclient, bytebuffer
from azurelinuxagent.common.protocol.hostplugin import HostPluginProtocol, URI_FORMAT_GET_EXTENSION_ARTIFACT, \
    HOST_PLUGIN_PORT
from azurelinuxagent.common.protocol.restapi import *
from azurelinuxagent.common.utils.archive import StateFlusher
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.utils.textutil import parse_doc, findall, find, \
    findtext, getattrib, gettext, remove_bom, get_bytes_from_pem, parse_json
from azurelinuxagent.common.version import AGENT_NAME
from azurelinuxagent.common.osutil import get_osutil

VERSION_INFO_URI = ""http://{0}/?comp=versions""
GOAL_STATE_URI = ""http://{0}/machine/?comp=goalstate""
HEALTH_REPORT_URI = ""http://{0}/machine?comp=health""
ROLE_PROP_URI = ""http://{0}/machine?comp=roleProperties""
TELEMETRY_URI = ""http://{0}/machine?comp=telemetrydata""

WIRE_SERVER_ADDR_FILE_NAME = ""WireServer""
INCARNATION_FILE_NAME = ""Incarnation""
GOAL_STATE_FILE_NAME = ""GoalState.{0}.xml""
HOSTING_ENV_FILE_NAME = ""HostingEnvironmentConfig.xml""
SHARED_CONF_FILE_NAME = ""SharedConfig.xml""
CERTS_FILE_NAME = ""Certificates.xml""
REMOTE_ACCESS_FILE_NAME = ""RemoteAccess.{0}.xml""
P7M_FILE_NAME = ""Certificates.p7m""
PEM_FILE_NAME = ""Certificates.pem""
EXT_CONF_FILE_NAME = ""ExtensionsConfig.{0}.xml""
MANIFEST_FILE_NAME = ""{0}.{1}.manifest.xml""
AGENTS_MANIFEST_FILE_NAME = ""{0}.{1}.agentsManifest""
TRANSPORT_CERT_FILE_NAME = ""TransportCert.pem""
TRANSPORT_PRV_FILE_NAME = ""TransportPrivate.pem""

PROTOCOL_VERSION = ""2012-11-30""
ENDPOINT_FINE_NAME = ""WireServer""

SHORT_WAITING_INTERVAL = 1  # 1 second


class UploadError(HttpError):
    pass


class WireProtocol(Protocol):
    """"""Slim layer to adapt wire protocol data to metadata protocol interface""""""

    # TODO: Clean-up goal state processing
    #  At present, some methods magically update GoalState (e.g.,
    #  get_vmagent_manifests), others (e.g., get_vmagent_pkgs)
    #  assume its presence. A better approach would make an explicit update
    #  call that returns the incarnation number and
    #  establishes that number the ""context"" for all other calls (either by
    #  updating the internal state of the protocol or
    #  by having callers pass the incarnation number to the method).

    def __init__(self, endpoint):
        if endpoint is None:
            raise ProtocolError(""WireProtocol endpoint is None"")
        self.endpoint = endpoint
        self.client = WireClient(self.endpoint)

    def detect(self):
        self.client.check_wire_protocol_version()

        trans_prv_file = os.path.join(conf.get_lib_dir(),
                                      TRANSPORT_PRV_FILE_NAME)
        trans_cert_file = os.path.join(conf.get_lib_dir(),
                                       TRANSPORT_CERT_FILE_NAME)
        cryptutil = CryptUtil(conf.get_openssl_cmd())
        cryptutil.gen_transport_cert(trans_prv_file, trans_cert_file)

        self.update_goal_state(forced=True)

    def update_goal_state(self, forced=False, max_retry=3):
        self.client.update_goal_state(forced=forced, max_retry=max_retry)

    def get_vminfo(self):
        goal_state = self.client.get_goal_state()
        hosting_env = self.client.get_hosting_env()

        vminfo = VMInfo()
        vminfo.subscriptionId = None
        vminfo.vmName = hosting_env.vm_name
        vminfo.tenantName = hosting_env.deployment_name
        vminfo.roleName = hosting_env.role_name
        vminfo.roleInstanceName = goal_state.role_instance_id
        vminfo.containerId = goal_state.container_id
        return vminfo

    def get_certs(self):
        certificates = self.client.get_certs()
        return certificates.cert_list

    def get_incarnation(self):
        path = os.path.join(conf.get_lib_dir(), INCARNATION_FILE_NAME)
        if os.path.exists(path):
            return fileutil.read_file(path)
        else:
            return 0

    def get_vmagent_manifests(self):
        # Update goal state to get latest extensions config
        self.update_goal_state()
        goal_state = self.client.get_goal_state()
        ext_conf = self.client.get_ext_conf()
        return ext_conf.vmagent_manifests, goal_state.incarnation

    def get_vmagent_pkgs(self, vmagent_manifest):
        goal_state = self.client.get_goal_state()
        ga_manifest = self.client.get_gafamily_manifest(vmagent_manifest, goal_state)
        valid_pkg_list = self.client.filter_package_list(vmagent_manifest.family, ga_manifest, goal_state)
        return valid_pkg_list

    def get_ext_handlers(self):
        logger.verbose(""Get extension handler config"")
        # Update goal state to get latest extensions config
        self.update_goal_state()
        goal_state = self.client.get_goal_state()
        ext_conf = self.client.get_ext_conf()
        # In wire protocol, incarnation is equivalent to ETag
        return ext_conf.ext_handlers, goal_state.incarnation

    def get_ext_handler_pkgs(self, ext_handler):
        logger.verbose(""Get extension handler package"")
        goal_state = self.client.get_goal_state()
        man = self.client.get_ext_manifest(ext_handler, goal_state)
        return man.pkg_list

    def get_artifacts_profile(self):
        logger.verbose(""Get In-VM Artifacts Profile"")
        return self.client.get_artifacts_profile()

    def download_ext_handler_pkg(self, uri, headers=None, use_proxy=True):
        package = self.client.fetch(uri, headers=headers, use_proxy=use_proxy, decode=False)

        if package is None:
            logger.verbose(""Download did not succeed, falling back to host plugin"")
            host = self.client.get_host_plugin()
            uri, headers = host.get_artifact_request(uri, host.manifest_uri)
            package = self.client.fetch(uri, headers=headers, use_proxy=False, decode=False)

        return package

    def report_provision_status(self, provision_status):
        validate_param(""provision_status"", provision_status, ProvisionStatus)

        if provision_status.status is not None:
            self.client.report_health(provision_status.status,
                                      provision_status.subStatus,
                                      provision_status.description)
        if provision_status.properties.certificateThumbprint is not None:
            thumbprint = provision_status.properties.certificateThumbprint
            self.client.report_role_prop(thumbprint)

    def report_vm_status(self, vm_status):
        validate_param(""vm_status"", vm_status, VMStatus)
        self.client.status_blob.set_vm_status(vm_status)
        self.client.upload_status_blob()

    def report_ext_status(self, ext_handler_name, ext_name, ext_status):
        validate_param(""ext_status"", ext_status, ExtensionStatus)
        self.client.status_blob.set_ext_status(ext_handler_name, ext_status)

    def report_event(self, events):
        validate_param(""events"", events, TelemetryEventList)
        self.client.report_event(events)


def _build_role_properties(container_id, role_instance_id, thumbprint):
    xml = (u""<?xml version=\""1.0\"" encoding=\""utf-8\""?>""
           u""<RoleProperties>""
           u""<Container>""
           u""<ContainerId>{0}</ContainerId>""
           u""<RoleInstances>""
           u""<RoleInstance>""
           u""<Id>{1}</Id>""
           u""<Properties>""
           u""<Property name=\""CertificateThumbprint\"" value=\""{2}\"" />""
           u""</Properties>""
           u""</RoleInstance>""
           u""</RoleInstances>""
           u""</Container>""
           u""</RoleProperties>""
           u"""").format(container_id, role_instance_id, thumbprint)
    return xml


def _build_health_report(incarnation, container_id, role_instance_id,
                         status, substatus, description):
    # Escape '&', '<' and '>'
    description = saxutils.escape(ustr(description))
    detail = u''
    if substatus is not None:
        substatus = saxutils.escape(ustr(substatus))
        detail = (u""<Details>""
                  u""<SubStatus>{0}</SubStatus>""
                  u""<Description>{1}</Description>""
                  u""</Details>"").format(substatus, description)
    xml = (u""<?xml version=\""1.0\"" encoding=\""utf-8\""?>""
           u""<Health ""
           u""xmlns:xsi=\""http://www.w3.org/2001/XMLSchema-instance\""""
           u"" xmlns:xsd=\""http://www.w3.org/2001/XMLSchema\"">""
           u""<GoalStateIncarnation>{0}</GoalStateIncarnation>""
           u""<Container>""
           u""<ContainerId>{1}</ContainerId>""
           u""<RoleInstanceList>""
           u""<Role>""
           u""<InstanceId>{2}</InstanceId>""
           u""<Health>""
           u""<State>{3}</State>""
           u""{4}""
           u""</Health>""
           u""</Role>""
           u""</RoleInstanceList>""
           u""</Container>""
           u""</Health>""
           u"""").format(incarnation,
                       container_id,
                       role_instance_id,
                       status,
                       detail)
    return xml


def ga_status_to_guest_info(ga_status):
    """"""
    Convert VMStatus object to status blob format
    """"""
    v1_ga_guest_info = {
        ""computerName"" : ga_status.hostname,
        ""osName"" : ga_status.osname,
        ""osVersion"" : ga_status.osversion,
        ""version"" : ga_status.version,
    }
    return v1_ga_guest_info


def ga_status_to_v1(ga_status):
    formatted_msg = {
        'lang': 'en-US',
        'message': ga_status.message
    }
    v1_ga_status = {
        ""version"" : ga_status.version,
        ""status"" : ga_status.status,
        ""formattedMessage"" : formatted_msg
    }
    return v1_ga_status


def ext_substatus_to_v1(sub_status_list):
    status_list = []
    for substatus in sub_status_list:
        status = {
            ""name"": substatus.name,
            ""status"": substatus.status,
            ""code"": substatus.code,
            ""formattedMessage"": {
                ""lang"": ""en-US"",
                ""message"": substatus.message
            }
        }
        status_list.append(status)
    return status_list


def ext_status_to_v1(ext_name, ext_status):
    if ext_status is None:
        return None
    timestamp = time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime())
    v1_sub_status = ext_substatus_to_v1(ext_status.substatusList)
    v1_ext_status = {
        ""status"": {
            ""name"": ext_name,
            ""configurationAppliedTime"": ext_status.configurationAppliedTime,
            ""operation"": ext_status.operation,
            ""status"": ext_status.status,
            ""code"": ext_status.code,
            ""formattedMessage"": {
                ""lang"": ""en-US"",
                ""message"": ext_status.message
            }
        },
        ""version"": 1.0,
        ""timestampUTC"": timestamp
    }
    if len(v1_sub_status) != 0:
        v1_ext_status['status']['substatus'] = v1_sub_status
    return v1_ext_status


def ext_handler_status_to_v1(handler_status, ext_statuses, timestamp):
    v1_handler_status = {
        'handlerVersion': handler_status.version,
        'handlerName': handler_status.name,
        'status': handler_status.status,
        'code': handler_status.code
    }
    if handler_status.message is not None:
        v1_handler_status[""formattedMessage""] = {
            ""lang"": ""en-US"",
            ""message"": handler_status.message
        }

    if handler_status.upgradeGuid is not None:
        v1_handler_status[""upgradeGuid""] = handler_status.upgradeGuid

    if len(handler_status.extensions) > 0:
        # Currently, no more than one extension per handler
        ext_name = handler_status.extensions[0]
        ext_status = ext_statuses.get(ext_name)
        v1_ext_status = ext_status_to_v1(ext_name, ext_status)
        if ext_status is not None and v1_ext_status is not None:
            v1_handler_status[""runtimeSettingsStatus""] = {
                'settingsStatus': v1_ext_status,
                'sequenceNumber': ext_status.sequenceNumber
            }
    return v1_handler_status


def vm_status_to_v1(vm_status, ext_statuses):
    timestamp = time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime())

    v1_ga_guest_info = ga_status_to_guest_info(vm_status.vmAgent)
    v1_ga_status = ga_status_to_v1(vm_status.vmAgent)
    v1_handler_status_list = []
    for handler_status in vm_status.vmAgent.extensionHandlers:
        v1_handler_status = ext_handler_status_to_v1(handler_status,
                                                     ext_statuses, timestamp)
        if v1_handler_status is not None:
            v1_handler_status_list.append(v1_handler_status)

    v1_agg_status = {
        'guestAgentStatus': v1_ga_status,
        'handlerAggregateStatus': v1_handler_status_list
    }
    v1_vm_status = {
        'version': '1.1',
        'timestampUTC': timestamp,
        'aggregateStatus': v1_agg_status,
        'guestOSInfo' : v1_ga_guest_info
    }
    return v1_vm_status


class StatusBlob(object):
    def __init__(self, client):
        self.vm_status = None
        self.ext_statuses = {}
        self.client = client
        self.type = None
        self.data = None

    def set_vm_status(self, vm_status):
        validate_param(""vmAgent"", vm_status, VMStatus)
        self.vm_status = vm_status

    def set_ext_status(self, ext_handler_name, ext_status):
        validate_param(""extensionStatus"", ext_status, ExtensionStatus)
        self.ext_statuses[ext_handler_name] = ext_status

    def to_json(self):
        report = vm_status_to_v1(self.vm_status, self.ext_statuses)
        return json.dumps(report)

    __storage_version__ = ""2014-02-14""

    def prepare(self, blob_type):
        logger.verbose(""Prepare status blob"")
        self.data = self.to_json()
        self.type = blob_type

    def upload(self, url):
        try:
            if not self.type in [""BlockBlob"", ""PageBlob""]:
                raise ProtocolError(""Illegal blob type: {0}"".format(self.type))

            if self.type == ""BlockBlob"":
                self.put_block_blob(url, self.data)
            else:
                self.put_page_blob(url, self.data)
            return True

        except Exception as e:
            logger.verbose(""Initial status upload failed: {0}"", e)

        return False

    def get_block_blob_headers(self, blob_size):
        return {
            ""Content-Length"": ustr(blob_size),
            ""x-ms-blob-type"": ""BlockBlob"",
            ""x-ms-date"": time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime()),
            ""x-ms-version"": self.__class__.__storage_version__
        }

    def put_block_blob(self, url, data):
        logger.verbose(""Put block blob"")
        headers = self.get_block_blob_headers(len(data))
        resp = self.client.call_storage_service(restutil.http_put, url, data, headers)
        if resp.status != httpclient.CREATED:
            raise UploadError(
                ""Failed to upload block blob: {0}"".format(resp.status))

    def get_page_blob_create_headers(self, blob_size):
        return {
            ""Content-Length"": ""0"",
            ""x-ms-blob-content-length"": ustr(blob_size),
            ""x-ms-blob-type"": ""PageBlob"",
            ""x-ms-date"": time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime()),
            ""x-ms-version"": self.__class__.__storage_version__
        }

    def get_page_blob_page_headers(self, start, end):
        return {
            ""Content-Length"": ustr(end - start),
            ""x-ms-date"": time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime()),
            ""x-ms-range"": ""bytes={0}-{1}"".format(start, end - 1),
            ""x-ms-page-write"": ""update"",
            ""x-ms-version"": self.__class__.__storage_version__
        }

    def put_page_blob(self, url, data):
        logger.verbose(""Put page blob"")

        # Convert string into bytes and align to 512 bytes
        data = bytearray(data, encoding='utf-8')
        page_blob_size = int((len(data) + 511) / 512) * 512

        headers = self.get_page_blob_create_headers(page_blob_size)
        resp = self.client.call_storage_service(restutil.http_put, url, """", headers)
        if resp.status != httpclient.CREATED:
            raise UploadError(
                ""Failed to clean up page blob: {0}"".format(resp.status))

        if url.count(""?"") <= 0:
            url = ""{0}?comp=page"".format(url)
        else:
            url = ""{0}&comp=page"".format(url)

        logger.verbose(""Upload page blob"")
        page_max = 4 * 1024 * 1024  # Max page size: 4MB
        start = 0
        end = 0
        while end < len(data):
            end = min(len(data), start + page_max)
            content_size = end - start
            # Align to 512 bytes
            page_end = int((end + 511) / 512) * 512
            buf_size = page_end - start
            buf = bytearray(buf_size)
            buf[0: content_size] = data[start: end]
            headers = self.get_page_blob_page_headers(start, page_end)
            resp = self.client.call_storage_service(
                restutil.http_put,
                url,
                bytebuffer(buf),
                headers)
            if resp is None or resp.status != httpclient.CREATED:
                raise UploadError(
                    ""Failed to upload page blob: {0}"".format(resp.status))
            start = end


def event_param_to_v1(param):
    param_format = '<Param Name=""{0}"" Value={1} T=""{2}"" />'
    param_type = type(param.value)
    attr_type = """"
    if param_type is int:
        attr_type = 'mt:uint64'
    elif param_type is str:
        attr_type = 'mt:wstr'
    elif ustr(param_type).count(""'unicode'"") > 0:
        attr_type = 'mt:wstr'
    elif param_type is bool:
        attr_type = 'mt:bool'
    elif param_type is float:
        attr_type = 'mt:float64'
    return param_format.format(param.name,
                               saxutils.quoteattr(ustr(param.value)),
                               attr_type)


def event_to_v1(event):
    params = """"
    for param in event.parameters:
        params += event_param_to_v1(param)
    event_str = ('<Event id=""{0}"">'
                 '<![CDATA[{1}]]>'
                 '</Event>').format(event.eventId, params)
    return event_str


class WireClient(object):
    def __init__(self, endpoint):
        logger.info(""Wire server endpoint:{0}"", endpoint)
        self.endpoint = endpoint
        self.goal_state = None
        self.updated = None
        self.hosting_env = None
        self.shared_conf = None
        self.remote_access = None
        self.certs = None
        self.ext_conf = None
        self.host_plugin = None
        self.status_blob = StatusBlob(self)
        self.goal_state_flusher = StateFlusher(conf.get_lib_dir())

    def call_wireserver(self, http_req, *args, **kwargs):
        try:
            # Never use the HTTP proxy for wireserver
            kwargs['use_proxy'] = False
            resp = http_req(*args, **kwargs)

            if restutil.request_failed(resp):
                msg = ""[Wireserver Failed] URI {0} "".format(args[0])
                if resp is not None:
                    msg += "" [HTTP Failed] Status Code {0}"".format(resp.status)
                raise ProtocolError(msg)

        # If the GoalState is stale, pass along the exception to the caller
        except ResourceGoneError:
            raise

        except Exception as e:
            raise ProtocolError(""[Wireserver Exception] {0}"".format(
                ustr(e)))

        return resp

    def decode_config(self, data):
        if data is None:
            return None
        data = remove_bom(data)
        xml_text = ustr(data, encoding='utf-8')
        return xml_text

    def fetch_config(self, uri, headers):
        resp = self.call_wireserver(restutil.http_get,
                                    uri,
                                    headers=headers)
        return self.decode_config(resp.read())

    def fetch_cache(self, local_file):
        if not os.path.isfile(local_file):
            raise ProtocolError(""{0} is missing."".format(local_file))
        try:
            return fileutil.read_file(local_file)
        except IOError as e:
            raise ProtocolError(""Failed to read cache: {0}"".format(e))

    def save_cache(self, local_file, data):
        try:
            fileutil.write_file(local_file, data)
        except IOError as e:
            fileutil.clean_ioerror(e,
                paths=[local_file])
            raise ProtocolError(""Failed to write cache: {0}"".format(e))

    @staticmethod
    def call_storage_service(http_req, *args, **kwargs):
        # Default to use the configured HTTP proxy
        if not 'use_proxy' in kwargs or kwargs['use_proxy'] is None:
            kwargs['use_proxy'] = True

        return http_req(*args, **kwargs)

    def fetch_manifest(self, version_uris):
        logger.verbose(""Fetch manifest"")
        version_uris_shuffled = version_uris
        random.shuffle(version_uris_shuffled)

        for version in version_uris_shuffled:
            # GA expects a location and failoverLocation in ExtensionsConfig, but
            # this is not always the case. See #1147.
            if version.uri is None:
                logger.verbose('The specified manifest URL is empty, ignored.')
                continue

            response = None
            if not HostPluginProtocol.is_default_channel():
                response = self.fetch(version.uri)

            if not response:
                if HostPluginProtocol.is_default_channel():
                    logger.verbose(""Using host plugin as default channel"")
                else:
                    logger.verbose(""Failed to download manifest, ""
                                   ""switching to host plugin"")

                try:
                    host = self.get_host_plugin()
                    uri, headers = host.get_artifact_request(version.uri)
                    response = self.fetch(uri, headers, use_proxy=False)

                # If the HostPlugin rejects the request,
                # let the error continue, but set to use the HostPlugin
                except ResourceGoneError:
                    HostPluginProtocol.set_default_channel(True)
                    raise

                host.manifest_uri = version.uri
                logger.verbose(""Manifest downloaded successfully from host plugin"")
                if not HostPluginProtocol.is_default_channel():
                    logger.info(""Setting host plugin as default channel"")
                    HostPluginProtocol.set_default_channel(True)

            if response:
                return response

        raise ProtocolError(""Failed to fetch manifest from all sources"")

    def fetch(self, uri, headers=None, use_proxy=None, decode=True):
        content = None
        logger.verbose(""Fetch [{0}] with headers [{1}]"", uri, headers)
        try:
            resp = self.call_storage_service(
                        restutil.http_get,
                        uri,
                        headers=headers,
                        use_proxy=use_proxy)

            if restutil.request_failed(resp):
                error_response = restutil.read_response_error(resp)
                msg = ""Fetch failed from [{0}]: {1}"".format(uri, error_response)
                logger.warn(msg)
                if self.host_plugin is not None:
                    self.host_plugin.report_fetch_health(uri,
                                                         is_healthy=not restutil.request_failed_at_hostplugin(resp),
                                                         source='WireClient',
                                                         response=error_response)
                raise ProtocolError(msg)
            else:
                response_content = resp.read()
                content = self.decode_config(response_content) if decode else response_content
                if self.host_plugin is not None:
                    self.host_plugin.report_fetch_health(uri, source='WireClient')

        except (HttpError, ProtocolError, IOError) as e:
            logger.verbose(""Fetch failed from [{0}]: {1}"", uri, e)
            if isinstance(e, ResourceGoneError):
                raise

        return content

    def update_hosting_env(self, goal_state):
        if goal_state.hosting_env_uri is None:
            raise ProtocolError(""HostingEnvironmentConfig uri is empty"")
        local_file = os.path.join(conf.get_lib_dir(), HOSTING_ENV_FILE_NAME)
        xml_text = self.fetch_config(goal_state.hosting_env_uri,
                                     self.get_header())
        self.save_cache(local_file, xml_text)
        self.hosting_env = HostingEnv(xml_text)

    def update_shared_conf(self, goal_state):
        if goal_state.shared_conf_uri is None:
            raise ProtocolError(""SharedConfig uri is empty"")
        local_file = os.path.join(conf.get_lib_dir(), SHARED_CONF_FILE_NAME)
        xml_text = self.fetch_config(goal_state.shared_conf_uri,
                                     self.get_header())
        self.save_cache(local_file, xml_text)
        self.shared_conf = SharedConfig(xml_text)

    def update_certs(self, goal_state):
        if goal_state.certs_uri is None:
            return
        local_file = os.path.join(conf.get_lib_dir(), CERTS_FILE_NAME)
        xml_text = self.fetch_config(goal_state.certs_uri,
                                     self.get_header_for_cert())
        self.save_cache(local_file, xml_text)
        self.certs = Certificates(self, xml_text)

    def update_remote_access_conf(self, goal_state):
        if goal_state.remote_access_uri is None:
            # Nothing in accounts data.  Just return, nothing to do.
            return
        xml_text = self.fetch_config(goal_state.remote_access_uri, 
                                     self.get_header_for_cert())
        self.remote_access = RemoteAccess(xml_text)
        local_file = os.path.join(conf.get_lib_dir(), REMOTE_ACCESS_FILE_NAME.format(self.remote_access.incarnation))
        self.save_cache(local_file, xml_text)

    def get_remote_access(self):
        incarnation_file = os.path.join(conf.get_lib_dir(),
                                        INCARNATION_FILE_NAME)
        incarnation = self.fetch_cache(incarnation_file)
        file_name = REMOTE_ACCESS_FILE_NAME.format(incarnation)
        remote_access_file = os.path.join(conf.get_lib_dir(), file_name)
        if not os.path.isfile(remote_access_file):
            # no remote access data.
            return None
        xml_text = self.fetch_cache(remote_access_file)
        remote_access = RemoteAccess(xml_text)
        return remote_access
        
    def update_ext_conf(self, goal_state):
        if goal_state.ext_uri is None:
            logger.info(""ExtensionsConfig.xml uri is empty"")
            self.ext_conf = ExtensionsConfig(None)
            return
        incarnation = goal_state.incarnation
        local_file = os.path.join(conf.get_lib_dir(),
                                  EXT_CONF_FILE_NAME.format(incarnation))
        xml_text = self.fetch_config(goal_state.ext_uri, self.get_header())
        self.save_cache(local_file, xml_text)
        self.ext_conf = ExtensionsConfig(xml_text)

    def update_goal_state(self, forced=False, max_retry=3):
        incarnation_file = os.path.join(conf.get_lib_dir(),
                                        INCARNATION_FILE_NAME)
        uri = GOAL_STATE_URI.format(self.endpoint)

        goal_state = None
        for retry in range(0, max_retry):
            try:
                if goal_state is None:
                    xml_text = self.fetch_config(uri, self.get_header())
                    goal_state = GoalState(xml_text)

                    if not forced:
                        last_incarnation = None
                        if os.path.isfile(incarnation_file):
                            last_incarnation = fileutil.read_file(
                                                    incarnation_file)
                        new_incarnation = goal_state.incarnation
                        if last_incarnation is not None and \
                                        last_incarnation == new_incarnation:
                            # Goalstate is not updated.
                            return                
                self.goal_state_flusher.flush(datetime.utcnow())

                self.goal_state = goal_state
                file_name = GOAL_STATE_FILE_NAME.format(goal_state.incarnation)
                goal_state_file = os.path.join(conf.get_lib_dir(), file_name)
                self.save_cache(goal_state_file, xml_text)
                self.update_hosting_env(goal_state)
                self.update_shared_conf(goal_state)
                self.update_certs(goal_state)
                self.update_ext_conf(goal_state)
                self.update_remote_access_conf(goal_state)
                self.save_cache(incarnation_file, goal_state.incarnation)

                if self.host_plugin is not None:
                    self.host_plugin.container_id = goal_state.container_id
                    self.host_plugin.role_config_name = goal_state.role_config_name

                return

            except ResourceGoneError:
                logger.info(""GoalState is stale -- re-fetching"")
                goal_state = None

            except Exception as e:
                log_method = logger.info \
                                if type(e) is ProtocolError \
                                else logger.warn
                log_method(
                    ""Exception processing GoalState-related files: {0}"".format(
                        ustr(e)))

                if retry < max_retry-1:
                    continue
                raise

        raise ProtocolError(""Exceeded max retry updating goal state"")

    def get_goal_state(self):
        if self.goal_state is None:
            incarnation_file = os.path.join(conf.get_lib_dir(),
                                            INCARNATION_FILE_NAME)
            incarnation = self.fetch_cache(incarnation_file)

            file_name = GOAL_STATE_FILE_NAME.format(incarnation)
            goal_state_file = os.path.join(conf.get_lib_dir(), file_name)
            xml_text = self.fetch_cache(goal_state_file)
            self.goal_state = GoalState(xml_text)
        return self.goal_state

    def get_hosting_env(self):
        if self.hosting_env is None:
            local_file = os.path.join(conf.get_lib_dir(),
                                      HOSTING_ENV_FILE_NAME)
            xml_text = self.fetch_cache(local_file)
            self.hosting_env = HostingEnv(xml_text)
        return self.hosting_env

    def get_shared_conf(self):
        if self.shared_conf is None:
            local_file = os.path.join(conf.get_lib_dir(),
                                      SHARED_CONF_FILE_NAME)
            xml_text = self.fetch_cache(local_file)
            self.shared_conf = SharedConfig(xml_text)
        return self.shared_conf

    def get_certs(self):
        if self.certs is None:
            local_file = os.path.join(conf.get_lib_dir(), CERTS_FILE_NAME)
            xml_text = self.fetch_cache(local_file)
            self.certs = Certificates(self, xml_text)
        if self.certs is None:
            return None
        return self.certs

    def get_ext_conf(self):
        if self.ext_conf is None:
            goal_state = self.get_goal_state()
            if goal_state.ext_uri is None:
                self.ext_conf = ExtensionsConfig(None)
            else:
                local_file = EXT_CONF_FILE_NAME.format(goal_state.incarnation)
                local_file = os.path.join(conf.get_lib_dir(), local_file)
                xml_text = self.fetch_cache(local_file)
                self.ext_conf = ExtensionsConfig(xml_text)
        return self.ext_conf      

    def get_ext_manifest(self, ext_handler, goal_state):
        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    self.update_goal_state(forced=True)
                    goal_state = self.get_goal_state()

                local_file = MANIFEST_FILE_NAME.format(
                                ext_handler.name,
                                goal_state.incarnation)
                local_file = os.path.join(conf.get_lib_dir(), local_file)
                xml_text = self.fetch_manifest(ext_handler.versionUris)
                self.save_cache(local_file, xml_text)
                return ExtensionManifest(xml_text)

            except ResourceGoneError:
                continue

        raise ProtocolError(""Failed to retrieve extension manifest"")

    def filter_package_list(self, family, ga_manifest, goal_state):
        complete_list = ga_manifest.pkg_list
        agent_manifest = os.path.join(conf.get_lib_dir(),
                                      AGENTS_MANIFEST_FILE_NAME.format(
                                          family,
                                          goal_state.incarnation))

        if not os.path.exists(agent_manifest):
            # clear memory cache
            ga_manifest.allowed_versions = None

            # create disk cache
            with open(agent_manifest, mode='w') as manifest_fh:
                for version in complete_list.versions:
                    manifest_fh.write('{0}\n'.format(version.version))
            fileutil.chmod(agent_manifest, 0o644)

            return complete_list

        else:
            # use allowed versions from cache, otherwise from disk
            if ga_manifest.allowed_versions is None:
                with open(agent_manifest, mode='r') as manifest_fh:
                    ga_manifest.allowed_versions = [v.strip('\n') for v
                                                    in manifest_fh.readlines()]

            # use the updated manifest urls for allowed versions
            allowed_list = ExtHandlerPackageList()
            allowed_list.versions = [version for version
                                     in complete_list.versions
                                     if version.version
                                     in ga_manifest.allowed_versions]

            return allowed_list

    def get_gafamily_manifest(self, vmagent_manifest, goal_state):
        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    self.update_goal_state(forced=True)
                    goal_state = self.get_goal_state()

                self._remove_stale_agent_manifest(
                    vmagent_manifest.family,
                    goal_state.incarnation)

                local_file = MANIFEST_FILE_NAME.format(
                                vmagent_manifest.family,
                                goal_state.incarnation)
                local_file = os.path.join(conf.get_lib_dir(), local_file)
                xml_text = self.fetch_manifest(
                            vmagent_manifest.versionsManifestUris)
                fileutil.write_file(local_file, xml_text)
                return ExtensionManifest(xml_text)

            except ResourceGoneError:
                continue

        raise ProtocolError(""Failed to retrieve GAFamily manifest"")

    def _remove_stale_agent_manifest(self, family, incarnation):
        """"""
        The incarnation number can reset at any time, which means there
        could be a stale agentsManifest on disk.  Stale files are cleaned
        on demand as new goal states arrive from WireServer. If the stale
        file is not removed agent upgrade may be delayed.

        :param family: GA family, e.g. Prod or Test
        :param incarnation: incarnation of the current goal state
        """"""
        fn = AGENTS_MANIFEST_FILE_NAME.format(
            family,
            incarnation)

        agent_manifest = os.path.join(conf.get_lib_dir(), fn)

        if os.path.exists(agent_manifest):
            os.unlink(agent_manifest)

    def check_wire_protocol_version(self):
        uri = VERSION_INFO_URI.format(self.endpoint)
        version_info_xml = self.fetch_config(uri, None)
        version_info = VersionInfo(version_info_xml)

        preferred = version_info.get_preferred()
        if PROTOCOL_VERSION == preferred:
            logger.info(""Wire protocol version:{0}"", PROTOCOL_VERSION)
        elif PROTOCOL_VERSION in version_info.get_supported():
            logger.info(""Wire protocol version:{0}"", PROTOCOL_VERSION)
            logger.info(""Server preferred version:{0}"", preferred)
        else:
            error = (""Agent supported wire protocol version: {0} was not ""
                     ""advised by Fabric."").format(PROTOCOL_VERSION)
            raise ProtocolNotFoundError(error)

    def upload_status_blob(self):
        try:
            self.update_goal_state()
            ext_conf = self.get_ext_conf()

            blob_uri = ext_conf.status_upload_blob
            blob_type = ext_conf.status_upload_blob_type

            if blob_uri is None:
                raise ProtocolError(""No blob uri found"")

            if blob_type not in [""BlockBlob"", ""PageBlob""]:
                blob_type = ""BlockBlob""
                logger.verbose(""Status Blob type is unspecified, assuming BlockBlob"")

            try:
                self.status_blob.prepare(blob_type)
            except Exception as e:
                raise ProtocolError(""Exception creating status blob: {0}"", ustr(e))

            # Swap the order of use for the HostPlugin vs. the ""direct"" route.
            # Prefer the use of HostPlugin. If HostPlugin fails fall back to the
            # direct route.
            #
            # The code previously preferred the ""direct"" route always, and only fell back
            # to the HostPlugin *if* there was an error.  We would like to move to
            # the HostPlugin for all traffic, but this is a big change.  We would like
            # to see how this behaves at scale, and have a fallback should things go
            # wrong.  This is why we try HostPlugin then direct.
            try:
                host = self.get_host_plugin()
                host.put_vm_status(self.status_blob,
                                   ext_conf.status_upload_blob,
                                   ext_conf.status_upload_blob_type)
                return
            except ResourceGoneError:
                # do not attempt direct, force goal state update and wait to try again
                self.update_goal_state(forced=True)
                return
            except Exception as e:
                # for all other errors, fall back to direct
                pass

            self.report_status_event(""direct"")
            if self.status_blob.upload(blob_uri):
                return

        except Exception as e:
            self.report_status_event(""Exception uploading status blob: {0}"", ustr(e))

        raise ProtocolError(""Failed to upload status blob via either channel"")

    def report_role_prop(self, thumbprint):
        goal_state = self.get_goal_state()
        role_prop = _build_role_properties(goal_state.container_id,
                                           goal_state.role_instance_id,
                                           thumbprint)
        role_prop = role_prop.encode(""utf-8"")
        role_prop_uri = ROLE_PROP_URI.format(self.endpoint)
        headers = self.get_header_for_xml_content()
        try:
            resp = self.call_wireserver(restutil.http_post,
                                        role_prop_uri,
                                        role_prop,
                                        headers=headers)
        except HttpError as e:
            raise ProtocolError((u""Failed to send role properties: ""
                                 u""{0}"").format(e))
        if resp.status != httpclient.ACCEPTED:
            raise ProtocolError((u""Failed to send role properties: ""
                                 u"",{0}: {1}"").format(resp.status,
                                                      resp.read()))

    def report_health(self, status, substatus, description):
        goal_state = self.get_goal_state()
        health_report = _build_health_report(goal_state.incarnation,
                                             goal_state.container_id,
                                             goal_state.role_instance_id,
                                             status,
                                             substatus,
                                             description)
        health_report = health_report.encode(""utf-8"")
        health_report_uri = HEALTH_REPORT_URI.format(self.endpoint)
        headers = self.get_header_for_xml_content()
        try:
            # 30 retries with 10s sleep gives ~5min for wireserver updates;
            # this is retried 3 times with 15s sleep before throwing a
            # ProtocolError, for a total of ~15min.
            resp = self.call_wireserver(restutil.http_post,
                                        health_report_uri,
                                        health_report,
                                        headers=headers,
                                        max_retry=30,
                                        retry_delay=15)
        except HttpError as e:
            raise ProtocolError((u""Failed to send provision status: ""
                                 u""{0}"").format(e))
        if restutil.request_failed(resp):
            raise ProtocolError((u""Failed to send provision status: ""
                                 u"",{0}: {1}"").format(resp.status,
                                                      resp.read()))

    def send_event(self, provider_id, event_str):
        uri = TELEMETRY_URI.format(self.endpoint)
        data_format = ('<?xml version=""1.0""?>'
                       '<TelemetryData version=""1.0"">'
                       '<Provider id=""{0}"">{1}'
                       '</Provider>'
                       '</TelemetryData>')
        data = data_format.format(provider_id, event_str)
        try:
            header = self.get_header_for_xml_content()
            resp = self.call_wireserver(restutil.http_post, uri, data, header)
        except HttpError as e:
            raise ProtocolError(""Failed to send events:{0}"".format(e))

        if restutil.request_failed(resp):
            logger.verbose(resp.read())
            raise ProtocolError(
                ""Failed to send events:{0}"".format(resp.status))

    def report_event(self, event_list):
        buf = {}
        # Group events by providerId
        for event in event_list.events:
            if event.providerId not in buf:
                buf[event.providerId] = """"
            event_str = event_to_v1(event)
            if len(event_str) >= 63 * 1024:
                logger.warn(""Single event too large: {0}"", event_str[300:])
                continue
            if len(buf[event.providerId] + event_str) >= 63 * 1024:
                self.send_event(event.providerId, buf[event.providerId])
                buf[event.providerId] = """"
            buf[event.providerId] = buf[event.providerId] + event_str

        # Send out all events left in buffer.
        for provider_id in list(buf.keys()):
            if len(buf[provider_id]) > 0:
                self.send_event(provider_id, buf[provider_id])

    def report_status_event(self, message, *args):
        from azurelinuxagent.common.event import report_event, \
                WALAEventOperation

        message = message.format(*args)
        logger.warn(message)
        report_event(op=WALAEventOperation.ReportStatus,
                    is_success=False,
                    message=message)

    def get_header(self):
        return {
            ""x-ms-agent-name"": ""WALinuxAgent"",
            ""x-ms-version"": PROTOCOL_VERSION
        }

    def get_header_for_xml_content(self):
        return {
            ""x-ms-agent-name"": ""WALinuxAgent"",
            ""x-ms-version"": PROTOCOL_VERSION,
            ""Content-Type"": ""text/xml;charset=utf-8""
        }

    def get_header_for_cert(self):
        trans_cert_file = os.path.join(conf.get_lib_dir(),
                                       TRANSPORT_CERT_FILE_NAME)
        content = self.fetch_cache(trans_cert_file)
        cert = get_bytes_from_pem(content)
        return {
            ""x-ms-agent-name"": ""WALinuxAgent"",
            ""x-ms-version"": PROTOCOL_VERSION,
            ""x-ms-cipher-name"": ""DES_EDE3_CBC"",
            ""x-ms-guest-agent-public-x509-cert"": cert
        }

    def get_host_plugin(self):
        if self.host_plugin is None:
            goal_state = self.get_goal_state()
            self.host_plugin = HostPluginProtocol(self.endpoint,
                                                  goal_state.container_id,
                                                  goal_state.role_config_name)
        return self.host_plugin

    def has_artifacts_profile_blob(self):
        return self.ext_conf and not \
               textutil.is_str_none_or_whitespace(self.ext_conf.artifacts_profile_blob)

    def get_artifacts_profile(self):
        artifacts_profile = None
        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    self.update_goal_state(forced=True)

                if self.has_artifacts_profile_blob():
                    blob = self.ext_conf.artifacts_profile_blob

                    profile = None
                    if not HostPluginProtocol.is_default_channel():
                        logger.verbose(""Retrieving the artifacts profile"")
                        profile = self.fetch(blob)

                    if profile is None:
                        if HostPluginProtocol.is_default_channel():
                            logger.verbose(""Using host plugin as default channel"")
                        else:
                            logger.verbose(""Failed to download artifacts profile, ""
                                           ""switching to host plugin"")

                        host = self.get_host_plugin()
                        uri, headers = host.get_artifact_request(blob)
                        profile = self.fetch(uri, headers, use_proxy=False)

                    if not textutil.is_str_none_or_whitespace(profile):
                        logger.verbose(""Artifacts profile downloaded"")
                        artifacts_profile = InVMArtifactsProfile(profile)

                return artifacts_profile

            except ResourceGoneError:
                HostPluginProtocol.set_default_channel(True)
                continue

            except Exception as e:
                logger.warn(
                    ""Exception retrieving artifacts profile: {0}"".format(
                        ustr(e)))

        return None


class VersionInfo(object):
    def __init__(self, xml_text):
        """"""
        Query endpoint server for wire protocol version.
        Fail if our desired protocol version is not seen.
        """"""
        logger.verbose(""Load Version.xml"")
        self.parse(xml_text)

    def parse(self, xml_text):
        xml_doc = parse_doc(xml_text)
        preferred = find(xml_doc, ""Preferred"")
        self.preferred = findtext(preferred, ""Version"")
        logger.info(""Fabric preferred wire protocol version:{0}"",
                    self.preferred)

        self.supported = []
        supported = find(xml_doc, ""Supported"")
        supported_version = findall(supported, ""Version"")
        for node in supported_version:
            version = gettext(node)
            logger.verbose(""Fabric supported wire protocol version:{0}"",
                           version)
            self.supported.append(version)

    def get_preferred(self):
        return self.preferred

    def get_supported(self):
        return self.supported


class GoalState(object):
    def __init__(self, xml_text):
        if xml_text is None:
            raise ValueError(""GoalState.xml is None"")
        logger.verbose(""Load GoalState.xml"")
        self.incarnation = None
        self.expected_state = None
        self.hosting_env_uri = None
        self.shared_conf_uri = None
        self.remote_access_uri = None
        self.certs_uri = None
        self.ext_uri = None
        self.role_instance_id = None
        self.role_config_name = None
        self.container_id = None
        self.load_balancer_probe_port = None
        self.xml_text = None
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Request configuration data from endpoint server.
        """"""
        self.xml_text = xml_text
        xml_doc = parse_doc(xml_text)
        self.incarnation = findtext(xml_doc, ""Incarnation"")
        self.expected_state = findtext(xml_doc, ""ExpectedState"")
        self.hosting_env_uri = findtext(xml_doc, ""HostingEnvironmentConfig"")
        self.shared_conf_uri = findtext(xml_doc, ""SharedConfig"")
        self.certs_uri = findtext(xml_doc, ""Certificates"")
        self.ext_uri = findtext(xml_doc, ""ExtensionsConfig"")
        role_instance = find(xml_doc, ""RoleInstance"")
        self.role_instance_id = findtext(role_instance, ""InstanceId"")
        role_config = find(role_instance, ""Configuration"")
        self.role_config_name = findtext(role_config, ""ConfigName"")
        container = find(xml_doc, ""Container"")
        self.container_id = findtext(container, ""ContainerId"")
        self.remote_access_uri = findtext(container, ""RemoteAccessInfo"")
        lbprobe_ports = find(xml_doc, ""LBProbePorts"")
        self.load_balancer_probe_port = findtext(lbprobe_ports, ""Port"")
        return self


class HostingEnv(object):
    """"""
    parse Hosting enviromnet config and store in
    HostingEnvironmentConfig.xml
    """"""

    def __init__(self, xml_text):
        if xml_text is None:
            raise ValueError(""HostingEnvironmentConfig.xml is None"")
        logger.verbose(""Load HostingEnvironmentConfig.xml"")
        self.vm_name = None
        self.role_name = None
        self.deployment_name = None
        self.xml_text = None
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        parse and create HostingEnvironmentConfig.xml.
        """"""
        self.xml_text = xml_text
        xml_doc = parse_doc(xml_text)
        incarnation = find(xml_doc, ""Incarnation"")
        self.vm_name = getattrib(incarnation, ""instance"")
        role = find(xml_doc, ""Role"")
        self.role_name = getattrib(role, ""name"")
        deployment = find(xml_doc, ""Deployment"")
        self.deployment_name = getattrib(deployment, ""name"")
        return self


class SharedConfig(object):
    """"""
    parse role endpoint server and goal state config.
    """"""

    def __init__(self, xml_text):
        logger.verbose(""Load SharedConfig.xml"")
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        parse and write configuration to file SharedConfig.xml.
        """"""
        # Not used currently
        return self


class RemoteAccess(object):
    """"""
    Object containing information about user accounts
    """"""
    #
    # <RemoteAccess>
    #   <Version/>
    #   <Incarnation/>
    #    <Users>
    #       <User>
    #         <Name/>
    #         <Password/>
    #         <Expiration/>
    #       </User>
    #     </Users>
    #   </RemoteAccess>
    #

    def __init__(self, xml_text):
        logger.verbose(""Load RemoteAccess.xml"")
        self.version = None
        self.incarnation = None
        self.user_list = RemoteAccessUsersList()

        self.xml_text = None
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Parse xml document containing user account information
        """"""
        if xml_text is None or len(xml_text) == 0:
            return None
        self.xml_text = xml_text
        xml_doc = parse_doc(xml_text)
        self.incarnation = findtext(xml_doc, ""Incarnation"")
        self.version = findtext(xml_doc, ""Version"")
        user_collection = find(xml_doc, ""Users"")
        users = findall(user_collection, ""User"")

        for user in users:
            remote_access_user = self.parse_user(user)
            self.user_list.users.append(remote_access_user)
        return self

    def parse_user(self, user):
        name = findtext(user, ""Name"")
        encrypted_password = findtext(user, ""Password"")
        expiration = findtext(user, ""Expiration"")
        remote_access_user = RemoteAccessUser(name, encrypted_password, expiration)
        return remote_access_user

class UserAccount(object):
    """"""
    Stores information about single user account
    """"""
    def __init__(self):
        self.Name = None
        self.EncryptedPassword = None
        self.Password = None
        self.Expiration = None
        self.Groups = []


class Certificates(object):
    """"""
    Object containing certificates of host and provisioned user.
    """"""

    def __init__(self, client, xml_text):
        logger.verbose(""Load Certificates.xml"")
        self.client = client
        self.cert_list = CertList()
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Parse multiple certificates into seperate files.
        """"""
        xml_doc = parse_doc(xml_text)
        data = findtext(xml_doc, ""Data"")
        if data is None:
            return

        cryptutil = CryptUtil(conf.get_openssl_cmd())
        p7m_file = os.path.join(conf.get_lib_dir(), P7M_FILE_NAME)
        p7m = (""MIME-Version:1.0\n""
               ""Content-Disposition: attachment; filename=\""{0}\""\n""
               ""Content-Type: application/x-pkcs7-mime; name=\""{1}\""\n""
               ""Content-Transfer-Encoding: base64\n""
               ""\n""
               ""{2}"").format(p7m_file, p7m_file, data)

        self.client.save_cache(p7m_file, p7m)

        trans_prv_file = os.path.join(conf.get_lib_dir(),
                                      TRANSPORT_PRV_FILE_NAME)
        trans_cert_file = os.path.join(conf.get_lib_dir(),
                                       TRANSPORT_CERT_FILE_NAME)
        pem_file = os.path.join(conf.get_lib_dir(), PEM_FILE_NAME)
        # decrypt certificates
        cryptutil.decrypt_p7m(p7m_file, trans_prv_file, trans_cert_file,
                              pem_file)

        # The parsing process use public key to match prv and crt.
        buf = []
        begin_crt = False
        begin_prv = False
        prvs = {}
        thumbprints = {}
        index = 0
        v1_cert_list = []
        with open(pem_file) as pem:
            for line in pem.readlines():
                buf.append(line)
                if re.match(r'[-]+BEGIN.*KEY[-]+', line):
                    begin_prv = True
                elif re.match(r'[-]+BEGIN.*CERTIFICATE[-]+', line):
                    begin_crt = True
                elif re.match(r'[-]+END.*KEY[-]+', line):
                    tmp_file = self.write_to_tmp_file(index, 'prv', buf)
                    pub = cryptutil.get_pubkey_from_prv(tmp_file)
                    prvs[pub] = tmp_file
                    buf = []
                    index += 1
                    begin_prv = False
                elif re.match(r'[-]+END.*CERTIFICATE[-]+', line):
                    tmp_file = self.write_to_tmp_file(index, 'crt', buf)
                    pub = cryptutil.get_pubkey_from_crt(tmp_file)
                    thumbprint = cryptutil.get_thumbprint_from_crt(tmp_file)
                    thumbprints[pub] = thumbprint
                    # Rename crt with thumbprint as the file name
                    crt = ""{0}.crt"".format(thumbprint)
                    v1_cert_list.append({
                        ""name"": None,
                        ""thumbprint"": thumbprint
                    })
                    os.rename(tmp_file, os.path.join(conf.get_lib_dir(), crt))
                    buf = []
                    index += 1
                    begin_crt = False

        # Rename prv key with thumbprint as the file name
        for pubkey in prvs:
            thumbprint = thumbprints[pubkey]
            if thumbprint:
                tmp_file = prvs[pubkey]
                prv = ""{0}.prv"".format(thumbprint)
                os.rename(tmp_file, os.path.join(conf.get_lib_dir(), prv))

        for v1_cert in v1_cert_list:
            cert = Cert()
            set_properties(""certs"", cert, v1_cert)
            self.cert_list.certificates.append(cert)

    def write_to_tmp_file(self, index, suffix, buf):
        file_name = os.path.join(conf.get_lib_dir(),
                                 ""{0}.{1}"".format(index, suffix))
        self.client.save_cache(file_name, """".join(buf))
        return file_name


class ExtensionsConfig(object):
    """"""
    parse ExtensionsConfig, downloading and unpacking them to /var/lib/waagent.
    Install if <enabled>true</enabled>, remove if it is set to false.
    """"""

    def __init__(self, xml_text):
        logger.verbose(""Load ExtensionsConfig.xml"")
        self.ext_handlers = ExtHandlerList()
        self.vmagent_manifests = VMAgentManifestList()
        self.status_upload_blob = None
        self.status_upload_blob_type = None
        self.artifacts_profile_blob = None
        if xml_text is not None:
            self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Write configuration to file ExtensionsConfig.xml.
        """"""
        xml_doc = parse_doc(xml_text)

        ga_families_list = find(xml_doc, ""GAFamilies"")
        ga_families = findall(ga_families_list, ""GAFamily"")

        for ga_family in ga_families:
            family = findtext(ga_family, ""Name"")
            uris_list = find(ga_family, ""Uris"")
            uris = findall(uris_list, ""Uri"")
            manifest = VMAgentManifest()
            manifest.family = family
            for uri in uris:
                manifestUri = VMAgentManifestUri(uri=gettext(uri))
                manifest.versionsManifestUris.append(manifestUri)
            self.vmagent_manifests.vmAgentManifests.append(manifest)

        plugins_list = find(xml_doc, ""Plugins"")
        plugins = findall(plugins_list, ""Plugin"")
        plugin_settings_list = find(xml_doc, ""PluginSettings"")
        plugin_settings = findall(plugin_settings_list, ""Plugin"")

        for plugin in plugins:
            ext_handler = self.parse_plugin(plugin)
            self.ext_handlers.extHandlers.append(ext_handler)
            self.parse_plugin_settings(ext_handler, plugin_settings)

        self.status_upload_blob = findtext(xml_doc, ""StatusUploadBlob"")
        self.artifacts_profile_blob = findtext(xml_doc, ""InVMArtifactsProfileBlob"")

        status_upload_node = find(xml_doc, ""StatusUploadBlob"")
        self.status_upload_blob_type = getattrib(status_upload_node,
                                                 ""statusBlobType"")
        logger.verbose(""Extension config shows status blob type as [{0}]"",
                       self.status_upload_blob_type)

    def parse_plugin(self, plugin):
        ext_handler = ExtHandler()
        ext_handler.name = getattrib(plugin, ""name"")
        ext_handler.properties.version = getattrib(plugin, ""version"")
        ext_handler.properties.state = getattrib(plugin, ""state"")

        ext_handler.properties.upgradeGuid = getattrib(plugin, ""upgradeGuid"")
        if not ext_handler.properties.upgradeGuid:
            ext_handler.properties.upgradeGuid = None

        try:
            ext_handler.properties.dependencyLevel = int(getattrib(plugin, ""dependencyLevel""))
        except ValueError:
            ext_handler.properties.dependencyLevel = 0

        auto_upgrade = getattrib(plugin, ""autoUpgrade"")
        if auto_upgrade is not None and auto_upgrade.lower() == ""true"":
            ext_handler.properties.upgradePolicy = ""auto""
        else:
            ext_handler.properties.upgradePolicy = ""manual""

        location = getattrib(plugin, ""location"")
        failover_location = getattrib(plugin, ""failoverlocation"")
        for uri in [location, failover_location]:
            version_uri = ExtHandlerVersionUri()
            version_uri.uri = uri
            ext_handler.versionUris.append(version_uri)
        return ext_handler

    def parse_plugin_settings(self, ext_handler, plugin_settings):
        if plugin_settings is None:
            return

        name = ext_handler.name
        version = ext_handler.properties.version
        settings = [x for x in plugin_settings \
                    if getattrib(x, ""name"") == name and \
                    getattrib(x, ""version"") == version]

        if settings is None or len(settings) == 0:
            return

        runtime_settings = None
        runtime_settings_node = find(settings[0], ""RuntimeSettings"")
        seqNo = getattrib(runtime_settings_node, ""seqNo"")
        runtime_settings_str = gettext(runtime_settings_node)
        try:
            runtime_settings = json.loads(runtime_settings_str)
        except ValueError as e:
            logger.error(""Invalid extension settings"")
            return

        for plugin_settings_list in runtime_settings[""runtimeSettings""]:
            handler_settings = plugin_settings_list[""handlerSettings""]
            ext = Extension()
            # There is no ""extension name"" in wire protocol.
            # Put
            ext.name = ext_handler.name
            ext.sequenceNumber = seqNo
            ext.publicSettings = handler_settings.get(""publicSettings"")
            ext.protectedSettings = handler_settings.get(""protectedSettings"")
            thumbprint = handler_settings.get(
                ""protectedSettingsCertThumbprint"")
            ext.certificateThumbprint = thumbprint
            ext_handler.properties.extensions.append(ext)


class ExtensionManifest(object):
    def __init__(self, xml_text):
        if xml_text is None:
            raise ValueError(""ExtensionManifest is None"")
        logger.verbose(""Load ExtensionManifest.xml"")
        self.pkg_list = ExtHandlerPackageList()
        self.allowed_versions = None
        self.parse(xml_text)

    def parse(self, xml_text):
        xml_doc = parse_doc(xml_text)
        self._handle_packages(findall(find(xml_doc,
                                           ""Plugins""),
                                      ""Plugin""),
                              False)
        self._handle_packages(findall(find(xml_doc,
                                           ""InternalPlugins""),
                                      ""Plugin""),
                              True)

    def _handle_packages(self, packages, isinternal):
        for package in packages:
            version = findtext(package, ""Version"")

            disallow_major_upgrade = findtext(package,
                                              ""DisallowMajorVersionUpgrade"")
            if disallow_major_upgrade is None:
                disallow_major_upgrade = ''
            disallow_major_upgrade = disallow_major_upgrade.lower() == ""true""

            uris = find(package, ""Uris"")
            uri_list = findall(uris, ""Uri"")
            uri_list = [gettext(x) for x in uri_list]
            pkg = ExtHandlerPackage()
            pkg.version = version
            pkg.disallow_major_upgrade = disallow_major_upgrade
            for uri in uri_list:
                pkg_uri = ExtHandlerVersionUri()
                pkg_uri.uri = uri
                pkg.uris.append(pkg_uri)

            pkg.isinternal = isinternal
            self.pkg_list.versions.append(pkg)


# Do not extend this class
class InVMArtifactsProfile(object):
    """"""
    deserialized json string of InVMArtifactsProfile.
    It is expected to contain the following fields:
    * inVMArtifactsProfileBlobSeqNo
    * profileId (optional)
    * onHold (optional)
    * certificateThumbprint (optional)
    * encryptedHealthChecks (optional)
    * encryptedApplicationProfile (optional)
    """"""
    def __init__(self, artifacts_profile):
        if not textutil.is_str_none_or_whitespace(artifacts_profile):
            self.__dict__.update(parse_json(artifacts_profile))

    def is_on_hold(self):
        # hasattr() is not available in Python 2.6
        if 'onHold' in self.__dict__:
            return self.onHold.lower() == 'true'
        return False
/n/n/nazurelinuxagent/common/utils/cryptutil.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import base64
import struct
import sys
import os.path
import subprocess

from azurelinuxagent.common.future import ustr, bytebuffer
from azurelinuxagent.common.exception import CryptError

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil

DECRYPT_SECRET_CMD = ""{0} cms -decrypt -inform DER -inkey {1} -in /dev/stdin""

class CryptUtil(object):
    def __init__(self, openssl_cmd):
        self.openssl_cmd = openssl_cmd

    def gen_transport_cert(self, prv_file, crt_file):
        """"""
        Create ssl certificate for https communication with endpoint server.
        """"""
        cmd = (""{0} req -x509 -nodes -subj /CN=LinuxTransport -days 730 ""
               ""-newkey rsa:2048 -keyout {1} ""
               ""-out {2}"").format(self.openssl_cmd, prv_file, crt_file)
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""Failed to create {0} and {1} certificates"".format(
                prv_file, crt_file))

    def get_pubkey_from_prv(self, file_name):
        cmd = ""{0} rsa -in {1} -pubout 2>/dev/null"".format(self.openssl_cmd, 
                                                           file_name)
        pub = shellutil.run_get_output(cmd)[1]
        return pub

    def get_pubkey_from_crt(self, file_name):
        cmd = ""{0} x509 -in {1} -pubkey -noout"".format(self.openssl_cmd, 
                                                       file_name)
        pub = shellutil.run_get_output(cmd)[1]
        return pub

    def get_thumbprint_from_crt(self, file_name):
        cmd=""{0} x509 -in {1} -fingerprint -noout"".format(self.openssl_cmd, 
                                                          file_name)
        thumbprint = shellutil.run_get_output(cmd)[1]
        thumbprint = thumbprint.rstrip().split('=')[1].replace(':', '').upper()
        return thumbprint

    def decrypt_p7m(self, p7m_file, trans_prv_file, trans_cert_file, pem_file):
        cmd = (""{0} cms -decrypt -in {1} -inkey {2} -recip {3} ""
               ""| {4} pkcs12 -nodes -password pass: -out {5}""
               """").format(self.openssl_cmd, p7m_file, trans_prv_file, 
                          trans_cert_file, self.openssl_cmd, pem_file)
        shellutil.run(cmd)
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""Failed to decrypt {0}"".format(p7m_file))

    def crt_to_ssh(self, input_file, output_file):
        shellutil.run(""ssh-keygen -i -m PKCS8 -f {0} >> {1}"".format(input_file,
                                                                    output_file))

    def asn1_to_ssh(self, pubkey):
        lines = pubkey.split(""\n"")
        lines = [x for x in lines if not x.startswith(""----"")]
        base64_encoded = """".join(lines)
        try:
            #TODO remove pyasn1 dependency
            from pyasn1.codec.der import decoder as der_decoder
            der_encoded = base64.b64decode(base64_encoded)
            der_encoded = der_decoder.decode(der_encoded)[0][1]
            key = der_decoder.decode(self.bits_to_bytes(der_encoded))[0]
            n=key[0]
            e=key[1]
            keydata = bytearray()
            keydata.extend(struct.pack('>I', len(""ssh-rsa"")))
            keydata.extend(b""ssh-rsa"")
            keydata.extend(struct.pack('>I', len(self.num_to_bytes(e))))
            keydata.extend(self.num_to_bytes(e))
            keydata.extend(struct.pack('>I', len(self.num_to_bytes(n)) + 1))
            keydata.extend(b""\0"")
            keydata.extend(self.num_to_bytes(n))
            keydata_base64 = base64.b64encode(bytebuffer(keydata))
            return ustr(b""ssh-rsa "" +  keydata_base64 + b""\n"", 
                        encoding='utf-8')
        except ImportError as e:
            raise CryptError(""Failed to load pyasn1.codec.der"")

    def num_to_bytes(self, num):
        """"""
        Pack number into bytes.  Retun as string.
        """"""
        result = bytearray()
        while num:
            result.append(num & 0xFF)
            num >>= 8
        result.reverse()
        return result

    def bits_to_bytes(self, bits):
        """"""
        Convert an array contains bits, [0,1] to a byte array
        """"""
        index = 7
        byte_array = bytearray()
        curr = 0
        for bit in bits:
            curr = curr | (bit << index)
            index = index - 1
            if index == -1:
                byte_array.append(curr)
                curr = 0
                index = 7
        return bytes(byte_array)

    def decrypt_secret(self, encrypted_password, private_key):
        try:
            decoded = base64.b64decode(encrypted_password)
        except Exception as e:
            raise CryptError(""Error decoding secret"", e)
        args = DECRYPT_SECRET_CMD.format(self.openssl_cmd, private_key).split(' ')
        p = subprocess.Popen(args, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.STDOUT)
        p.stdin.write(decoded)
        output = p.communicate()[0]
        retcode = p.poll()
        if retcode:
            raise subprocess.CalledProcessError(retcode, ""openssl cms -decrypt"", output=output)
        return output.decode('utf-16')
/n/n/nazurelinuxagent/ga/remoteaccess.py/n/n# Microsoft Azure Linux Agent
#
# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import datetime
import glob
import json
import operator
import os
import os.path
import pwd
import random
import re
import shutil
import stat
import subprocess
import textwrap
import time
import traceback
import zipfile

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.version as version
import azurelinuxagent.common.protocol.wire
import azurelinuxagent.common.protocol.metadata as metadata

from datetime import datetime, timedelta
from pwd import getpwall
from azurelinuxagent.common.errorstate import ErrorState

from azurelinuxagent.common.event import add_event, WALAEventOperation, elapsed_milliseconds
from azurelinuxagent.common.exception import ExtensionError, ProtocolError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.protocol.restapi import ExtHandlerStatus, \
                                                    ExtensionStatus, \
                                                    ExtensionSubStatus, \
                                                    VMStatus, ExtHandler, \
                                                    get_properties, \
                                                    set_properties
from azurelinuxagent.common.protocol.metadata import MetadataProtocol
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.utils.flexible_version import FlexibleVersion
from azurelinuxagent.common.utils.processutil import capture_from_process
from azurelinuxagent.common.protocol import get_protocol_util
from azurelinuxagent.common.version import AGENT_NAME, CURRENT_VERSION
from azurelinuxagent.common.osutil import get_osutil

REMOTE_USR_EXPIRATION_FORMAT = ""%a, %d %b %Y %H:%M:%S %Z""
DATE_FORMAT = ""%Y-%m-%d""
TRANSPORT_PRIVATE_CERT = ""TransportPrivate.pem""
REMOTE_ACCESS_ACCOUNT_COMMENT = ""JIT_Account""
MAX_TRY_ATTEMPT = 5
FAILED_ATTEMPT_THROTTLE = 1

def get_remote_access_handler():
    return RemoteAccessHandler()

class RemoteAccessHandler(object):
    def __init__(self):
        self.os_util = get_osutil()
        self.protocol_util = get_protocol_util()
        self.protocol = None
        self.cryptUtil = CryptUtil(conf.get_openssl_cmd())
        self.remote_access = None
        self.incarnation = 0

    def run(self):
        try:
            if self.os_util.jit_enabled:
                self.protocol = self.protocol_util.get_protocol()
                current_incarnation = self.protocol.get_incarnation()
                if self.incarnation != current_incarnation:
                    # something changed. Handle remote access if any.
                    self.incarnation = current_incarnation
                    self.remote_access = self.protocol.client.get_remote_access()
                    if self.remote_access is not None:
                        self.handle_remote_access()
        except Exception as e:
            msg = u""Exception processing remote access handler: {0} {1}"".format(ustr(e), traceback.format_exc())
            logger.error(msg)
            add_event(AGENT_NAME,
                      version=CURRENT_VERSION,
                      op=WALAEventOperation.RemoteAccessHandling,
                      is_success=False,
                      message=msg)

    def handle_remote_access(self):
        if self.remote_access is not None:
            # Get JIT user accounts.
            all_users = self.os_util.get_users()
            jit_users = set()
            for usr in all_users:
                if self.validate_jit_user(usr[4]):
                    jit_users.add(usr[0])
            for acc in self.remote_access.user_list.users:
                raw_expiration = acc.expiration
                account_expiration = datetime.strptime(raw_expiration, REMOTE_USR_EXPIRATION_FORMAT)
                now = datetime.utcnow()
                if acc.name not in jit_users and now < account_expiration:
                    self.add_user(acc.name, acc.encrypted_password, account_expiration)

    def validate_jit_user(self, comment):
        return comment == REMOTE_ACCESS_ACCOUNT_COMMENT

    def add_user(self, username, encrypted_password, account_expiration):
        try:
            expiration_date = (account_expiration + timedelta(days=1)).strftime(DATE_FORMAT)
            logger.verbose(""Adding user {0} with expiration date {1}""
                           .format(username, expiration_date))
            self.os_util.useradd(username, expiration_date, REMOTE_ACCESS_ACCOUNT_COMMENT)
        except OSError as oe:
            logger.error(""Error adding user {0}. {1}""
                         .format(username, oe.strerror))
            return
        except Exception as e:
            logger.error(""Error adding user {0}. {1}"".format(username, ustr(e)))
            return
        try:
            prv_key = os.path.join(conf.get_lib_dir(), TRANSPORT_PRIVATE_CERT)
            pwd = self.cryptUtil.decrypt_secret(encrypted_password, prv_key)
            self.os_util.chpasswd(username, pwd, conf.get_password_cryptid(), conf.get_password_crypt_salt_len())
            self.os_util.conf_sudoer(username)
            logger.info(""User '{0}' added successfully with expiration in {1}""
                        .format(username, expiration_date))
            return
        except OSError as oe:
            self.handle_failed_create(username, oe.strerror)
        except Exception as e:
            self.handle_failed_create(username, ustr(e))

    def handle_failed_create(self, username, error_message):
        logger.error(""Error creating user {0}. {1}""
                     .format(username, error_message))
        try:
            self.delete_user(username)
        except OSError as oe:
            logger.error(""Failed to clean up after account creation for {0}. {1}""
                         .format(username, oe.strerror()))
        except Exception as e:
            logger.error(""Failed to clean up after account creation for {0}. {1}""
                         .format(username, str(e)))

    def delete_user(self, username):
        self.os_util.del_account(username)
        logger.info(""User deleted {0}"".format(username))
/n/n/nazurelinuxagent/ga/update.py/n/n# Windows Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import glob
import json
import os
import platform
import random
import re
import shutil
import signal
import stat
import subprocess
import sys
import time
import traceback
import zipfile

from datetime import datetime, timedelta

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.restutil as restutil
import azurelinuxagent.common.utils.textutil as textutil

from azurelinuxagent.common.event import add_event, add_periodic, \
                                    elapsed_milliseconds, \
                                    WALAEventOperation
from azurelinuxagent.common.exception import ProtocolError, \
                                            ResourceGoneError, \
                                            UpdateError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.osutil import get_osutil
from azurelinuxagent.common.protocol import get_protocol_util
from azurelinuxagent.common.protocol.hostplugin import HostPluginProtocol
from azurelinuxagent.common.protocol.wire import WireProtocol
from azurelinuxagent.common.utils.flexible_version import FlexibleVersion
from azurelinuxagent.common.version import AGENT_NAME, AGENT_VERSION, AGENT_LONG_VERSION, \
                                            AGENT_DIR_GLOB, AGENT_PKG_GLOB, \
                                            AGENT_PATTERN, AGENT_NAME_PATTERN, AGENT_DIR_PATTERN, \
                                            CURRENT_AGENT, CURRENT_VERSION, \
                                            is_current_agent_installed

from azurelinuxagent.ga.exthandlers import HandlerManifest

AGENT_ERROR_FILE = ""error.json"" # File name for agent error record
AGENT_MANIFEST_FILE = ""HandlerManifest.json""
AGENT_PARTITION_FILE = ""partition""

CHILD_HEALTH_INTERVAL = 15 * 60
CHILD_LAUNCH_INTERVAL = 5 * 60
CHILD_LAUNCH_RESTART_MAX = 3
CHILD_POLL_INTERVAL = 60

MAX_FAILURE = 3 # Max failure allowed for agent before blacklisted

GOAL_STATE_INTERVAL = 3

ORPHAN_WAIT_INTERVAL = 15 * 60

AGENT_SENTINEL_FILE = ""current_version""

READONLY_FILE_GLOBS = [
    ""*.crt"",
    ""*.p7m"",
    ""*.pem"",
    ""*.prv"",
    ""ovf-env.xml""
]


def get_update_handler():
    return UpdateHandler()


def get_python_cmd():
    major_version = platform.python_version_tuple()[0]
    return ""python"" if int(major_version) <= 2 else ""python{0}"".format(major_version)


class UpdateHandler(object):

    def __init__(self):
        self.osutil = get_osutil()
        self.protocol_util = get_protocol_util()

        self.running = True
        self.last_attempt_time = None

        self.agents = []

        self.child_agent = None
        self.child_launch_time = None
        self.child_launch_attempts = 0
        self.child_process = None

        self.signal_handler = None

    def run_latest(self, child_args=None):
        """"""
        This method is called from the daemon to find and launch the most
        current, downloaded agent.

        Note:
        - Most events should be tagged to the launched agent (agent_version)
        """"""

        if self.child_process is not None:
            raise Exception(""Illegal attempt to launch multiple goal state Agent processes"")

        if self.signal_handler is None:
            self.signal_handler = signal.signal(signal.SIGTERM, self.forward_signal)

        latest_agent = self.get_latest_agent()
        if latest_agent is None:
            logger.info(u""Installed Agent {0} is the most current agent"", CURRENT_AGENT)
            agent_cmd = ""python -u {0} -run-exthandlers"".format(sys.argv[0])
            agent_dir = os.getcwd()
            agent_name = CURRENT_AGENT
            agent_version = CURRENT_VERSION
        else:
            logger.info(u""Determined Agent {0} to be the latest agent"", latest_agent.name)
            agent_cmd = latest_agent.get_agent_cmd()
            agent_dir = latest_agent.get_agent_dir()
            agent_name = latest_agent.name
            agent_version = latest_agent.version

        if child_args is not None:
            agent_cmd = ""{0} {1}"".format(agent_cmd, child_args)

        try:

            # Launch the correct Python version for python-based agents
            cmds = textutil.safe_shlex_split(agent_cmd)
            if cmds[0].lower() == ""python"":
                cmds[0] = get_python_cmd()
                agent_cmd = "" "".join(cmds)

            self._evaluate_agent_health(latest_agent)

            self.child_process = subprocess.Popen(
                cmds,
                cwd=agent_dir,
                stdout=sys.stdout,
                stderr=sys.stderr,
                env=os.environ)

            logger.verbose(u""Agent {0} launched with command '{1}'"", agent_name, agent_cmd)

            # If the most current agent is the installed agent and update is enabled,
            # assume updates are likely available and poll every second.
            # This reduces the start-up impact of finding / launching agent updates on
            # fresh VMs.
            if latest_agent is None and conf.get_autoupdate_enabled():
                poll_interval = 1
            else:
                poll_interval = CHILD_POLL_INTERVAL

            ret = None
            start_time = time.time()
            while (time.time() - start_time) < CHILD_HEALTH_INTERVAL:
                time.sleep(poll_interval)
                ret = self.child_process.poll()
                if ret is not None:
                    break

            if ret is None or ret <= 0:
                msg = u""Agent {0} launched with command '{1}' is successfully running"".format(
                    agent_name,
                    agent_cmd)
                logger.info(msg)
                add_event(
                    AGENT_NAME,
                    version=agent_version,
                    op=WALAEventOperation.Enable,
                    is_success=True,
                    message=msg,
                    log_event=False)

                if ret is None:
                    ret = self.child_process.wait()

            else:
                msg = u""Agent {0} launched with command '{1}' failed with return code: {2}"".format(
                    agent_name,
                    agent_cmd,
                    ret)
                logger.warn(msg)
                add_event(
                    AGENT_NAME,
                    version=agent_version,
                    op=WALAEventOperation.Enable,
                    is_success=False,
                    message=msg)

            if ret is not None and ret > 0:
                msg = u""Agent {0} launched with command '{1}' returned code: {2}"".format(
                    agent_name,
                    agent_cmd,
                    ret)
                logger.warn(msg)
                if latest_agent is not None:
                    latest_agent.mark_failure(is_fatal=True)

        except Exception as e:
            # Ignore child errors during termination
            if self.running:
                msg = u""Agent {0} launched with command '{1}' failed with exception: {2}"".format(
                    agent_name,
                    agent_cmd,
                    ustr(e))
                logger.warn(msg)
                add_event(
                    AGENT_NAME,
                    version=agent_version,
                    op=WALAEventOperation.Enable,
                    is_success=False,
                    message=msg)
                if latest_agent is not None:
                    latest_agent.mark_failure(is_fatal=True)

        self.child_process = None
        return

    def run(self):
        """"""
        This is the main loop which watches for agent and extension updates.
        """"""

        try:
            logger.info(u""Agent {0} is running as the goal state agent"",
                        CURRENT_AGENT)

            # Launch monitoring threads
            from azurelinuxagent.ga.monitor import get_monitor_handler
            monitor_thread = get_monitor_handler()
            monitor_thread.run()

            from azurelinuxagent.ga.env import get_env_handler
            env_thread = get_env_handler()
            env_thread.run()

            from azurelinuxagent.ga.exthandlers import get_exthandlers_handler, migrate_handler_state
            exthandlers_handler = get_exthandlers_handler()
            migrate_handler_state()

            from azurelinuxagent.ga.remoteaccess import get_remote_access_handler
            remote_access_handler = get_remote_access_handler()

            self._ensure_no_orphans()
            self._emit_restart_event()
            self._ensure_partition_assigned()
            self._ensure_readonly_files()

            while self.running:
                if self._is_orphaned:
                    logger.info(""Agent {0} is an orphan -- exiting"",
                                CURRENT_AGENT)
                    break

                if not monitor_thread.is_alive():
                    logger.warn(u""Monitor thread died, restarting"")
                    monitor_thread.start()

                if not env_thread.is_alive():
                    logger.warn(u""Environment thread died, restarting"")
                    env_thread.start()

                if self._upgrade_available():
                    available_agent = self.get_latest_agent()
                    if available_agent is None:
                        logger.info(
                            ""Agent {0} is reverting to the installed agent -- exiting"",
                            CURRENT_AGENT)
                    else:
                        logger.info(
                            u""Agent {0} discovered update {1} -- exiting"",
                            CURRENT_AGENT,
                            available_agent.name)
                    break

                utc_start = datetime.utcnow()

                last_etag = exthandlers_handler.last_etag
                exthandlers_handler.run()

                remote_access_handler.run()

                if last_etag != exthandlers_handler.last_etag:
                    self._ensure_readonly_files()
                    add_event(
                        AGENT_NAME,
                        version=CURRENT_VERSION,
                        op=WALAEventOperation.ProcessGoalState,
                        is_success=True,
                        duration=elapsed_milliseconds(utc_start),
                        message=""Incarnation {0}"".format(
                                            exthandlers_handler.last_etag),
                        log_event=True)

                time.sleep(GOAL_STATE_INTERVAL)

        except Exception as e:
            msg = u""Agent {0} failed with exception: {1}"".format(
                CURRENT_AGENT, ustr(e))
            self._set_sentinel(msg=msg)
            logger.warn(msg)
            logger.warn(traceback.format_exc())
            sys.exit(1)
            # additional return here because sys.exit is mocked in unit tests
            return

        self._shutdown()
        sys.exit(0)

    def forward_signal(self, signum, frame):
        if signum == signal.SIGTERM:
            self._shutdown()

        if self.child_process is None:
            return
        
        logger.info(
            u""Agent {0} forwarding signal {1} to {2}"",
            CURRENT_AGENT,
            signum,
            self.child_agent.name if self.child_agent is not None else CURRENT_AGENT)

        self.child_process.send_signal(signum)

        if self.signal_handler not in (None, signal.SIG_IGN, signal.SIG_DFL):
            self.signal_handler(signum, frame)
        elif self.signal_handler is signal.SIG_DFL:
            if signum == signal.SIGTERM:
                self._shutdown()
                sys.exit(0)
        return

    def get_latest_agent(self):
        """"""
        If autoupdate is enabled, return the most current, downloaded,
        non-blacklisted agent which is not the current version (if any).
        Otherwise, return None (implying to use the installed agent).
        """"""

        if not conf.get_autoupdate_enabled():
            return None
        
        self._find_agents()
        available_agents = [agent for agent in self.agents
                            if agent.is_available
                            and agent.version > FlexibleVersion(AGENT_VERSION)]

        return available_agents[0] if len(available_agents) >= 1 else None

    def _emit_restart_event(self):
        try:
            if not self._is_clean_start:
                msg = u""Agent did not terminate cleanly: {0}"".format(
                            fileutil.read_file(self._sentinel_file_path()))
                logger.info(msg)
                add_event(
                    AGENT_NAME,
                    version=CURRENT_VERSION,
                    op=WALAEventOperation.Restart,
                    is_success=False,
                    message=msg)
        except Exception:
            pass

        return

    def _ensure_no_orphans(self, orphan_wait_interval=ORPHAN_WAIT_INTERVAL):
        pid_files, ignored = self._write_pid_file()
        for pid_file in pid_files:
            try:
                pid = fileutil.read_file(pid_file)
                wait_interval = orphan_wait_interval

                while self.osutil.check_pid_alive(pid):
                    wait_interval -= GOAL_STATE_INTERVAL
                    if wait_interval <= 0:
                        logger.warn(
                            u""{0} forcibly terminated orphan process {1}"",
                            CURRENT_AGENT,
                            pid)
                        os.kill(pid, signal.SIGKILL)
                        break
                    
                    logger.info(
                        u""{0} waiting for orphan process {1} to terminate"",
                        CURRENT_AGENT,
                        pid)
                    time.sleep(GOAL_STATE_INTERVAL)

                os.remove(pid_file)

            except Exception as e:
                logger.warn(
                    u""Exception occurred waiting for orphan agent to terminate: {0}"",
                    ustr(e))
        return

    def _ensure_partition_assigned(self):
        """"""
        Assign the VM to a partition (0 - 99). Downloaded updates may be configured
        to run on only some VMs; the assigned partition determines eligibility.
        """"""
        if not os.path.exists(self._partition_file):
            partition = ustr(int(datetime.utcnow().microsecond / 10000))
            fileutil.write_file(self._partition_file, partition)
            add_event(
                AGENT_NAME,
                version=CURRENT_VERSION,
                op=WALAEventOperation.Partition,
                is_success=True,
                message=partition)

    def _ensure_readonly_files(self):
        for g in READONLY_FILE_GLOBS:
            for path in glob.iglob(os.path.join(conf.get_lib_dir(), g)):
                os.chmod(path, stat.S_IRUSR)

    def _evaluate_agent_health(self, latest_agent):
        """"""
        Evaluate the health of the selected agent: If it is restarting
        too frequently, raise an Exception to force blacklisting.
        """"""
        if latest_agent is None:
            self.child_agent = None
            return

        if self.child_agent is None or latest_agent.version != self.child_agent.version:
            self.child_agent = latest_agent
            self.child_launch_time = None
            self.child_launch_attempts = 0

        if self.child_launch_time is None:
            self.child_launch_time = time.time()

        self.child_launch_attempts += 1

        if (time.time() - self.child_launch_time) <= CHILD_LAUNCH_INTERVAL \
            and self.child_launch_attempts >= CHILD_LAUNCH_RESTART_MAX:
                msg = u""Agent {0} restarted more than {1} times in {2} seconds"".format(
                    self.child_agent.name,
                    CHILD_LAUNCH_RESTART_MAX,
                    CHILD_LAUNCH_INTERVAL)
                raise Exception(msg)
        return

    def _filter_blacklisted_agents(self):
        self.agents = [agent for agent in self.agents if not agent.is_blacklisted]

    def _find_agents(self):
        """"""
        Load all non-blacklisted agents currently on disk.
        """"""
        try:
            self._set_agents(self._load_agents())
            self._filter_blacklisted_agents()
        except Exception as e:
            logger.warn(u""Exception occurred loading available agents: {0}"", ustr(e))
        return

    def _get_host_plugin(self, protocol=None):
        return protocol.client.get_host_plugin() \
                                if protocol and \
                                    type(protocol) is WireProtocol and \
                                    protocol.client \
                                else None

    def _get_pid_parts(self):
        pid_file = conf.get_agent_pid_file_path()
        pid_dir = os.path.dirname(pid_file)
        pid_name = os.path.basename(pid_file)
        pid_re = re.compile(""(\d+)_{0}"".format(re.escape(pid_name)))
        return pid_dir, pid_name, pid_re

    def _get_pid_files(self):
        pid_dir, pid_name, pid_re = self._get_pid_parts()
        pid_files = [os.path.join(pid_dir, f) for f in os.listdir(pid_dir) if pid_re.match(f)]
        pid_files.sort(key=lambda f: int(pid_re.match(os.path.basename(f)).group(1)))
        return pid_files

    @property
    def _is_clean_start(self):
        return not os.path.isfile(self._sentinel_file_path())

    @property
    def _is_orphaned(self):
        parent_pid = os.getppid()
        if parent_pid in (1, None):
            return True

        if not os.path.isfile(conf.get_agent_pid_file_path()):
            return True

        return fileutil.read_file(conf.get_agent_pid_file_path()) != ustr(parent_pid)

    def _is_version_eligible(self, version):
        # Ensure the installed version is always eligible
        if version == CURRENT_VERSION and is_current_agent_installed():
            return True

        for agent in self.agents:
            if agent.version == version:
                return agent.is_available

        return False

    def _load_agents(self):
        path = os.path.join(conf.get_lib_dir(), ""{0}-*"".format(AGENT_NAME))
        return [GuestAgent(path=agent_dir)
                        for agent_dir in glob.iglob(path) if os.path.isdir(agent_dir)]

    def _partition(self):
        return int(fileutil.read_file(self._partition_file))

    @property
    def _partition_file(self):
        return os.path.join(conf.get_lib_dir(), AGENT_PARTITION_FILE)

    def _purge_agents(self):
        """"""
        Remove from disk all directories and .zip files of unknown agents
        (without removing the current, running agent).
        """"""
        path = os.path.join(conf.get_lib_dir(), ""{0}-*"".format(AGENT_NAME))

        known_versions = [agent.version for agent in self.agents]
        if CURRENT_VERSION not in known_versions:
            logger.verbose(
                u""Running Agent {0} was not found in the agent manifest - adding to list"",
                CURRENT_VERSION)
            known_versions.append(CURRENT_VERSION)

        for agent_path in glob.iglob(path):
            try:
                name = fileutil.trim_ext(agent_path, ""zip"")
                m = AGENT_DIR_PATTERN.match(name)
                if m is not None and FlexibleVersion(m.group(1)) not in known_versions:
                    if os.path.isfile(agent_path):
                        logger.info(u""Purging outdated Agent file {0}"", agent_path)
                        os.remove(agent_path)
                    else:
                        logger.info(u""Purging outdated Agent directory {0}"", agent_path)
                        shutil.rmtree(agent_path)
            except Exception as e:
                logger.warn(u""Purging {0} raised exception: {1}"", agent_path, ustr(e))
        return

    def _set_agents(self, agents=[]):
        self.agents = agents
        self.agents.sort(key=lambda agent: agent.version, reverse=True)
        return

    def _set_sentinel(self, agent=CURRENT_AGENT, msg=""Unknown cause""):
        try:
            fileutil.write_file(
                self._sentinel_file_path(),
                ""[{0}] [{1}]"".format(agent, msg))
        except Exception as e:
            logger.warn(
                u""Exception writing sentinel file {0}: {1}"",
                self._sentinel_file_path(),
                str(e))
        return

    def _sentinel_file_path(self):
        return os.path.join(conf.get_lib_dir(), AGENT_SENTINEL_FILE)

    def _shutdown(self):
        self.running = False

        if not os.path.isfile(self._sentinel_file_path()):
            return

        try:
            os.remove(self._sentinel_file_path())
        except Exception as e:
            logger.warn(
                u""Exception removing sentinel file {0}: {1}"",
                self._sentinel_file_path(),
                str(e))
        return

    def _upgrade_available(self, base_version=CURRENT_VERSION):
        # Emit an event expressing the state of AutoUpdate
        # Note:
        # - Duplicate events get suppressed; state transitions always emit
        add_event(
            AGENT_NAME,
            version=CURRENT_VERSION,
            op=WALAEventOperation.AutoUpdate,
            is_success=conf.get_autoupdate_enabled())

        # Ignore new agents if updating is disabled
        if not conf.get_autoupdate_enabled():
            return False

        now = time.time()
        if self.last_attempt_time is not None:
            next_attempt_time = self.last_attempt_time + \
                                    conf.get_autoupdate_frequency()
        else:
            next_attempt_time = now
        if next_attempt_time > now:
            return False

        family = conf.get_autoupdate_gafamily()
        logger.verbose(""Checking for agent family {0} updates"", family)

        self.last_attempt_time = now
        protocol = self.protocol_util.get_protocol()

        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    protocol.update_goal_state(forced=True)

                manifest_list, etag = protocol.get_vmagent_manifests()

                manifests = [m for m in manifest_list.vmAgentManifests \
                                if m.family == family and \
                                    len(m.versionsManifestUris) > 0]
                if len(manifests) == 0:
                    logger.verbose(u""Incarnation {0} has no {1} agent updates"",
                                    etag, family)
                    return False

                pkg_list = protocol.get_vmagent_pkgs(manifests[0])

                # Set the agents to those available for download at least as
                # current as the existing agent and remove from disk any agent
                # no longer reported to the VM.
                # Note:
                #  The code leaves on disk available, but blacklisted, agents
                #  so as to preserve the state. Otherwise, those agents could be
                #  again downloaded and inappropriately retried.
                host = self._get_host_plugin(protocol=protocol)
                self._set_agents([GuestAgent(pkg=pkg, host=host) \
                                     for pkg in pkg_list.versions])

                self._purge_agents()
                self._filter_blacklisted_agents()

                # Return True if current agent is no longer available or an
                # agent with a higher version number is available
                return not self._is_version_eligible(base_version) \
                    or (len(self.agents) > 0 \
                        and self.agents[0].version > base_version)

            except Exception as e:
                if isinstance(e, ResourceGoneError):
                    continue

                msg = u""Exception retrieving agent manifests: {0}"".format(
                            ustr(traceback.format_exc()))
                logger.warn(msg)
                add_event(
                    AGENT_NAME,
                    op=WALAEventOperation.Download,
                    version=CURRENT_VERSION,
                    is_success=False,
                    message=msg)
                return False

    def _write_pid_file(self):
        pid_files = self._get_pid_files()

        pid_dir, pid_name, pid_re = self._get_pid_parts()

        previous_pid_file = None \
                        if len(pid_files) <= 0 \
                        else pid_files[-1]
        pid_index = -1 \
                    if previous_pid_file is None \
                    else int(pid_re.match(os.path.basename(previous_pid_file)).group(1))
        pid_file = os.path.join(pid_dir, ""{0}_{1}"".format(pid_index+1, pid_name))

        try:
            fileutil.write_file(pid_file, ustr(os.getpid()))
            logger.info(u""{0} running as process {1}"", CURRENT_AGENT, ustr(os.getpid()))
        except Exception as e:
            pid_file = None
            logger.warn(
                u""Expection writing goal state agent {0} pid to {1}: {2}"",
                CURRENT_AGENT,
                pid_file,
                ustr(e))

        return pid_files, pid_file


class GuestAgent(object):
    def __init__(self, path=None, pkg=None, host=None):
        self.pkg = pkg
        self.host = host
        version = None
        if path is not None:
            m = AGENT_DIR_PATTERN.match(path)
            if m == None:
                raise UpdateError(u""Illegal agent directory: {0}"".format(path))
            version = m.group(1)
        elif self.pkg is not None:
            version = pkg.version

        if version == None:
            raise UpdateError(u""Illegal agent version: {0}"".format(version))
        self.version = FlexibleVersion(version)

        location = u""disk"" if path is not None else u""package""
        logger.verbose(u""Loading Agent {0} from {1}"", self.name, location)

        self.error = GuestAgentError(self.get_agent_error_file())
        self.error.load()

        try:
            self._ensure_downloaded()
            self._ensure_loaded()
        except Exception as e:
            if isinstance(e, ResourceGoneError):
                raise

            # The agent was improperly blacklisting versions due to a timeout
            # encountered while downloading a later version. Errors of type
            # socket.error are IOError, so this should provide sufficient
            # protection against a large class of I/O operation failures.
            if isinstance(e, IOError):
                raise

            # Note the failure, blacklist the agent if the package downloaded
            # - An exception with a downloaded package indicates the package
            #   is corrupt (e.g., missing the HandlerManifest.json file)
            self.mark_failure(is_fatal=os.path.isfile(self.get_agent_pkg_path()))

            msg = u""Agent {0} install failed with exception: {1}"".format(
                        self.name, ustr(e))
            logger.warn(msg)
            add_event(
                AGENT_NAME,
                version=self.version,
                op=WALAEventOperation.Install,
                is_success=False,
                message=msg)

    @property
    def name(self):
        return ""{0}-{1}"".format(AGENT_NAME, self.version)

    def get_agent_cmd(self):
        return self.manifest.get_enable_command()

    def get_agent_dir(self):
        return os.path.join(conf.get_lib_dir(), self.name)

    def get_agent_error_file(self):
        return os.path.join(conf.get_lib_dir(), self.name, AGENT_ERROR_FILE)

    def get_agent_manifest_path(self):
        return os.path.join(self.get_agent_dir(), AGENT_MANIFEST_FILE)

    def get_agent_pkg_path(self):
        return ""."".join((os.path.join(conf.get_lib_dir(), self.name), ""zip""))

    def clear_error(self):
        self.error.clear()
        self.error.save()

    @property
    def is_available(self):
        return self.is_downloaded and not self.is_blacklisted

    @property
    def is_blacklisted(self):
        return self.error is not None and self.error.is_blacklisted

    @property
    def is_downloaded(self):
        return self.is_blacklisted or \
                os.path.isfile(self.get_agent_manifest_path())

    def mark_failure(self, is_fatal=False):
        try:
            if not os.path.isdir(self.get_agent_dir()):
                os.makedirs(self.get_agent_dir())
            self.error.mark_failure(is_fatal=is_fatal)
            self.error.save()
            if self.error.is_blacklisted:
                logger.warn(u""Agent {0} is permanently blacklisted"", self.name)
        except Exception as e:
            logger.warn(u""Agent {0} failed recording error state: {1}"", self.name, ustr(e))

    def _ensure_downloaded(self):
        logger.verbose(u""Ensuring Agent {0} is downloaded"", self.name)

        if self.is_downloaded:
            logger.verbose(u""Agent {0} was previously downloaded - skipping download"", self.name)
            return

        if self.pkg is None:
            raise UpdateError(u""Agent {0} is missing package and download URIs"".format(
                self.name))
        
        self._download()
        self._unpack()

        msg = u""Agent {0} downloaded successfully"".format(self.name)
        logger.verbose(msg)
        add_event(
            AGENT_NAME,
            version=self.version,
            op=WALAEventOperation.Install,
            is_success=True,
            message=msg)

    def _ensure_loaded(self):
        self._load_manifest()
        self._load_error()

    def _download(self):
        uris_shuffled = self.pkg.uris
        random.shuffle(uris_shuffled)
        for uri in uris_shuffled:
            if not HostPluginProtocol.is_default_channel() and self._fetch(uri.uri):
                break

            elif self.host is not None and self.host.ensure_initialized():
                if not HostPluginProtocol.is_default_channel():
                    logger.warn(""Download failed, switching to host plugin"")
                else:
                    logger.verbose(""Using host plugin as default channel"")

                uri, headers = self.host.get_artifact_request(uri.uri, self.host.manifest_uri)
                try:
                    if self._fetch(uri, headers=headers, use_proxy=False):
                        if not HostPluginProtocol.is_default_channel():
                            logger.verbose(""Setting host plugin as default channel"")
                            HostPluginProtocol.set_default_channel(True)
                        break
                    else:
                        logger.warn(""Host plugin download failed"")

                # If the HostPlugin rejects the request,
                # let the error continue, but set to use the HostPlugin
                except ResourceGoneError:
                    HostPluginProtocol.set_default_channel(True)
                    raise

            else:
                logger.error(""No download channels available"")

        if not os.path.isfile(self.get_agent_pkg_path()):
            msg = u""Unable to download Agent {0} from any URI"".format(self.name)
            add_event(
                AGENT_NAME,
                op=WALAEventOperation.Download,
                version=CURRENT_VERSION,
                is_success=False,
                message=msg)
            raise UpdateError(msg)

    def _fetch(self, uri, headers=None, use_proxy=True):
        package = None
        try:
            is_healthy = True
            error_response = ''
            resp = restutil.http_get(uri, use_proxy=use_proxy, headers=headers)
            if restutil.request_succeeded(resp):
                package = resp.read()
                fileutil.write_file(self.get_agent_pkg_path(),
                                    bytearray(package),
                                    asbin=True)
                logger.verbose(u""Agent {0} downloaded from {1}"", self.name, uri)
            else:
                error_response = restutil.read_response_error(resp)
                logger.verbose(""Fetch was unsuccessful [{0}]"", error_response)
                is_healthy = not restutil.request_failed_at_hostplugin(resp)

            if self.host is not None:
                self.host.report_fetch_health(uri, is_healthy, source='GuestAgent', response=error_response)

        except restutil.HttpError as http_error:
            if isinstance(http_error, ResourceGoneError):
                raise

            logger.verbose(u""Agent {0} download from {1} failed [{2}]"",
                           self.name,
                           uri,
                           http_error)

        return package is not None

    def _load_error(self):
        try:
            self.error = GuestAgentError(self.get_agent_error_file())
            self.error.load()
            logger.verbose(u""Agent {0} error state: {1}"", self.name, ustr(self.error))
        except Exception as e:
            logger.warn(u""Agent {0} failed loading error state: {1}"", self.name, ustr(e))

    def _load_manifest(self):
        path = self.get_agent_manifest_path()
        if not os.path.isfile(path):
            msg = u""Agent {0} is missing the {1} file"".format(self.name, AGENT_MANIFEST_FILE)
            raise UpdateError(msg)

        with open(path, ""r"") as manifest_file:
            try:
                manifests = json.load(manifest_file)
            except Exception as e:
                msg = u""Agent {0} has a malformed {1}"".format(self.name, AGENT_MANIFEST_FILE)
                raise UpdateError(msg)
            if type(manifests) is list:
                if len(manifests) <= 0:
                    msg = u""Agent {0} has an empty {1}"".format(self.name, AGENT_MANIFEST_FILE)
                    raise UpdateError(msg)
                manifest = manifests[0]
            else:
                manifest = manifests

        try:
            self.manifest = HandlerManifest(manifest)
            if len(self.manifest.get_enable_command()) <= 0:
                raise Exception(u""Manifest is missing the enable command"")
        except Exception as e:
            msg = u""Agent {0} has an illegal {1}: {2}"".format(
                self.name,
                AGENT_MANIFEST_FILE,
                ustr(e))
            raise UpdateError(msg)

        logger.verbose(
            u""Agent {0} loaded manifest from {1}"",
            self.name,
            self.get_agent_manifest_path())
        logger.verbose(u""Successfully loaded Agent {0} {1}: {2}"",
            self.name,
            AGENT_MANIFEST_FILE,
            ustr(self.manifest.data))
        return

    def _unpack(self):
        try:
            if os.path.isdir(self.get_agent_dir()):
                shutil.rmtree(self.get_agent_dir())

            zipfile.ZipFile(self.get_agent_pkg_path()).extractall(self.get_agent_dir())

        except Exception as e:
            fileutil.clean_ioerror(e,
                paths=[self.get_agent_dir(), self.get_agent_pkg_path()])

            msg = u""Exception unpacking Agent {0} from {1}: {2}"".format(
                self.name,
                self.get_agent_pkg_path(),
                ustr(e))
            raise UpdateError(msg)

        if not os.path.isdir(self.get_agent_dir()):
            msg = u""Unpacking Agent {0} failed to create directory {1}"".format(
                self.name,
                self.get_agent_dir())
            raise UpdateError(msg)

        logger.verbose(
            u""Agent {0} unpacked successfully to {1}"",
            self.name,
            self.get_agent_dir())
        return


class GuestAgentError(object):
    def __init__(self, path):
        if path is None:
            raise UpdateError(u""GuestAgentError requires a path"")
        self.path = path

        self.clear()
        return
   
    def mark_failure(self, is_fatal=False):
        self.last_failure = time.time()
        self.failure_count += 1
        self.was_fatal = is_fatal
        return

    def clear(self):
        self.last_failure = 0.0
        self.failure_count = 0
        self.was_fatal = False
        return

    @property
    def is_blacklisted(self):
        return self.was_fatal or self.failure_count >= MAX_FAILURE

    def load(self):
        if self.path is not None and os.path.isfile(self.path):
            with open(self.path, 'r') as f:
                self.from_json(json.load(f))
        return

    def save(self):
        if os.path.isdir(os.path.dirname(self.path)):
            with open(self.path, 'w') as f:
                json.dump(self.to_json(), f)
        return
    
    def from_json(self, data):
        self.last_failure = max(
            self.last_failure,
            data.get(u""last_failure"", 0.0))
        self.failure_count = max(
            self.failure_count,
            data.get(u""failure_count"", 0))
        self.was_fatal = self.was_fatal or data.get(u""was_fatal"", False)
        return

    def to_json(self):
        data = {
            u""last_failure"": self.last_failure,
            u""failure_count"": self.failure_count,
            u""was_fatal"" : self.was_fatal
        }  
        return data

    def __str__(self):
        return ""Last Failure: {0}, Total Failures: {1}, Fatal: {2}"".format(
            self.last_failure,
            self.failure_count,
            self.was_fatal)
/n/n/ntests/common/osutil/mock_osutil.py/n/n# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

from azurelinuxagent.common.osutil.default import DefaultOSUtil

class MockOSUtil(DefaultOSUtil):
    def __init__(self):
        self.all_users = {}
        self.sudo_users = set()
        self.jit_enabled = True

    def useradd(self, username, expiration=None, comment=None):
        if username == """":
            raise Exception(""test exception for bad username"")
        if username in self.all_users:
            raise Exception(""test exception, user already exists"")
        self.all_users[username] = (username, None, None, None, comment, None, None, expiration)

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        if not remove:
            self.sudo_users.add(username)
        else:
            self.sudo_users.remove(username)

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if password == """":
            raise Exception(""test exception for bad password"")
        user = self.all_users[username]
        self.all_users[username] = (user[0], password, user[2], user[3], user[4], user[5], user[6], user[7])

    def del_account(self, username):
        if username == """":
            raise Exception(""test exception, bad data"")
        if username not in self.all_users:
            raise Exception(""test exception, user does not exist to delete"")
        self.all_users.pop(username)

    def get_users(self):
        return self.all_users.values()/n/n/ntests/ga/test_remoteaccess.py/n/n# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#
import xml

from tests.tools import *
from azurelinuxagent.common.protocol.wire import *
from azurelinuxagent.common.osutil import get_osutil

class TestRemoteAccess(AgentTestCase):
    def test_parse_remote_access(self):
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(""1"", remote_access.incarnation)
        self.assertEquals(1, len(remote_access.user_list.users), ""User count does not match."")
        self.assertEquals(""testAccount"", remote_access.user_list.users[0].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[0].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[0].expiration, ""Expiration does not match."")

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',
    return_value=GoalState(load_data('wire/goal_state.xml')))
    def test_update_remote_access_conf_no_remote_access(self, _):
        protocol = WireProtocol('12.34.56.78')
        goal_state = protocol.client.get_goal_state()
        protocol.client.update_remote_access_conf(goal_state)

    def test_parse_two_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_two_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(""1"", remote_access.incarnation)
        self.assertEquals(2, len(remote_access.user_list.users), ""User count does not match."")
        self.assertEquals(""testAccount1"", remote_access.user_list.users[0].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[0].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[0].expiration, ""Expiration does not match."")
        self.assertEquals(""testAccount2"", remote_access.user_list.users[1].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[1].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[1].expiration, ""Expiration does not match."")

    def test_parse_ten_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(10, len(remote_access.user_list.users), ""User count does not match."")
    
    def test_parse_duplicate_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_duplicate_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(2, len(remote_access.user_list.users), ""User count does not match."")
        self.assertEquals(""testAccount"", remote_access.user_list.users[0].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[0].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[0].expiration, ""Expiration does not match."")
        self.assertEquals(""testAccount"", remote_access.user_list.users[1].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[1].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[1].expiration, ""Expiration does not match."")

    def test_parse_zero_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_no_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(0, len(remote_access.user_list.users), ""User count does not match."")

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',
    return_value=GoalState(load_data('wire/goal_state_remote_access.xml')))
    @patch('azurelinuxagent.common.protocol.wire.WireClient.fetch_config',
    return_value=load_data('wire/remote_access_single_account.xml'))
    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_header_for_cert')
    def test_update_remote_access_conf_remote_access(self, _1, _2, _3):
        protocol = WireProtocol('12.34.56.78')
        goal_state = protocol.client.get_goal_state()
        protocol.client.update_remote_access_conf(goal_state)
        self.assertNotEquals(None, protocol.client.remote_access)
        self.assertEquals(1, len(protocol.client.remote_access.user_list.users))
        self.assertEquals('testAccount', protocol.client.remote_access.user_list.users[0].name)
        self.assertEquals('encryptedPasswordString', protocol.client.remote_access.user_list.users[0].encrypted_password)

    def test_parse_bad_remote_access_data(self):
        data = ""foobar""
        self.assertRaises(xml.parsers.expat.ExpatError, RemoteAccess, data)/n/n/ntests/ga/test_remoteaccess_handler.py/n/n# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

from datetime import timedelta
from azurelinuxagent.common.protocol.wire import *
from azurelinuxagent.ga.remoteaccess import RemoteAccessHandler
from tests.common.osutil.mock_osutil import MockOSUtil
from tests.tools import *


info_messages = []
error_messages = []


def get_user_dictionary(users):
    user_dictionary = {}
    for user in users:
        user_dictionary[user[0]] = user
    return user_dictionary


def log_info(msg_format, *args):
    info_messages.append(msg_format.format(args))


def log_error(msg_format, *args):
    error_messages.append(msg_format.format(args))


class TestRemoteAccessHandler(AgentTestCase):

    def setUp(self):
        super(TestRemoteAccessHandler, self).setUp()
        del info_messages[:]
        del error_messages[:]

    # add_user tests
    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=""]aPPEv}uNg1FPnl?"")
    def test_add_user(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        actual_user = users[tstuser]
        expected_expiration = (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d"")
        self.assertEqual(actual_user[7], expected_expiration)
        self.assertEqual(actual_user[4], ""JIT_Account"")
        self.assertEqual(0, len(error_messages))
        self.assertEqual(1, len(info_messages))
        self.assertEqual(info_messages[0], ""User '{0}' added successfully with expiration in {1}""
                         .format(tstuser, expected_expiration))

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=""]aPPEv}uNg1FPnl?"")
    def test_add_user_bad_creation_data(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = """"
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        self.assertEqual(0, len(rah.os_util.get_users()))
        self.assertEqual(1, len(error_messages))
        self.assertEqual(0, len(info_messages))
        error = ""Error adding user {0}. test exception for bad username"".format(tstuser)
        self.assertEqual(error, error_messages[0])

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value="""")
    def test_add_user_bad_password_data(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = """"
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        self.assertEqual(0, len(rah.os_util.get_users()))
        self.assertEqual(1, len(error_messages))
        self.assertEqual(1, len(info_messages))
        error = ""Error creating user {0}. test exception for bad password"".format(tstuser)
        self.assertEqual(error, error_messages[0])
        self.assertEqual(""User deleted {0}"".format(tstuser), info_messages[0])

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_add_user_already_existing(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        self.assertEqual(1, len(users.keys()))
        actual_user = users[tstuser]
        self.assertEqual(actual_user[7], (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d""))
        # add the new duplicate user, ensure it's not created and does not overwrite the existing user.
        # this does not test the user add function as that's mocked, it tests processing skips the remaining
        # calls after the initial failure
        new_user_expiration = datetime.utcnow() + timedelta(days=5)
        rah.add_user(tstuser, pwd, new_user_expiration)
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users after dup user attempted"".format(tstuser))
        self.assertEqual(1, len(users.keys()))
        actual_user = users[tstuser]
        self.assertEqual(actual_user[7], (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d""))

    # delete_user tests
    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=""]aPPEv}uNg1FPnl?"")
    def test_delete_user(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        expected_expiration = (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d"")
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        rah.delete_user(tstuser)
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertFalse(tstuser in users)
        self.assertEqual(0, len(error_messages))
        self.assertEqual(2, len(info_messages))
        self.assertEqual(""User '{0}' added successfully with expiration in {1}"".format(tstuser, expected_expiration),
                         info_messages[0])
        self.assertEqual(""User deleted {0}"".format(tstuser), info_messages[1])

    def test_handle_failed_create_with_bad_data(self):
        mock_os_util = MockOSUtil()
        testusr = ""foobar""
        mock_os_util.all_users[testusr] = (testusr, None, None, None, None, None, None, None)
        rah = RemoteAccessHandler()
        rah.os_util = mock_os_util
        rah.handle_failed_create("""", ""test message"")
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(1, len(users.keys()))
        self.assertTrue(testusr in users, ""Expected user {0} missing"".format(testusr))

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    def test_delete_user_does_not_exist(self, _1, _2):
        mock_os_util = MockOSUtil()
        testusr = ""foobar""
        mock_os_util.all_users[testusr] = (testusr, None, None, None, None, None, None, None)
        rah = RemoteAccessHandler()
        rah.os_util = mock_os_util
        testuser = ""Carl""
        test_message = ""test message""
        rah.handle_failed_create(testuser, test_message)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(1, len(users.keys()))
        self.assertTrue(testusr in users, ""Expected user {0} missing"".format(testusr))
        self.assertEqual(2, len(error_messages))
        self.assertEqual(0, len(info_messages))
        self.assertEqual(""Error creating user {0}. {1}"".format(testuser, test_message), error_messages[0])
        msg = ""Failed to clean up after account creation for {0}. test exception, user does not exist to delete""\
            .format(testuser)
        self.assertEqual(msg, error_messages[1])

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_new_user(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        tstuser = remote_access.user_list.users[0].name
        expiration_date = datetime.utcnow() + timedelta(days=1)
        expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        remote_access.user_list.users[0].expiration = expiration
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        actual_user = users[tstuser]
        expected_expiration = (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d"")
        self.assertEqual(actual_user[7], expected_expiration)
        self.assertEqual(actual_user[4], ""JIT_Account"")

    def test_do_not_add_expired_user(self):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()      
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        expiration = (datetime.utcnow() - timedelta(days=2)).strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        remote_access.user_list.users[0].expiration = expiration
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertFalse(""testAccount"" in users)

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    def test_error_add_user(self, _1, _2):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstuser = ""foobar""
        expiration = datetime.utcnow() + timedelta(days=1)
        pwd = ""bad password""
        rah.add_user(tstuser, pwd, expiration)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(0, len(users))
        self.assertEqual(1, len(error_messages))
        self.assertEqual(1, len(info_messages))
        error = ""Error creating user {0}. [CryptError] Error decoding secret\nInner error: Incorrect padding"".\
            format(tstuser)
        self.assertEqual(error, error_messages[0])
        self.assertEqual(""User deleted {0}"".format(tstuser), info_messages[0])

    def test_handle_remote_access_no_users(self):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_no_accounts.xml')
        remote_access = RemoteAccess(data_str)
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(0, len(users.keys()))

    def test_handle_remote_access_validate_jit_user_valid(self):
        rah = RemoteAccessHandler()
        comment = ""JIT_Account""
        result = rah.validate_jit_user(comment)
        self.assertTrue(result, ""Did not identify '{0}' as a JIT_Account"".format(comment))

    def test_handle_remote_access_validate_jit_user_invalid(self):
        rah = RemoteAccessHandler()
        test_users = [""John Doe"", None, """", "" ""]
        failed_results = """"
        for user in test_users:
            if rah.validate_jit_user(user):
                failed_results += ""incorrectly identified '{0} as a JIT_Account'.  "".format(user)
        if len(failed_results) > 0:
            self.fail(failed_results)

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_multiple_users(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_two_accounts.xml')
        remote_access = RemoteAccess(data_str)
        testusers = []
        count = 0
        while count < 2:
            user = remote_access.user_list.users[count].name
            expiration_date = datetime.utcnow() + timedelta(days=count + 1)
            expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
            remote_access.user_list.users[count].expiration = expiration
            testusers.append(user)
            count += 1
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(testusers[0] in users, ""{0} missing from users"".format(testusers[0]))
        self.assertTrue(testusers[1] in users, ""{0} missing from users"".format(testusers[1]))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    # max fabric supports in the Goal State
    def test_handle_remote_access_ten_users(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        count = 0
        for user in remote_access.user_list.users:
            count += 1
            user.name = ""tstuser{0}"".format(count)
            expiration_date = datetime.utcnow() + timedelta(days=count)
            user.expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(10, len(users.keys()))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_user_removed(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        count = 0
        for user in remote_access.user_list.users:
            count += 1
            user.name = ""tstuser{0}"".format(count)
            expiration_date = datetime.utcnow() + timedelta(days=count)
            user.expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(10, len(users.keys()))
        del rah.remote_access.user_list.users[:]
        self.assertEqual(10, len(users.keys()))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_bad_data_and_good_data(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        count = 0
        for user in remote_access.user_list.users:
            count += 1
            user.name = ""tstuser{0}"".format(count)
            if count is 2:
                user.name = """"
            expiration_date = datetime.utcnow() + timedelta(days=count)
            user.expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(9, len(users.keys()))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_deleted_user_readded(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        tstuser = remote_access.user_list.users[0].name
        expiration_date = datetime.utcnow() + timedelta(days=1)
        expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        remote_access.user_list.users[0].expiration = expiration
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        os_util = rah.os_util
        os_util.__class__ = MockOSUtil
        os_util.all_users.clear()
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser not in users)
        rah.handle_remote_access()
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    @patch('azurelinuxagent.common.osutil.get_osutil',
           return_value=MockOSUtil())
    @patch('azurelinuxagent.common.protocol.util.ProtocolUtil.get_protocol',
           return_value=WireProtocol(""12.34.56.78""))
    @patch('azurelinuxagent.common.protocol.wire.WireProtocol.get_incarnation',
           return_value=""1"")
    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_remote_access',
           return_value=""asdf"")
    def test_remote_access_handler_run_bad_data(self, _1, _2, _3, _4, _5):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        rah.run()
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
/n/n/ntests/ga/test_update.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the Apache License.

from __future__ import print_function

from azurelinuxagent.common.event import *
from azurelinuxagent.common.protocol.hostplugin import *
from azurelinuxagent.common.protocol.metadata import *
from azurelinuxagent.common.protocol.wire import *
from azurelinuxagent.common.utils.fileutil import *
from azurelinuxagent.ga.update import *

from tests.tools import *

NO_ERROR = {
    ""last_failure"" : 0.0,
    ""failure_count"" : 0,
    ""was_fatal"" : False
}

FATAL_ERROR = {
    ""last_failure"" : 42.42,
    ""failure_count"" : 2,
    ""was_fatal"" : True
}

WITH_ERROR = {
    ""last_failure"" : 42.42,
    ""failure_count"" : 2,
    ""was_fatal"" : False
}

EMPTY_MANIFEST = {
    ""name"": ""WALinuxAgent"",
    ""version"": 1.0,
    ""handlerManifest"": {
        ""installCommand"": """",
        ""uninstallCommand"": """",
        ""updateCommand"": """",
        ""enableCommand"": """",
        ""disableCommand"": """",
        ""rebootAfterInstall"": False,
        ""reportHeartbeat"": False
    }
}


def get_agent_pkgs(in_dir=os.path.join(data_dir, ""ga"")):
    path = os.path.join(in_dir, AGENT_PKG_GLOB)
    return glob.glob(path)


def get_agents(in_dir=os.path.join(data_dir, ""ga"")):
    path = os.path.join(in_dir, AGENT_DIR_GLOB)
    return [a for a in glob.glob(path) if os.path.isdir(a)]


def get_agent_file_path():
    return get_agent_pkgs()[0]


def get_agent_file_name():
    return os.path.basename(get_agent_file_path())


def get_agent_path():
    return fileutil.trim_ext(get_agent_file_path(), ""zip"")


def get_agent_name():
    return os.path.basename(get_agent_path())


def get_agent_version():
    return FlexibleVersion(get_agent_name().split(""-"")[1])


def faux_logger():
    print(""STDOUT message"")
    print(""STDERR message"", file=sys.stderr)
    return DEFAULT


class UpdateTestCase(AgentTestCase):

    def agent_bin(self, version, suffix):
        return ""bin/{0}-{1}{2}.egg"".format(AGENT_NAME, version, suffix)

    def rename_agent_bin(self, path, src_v, dst_v):
        src_bin = glob.glob(os.path.join(path, self.agent_bin(src_v, '*')))[0]
        dst_bin = os.path.join(path, self.agent_bin(dst_v, ''))
        shutil.move(src_bin, dst_bin)
    
    def agents(self):
        return [GuestAgent(path=path) for path in self.agent_dirs()]

    def agent_count(self):
        return len(self.agent_dirs())

    def agent_dirs(self):
        return get_agents(in_dir=self.tmp_dir)

    def agent_dir(self, version):
        return os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, version))

    def agent_paths(self):
        paths = glob.glob(os.path.join(self.tmp_dir, ""*""))
        paths.sort()
        return paths

    def agent_pkgs(self):
        return get_agent_pkgs(in_dir=self.tmp_dir)

    def agent_versions(self):
        v = [FlexibleVersion(AGENT_DIR_PATTERN.match(a).group(1)) for a in self.agent_dirs()]
        v.sort(reverse=True)
        return v

    def get_error_file(self, error_data=NO_ERROR):
        fp = tempfile.NamedTemporaryFile(mode=""w"")
        json.dump(error_data if error_data is not None else NO_ERROR, fp)
        fp.seek(0)
        return fp

    def create_error(self, error_data=NO_ERROR):
        with self.get_error_file(error_data) as path:
            err = GuestAgentError(path.name)
            err.load()
            return err

    def copy_agents(self, *agents):
        if len(agents) <= 0:
            agents = get_agent_pkgs()
        for agent in agents:
            shutil.copy(agent, self.tmp_dir)
        return

    def expand_agents(self):
        for agent in self.agent_pkgs():
            path = os.path.join(self.tmp_dir, fileutil.trim_ext(agent, ""zip""))
            zipfile.ZipFile(agent).extractall(path)

    def prepare_agent(self, version):
        """"""
        Create a download for the current agent version, copied from test data
        """"""
        self.copy_agents(get_agent_pkgs()[0])
        self.expand_agents()

        versions = self.agent_versions()
        src_v = FlexibleVersion(str(versions[0]))

        from_path = self.agent_dir(src_v)
        dst_v = FlexibleVersion(str(version))
        to_path = self.agent_dir(dst_v)

        if from_path != to_path:
            shutil.move(from_path + "".zip"", to_path + "".zip"")
            shutil.move(from_path, to_path)
            self.rename_agent_bin(to_path, src_v, dst_v)
        return

    def prepare_agents(self,
                       count=20,
                       is_available=True):

        # Ensure the test data is copied over
        agent_count = self.agent_count()
        if agent_count <= 0:
            self.copy_agents(get_agent_pkgs()[0])
            self.expand_agents()
            count -= 1

        # Determine the most recent agent version
        versions = self.agent_versions()
        src_v = FlexibleVersion(str(versions[0]))

        # Create agent packages and directories
        return self.replicate_agents(
            src_v=src_v,
            count=count-agent_count,
            is_available=is_available)

    def remove_agents(self):
        for agent in self.agent_paths():
            try:
                if os.path.isfile(agent):
                    os.remove(agent)
                else:
                    shutil.rmtree(agent)
            except:
                pass
        return

    def replicate_agents(self,
                         count=5,
                         src_v=AGENT_VERSION,
                         is_available=True,
                         increment=1):
        from_path = self.agent_dir(src_v)
        dst_v = FlexibleVersion(str(src_v))
        for i in range(0, count):
            dst_v += increment
            to_path = self.agent_dir(dst_v)
            shutil.copyfile(from_path + "".zip"", to_path + "".zip"")
            shutil.copytree(from_path, to_path)
            self.rename_agent_bin(to_path, src_v, dst_v)
            if not is_available:
                GuestAgent(to_path).mark_failure(is_fatal=True)
        return dst_v


class TestGuestAgentError(UpdateTestCase):
    def test_creation(self):
        self.assertRaises(TypeError, GuestAgentError)
        self.assertRaises(UpdateError, GuestAgentError, None)

        with self.get_error_file(error_data=WITH_ERROR) as path:
            err = GuestAgentError(path.name)
            err.load()
            self.assertEqual(path.name, err.path)
        self.assertNotEqual(None, err)

        self.assertEqual(WITH_ERROR[""last_failure""], err.last_failure)
        self.assertEqual(WITH_ERROR[""failure_count""], err.failure_count)
        self.assertEqual(WITH_ERROR[""was_fatal""], err.was_fatal)
        return

    def test_clear(self):
        with self.get_error_file(error_data=WITH_ERROR) as path:
            err = GuestAgentError(path.name)
            err.load()
            self.assertEqual(path.name, err.path)
        self.assertNotEqual(None, err)

        err.clear()
        self.assertEqual(NO_ERROR[""last_failure""], err.last_failure)
        self.assertEqual(NO_ERROR[""failure_count""], err.failure_count)
        self.assertEqual(NO_ERROR[""was_fatal""], err.was_fatal)
        return

    def test_save(self):
        err1 = self.create_error()
        err1.mark_failure()
        err1.mark_failure(is_fatal=True)

        err2 = self.create_error(err1.to_json())
        self.assertEqual(err1.last_failure, err2.last_failure)
        self.assertEqual(err1.failure_count, err2.failure_count)
        self.assertEqual(err1.was_fatal, err2.was_fatal)

    def test_mark_failure(self):
        err = self.create_error()
        self.assertFalse(err.is_blacklisted)

        for i in range(0, MAX_FAILURE):
            err.mark_failure()

        # Agent failed >= MAX_FAILURE, it should be blacklisted
        self.assertTrue(err.is_blacklisted)
        self.assertEqual(MAX_FAILURE, err.failure_count)
        return

    def test_mark_failure_permanent(self):
        err = self.create_error()

        self.assertFalse(err.is_blacklisted)

        # Fatal errors immediately blacklist
        err.mark_failure(is_fatal=True)
        self.assertTrue(err.is_blacklisted)
        self.assertTrue(err.failure_count < MAX_FAILURE)
        return

    def test_str(self):
        err = self.create_error(error_data=NO_ERROR)
        s = ""Last Failure: {0}, Total Failures: {1}, Fatal: {2}"".format(
            NO_ERROR[""last_failure""],
            NO_ERROR[""failure_count""],
            NO_ERROR[""was_fatal""])
        self.assertEqual(s, str(err))

        err = self.create_error(error_data=WITH_ERROR)
        s = ""Last Failure: {0}, Total Failures: {1}, Fatal: {2}"".format(
            WITH_ERROR[""last_failure""],
            WITH_ERROR[""failure_count""],
            WITH_ERROR[""was_fatal""])
        self.assertEqual(s, str(err))
        return


class TestGuestAgent(UpdateTestCase):
    def setUp(self):
        UpdateTestCase.setUp(self)
        self.copy_agents(get_agent_file_path())
        self.agent_path = os.path.join(self.tmp_dir, get_agent_name())

    def test_creation(self):
        self.assertRaises(UpdateError, GuestAgent, ""A very bad file name"")
        n = ""{0}-a.bad.version"".format(AGENT_NAME)
        self.assertRaises(UpdateError, GuestAgent, n)

        self.expand_agents()

        agent = GuestAgent(path=self.agent_path)
        self.assertNotEqual(None, agent)
        self.assertEqual(get_agent_name(), agent.name)
        self.assertEqual(get_agent_version(), agent.version)

        self.assertEqual(self.agent_path, agent.get_agent_dir())

        path = os.path.join(self.agent_path, AGENT_MANIFEST_FILE)
        self.assertEqual(path, agent.get_agent_manifest_path())

        self.assertEqual(
            os.path.join(self.agent_path, AGENT_ERROR_FILE),
            agent.get_agent_error_file())

        path = ""."".join((os.path.join(conf.get_lib_dir(), get_agent_name()), ""zip""))
        self.assertEqual(path, agent.get_agent_pkg_path())

        self.assertTrue(agent.is_downloaded)
        self.assertFalse(agent.is_blacklisted)
        self.assertTrue(agent.is_available)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    def test_clear_error(self, mock_downloaded):
        self.expand_agents()

        agent = GuestAgent(path=self.agent_path)
        agent.mark_failure(is_fatal=True)

        self.assertTrue(agent.error.last_failure > 0.0)
        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

        agent.clear_error()
        self.assertEqual(0.0, agent.error.last_failure)
        self.assertEqual(0, agent.error.failure_count)
        self.assertFalse(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_is_available(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)

        self.assertFalse(agent.is_available)
        agent._unpack()
        self.assertTrue(agent.is_available)

        agent.mark_failure(is_fatal=True)
        self.assertFalse(agent.is_available)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_is_blacklisted(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(agent.is_blacklisted)

        agent._unpack()
        self.assertFalse(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

        agent.mark_failure(is_fatal=True)
        self.assertTrue(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_resource_gone_error_not_blacklisted(self, mock_loaded, mock_downloaded):
        try:
            mock_downloaded.side_effect = ResourceGoneError()
            agent = GuestAgent(path=self.agent_path)
            self.assertFalse(agent.is_blacklisted)
        except ResourceGoneError:
            pass
        except:
            self.fail(""Exception was not expected!"")

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_ioerror_not_blacklisted(self, mock_loaded, mock_downloaded):
        try:
            mock_downloaded.side_effect = IOError()
            agent = GuestAgent(path=self.agent_path)
            self.assertFalse(agent.is_blacklisted)
        except IOError:
            pass
        except:
            self.fail(""Exception was not expected!"")

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_is_downloaded(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(agent.is_downloaded)
        agent._unpack()
        self.assertTrue(agent.is_downloaded)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_mark_failure(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)

        agent.mark_failure()
        self.assertEqual(1, agent.error.failure_count)

        agent.mark_failure(is_fatal=True)
        self.assertEqual(2, agent.error.failure_count)
        self.assertTrue(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_unpack(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        self.assertTrue(os.path.isdir(agent.get_agent_dir()))
        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_unpack_fail(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        os.remove(agent.get_agent_pkg_path())
        self.assertRaises(UpdateError, agent._unpack)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        agent._unpack()
        agent._load_manifest()
        self.assertEqual(agent.manifest.get_enable_command(),
                         agent.get_agent_cmd())

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest_missing(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        os.remove(agent.get_agent_manifest_path())
        self.assertRaises(UpdateError, agent._load_manifest)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest_is_empty(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))

        with open(agent.get_agent_manifest_path(), ""w"") as file:
            json.dump(EMPTY_MANIFEST, file)
        self.assertRaises(UpdateError, agent._load_manifest)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest_is_malformed(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))

        with open(agent.get_agent_manifest_path(), ""w"") as file:
            file.write(""This is not JSON data"")
        self.assertRaises(UpdateError, agent._load_manifest)

    def test_load_error(self):
        agent = GuestAgent(path=self.agent_path)
        agent.error = None

        agent._load_error()
        self.assertTrue(agent.error is not None)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    def test_download(self, mock_http_get, mock_loaded, mock_downloaded):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        agent_pkg = load_bin_data(os.path.join(""ga"", get_agent_file_name()))
        mock_http_get.return_value= ResponseMock(response=agent_pkg)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)
        agent._download()

        self.assertTrue(os.path.isfile(agent.get_agent_pkg_path()))

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    def test_download_fail(self, mock_http_get, mock_loaded, mock_downloaded):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        mock_http_get.return_value= ResponseMock(status=restutil.httpclient.SERVICE_UNAVAILABLE)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertRaises(UpdateError, agent._download)
        self.assertFalse(os.path.isfile(agent.get_agent_pkg_path()))
        self.assertFalse(agent.is_downloaded)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    @patch(""azurelinuxagent.ga.update.restutil.http_post"")
    def test_download_fallback(self, mock_http_post, mock_http_get, mock_loaded, mock_downloaded):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        mock_http_get.return_value = ResponseMock(
            status=restutil.httpclient.SERVICE_UNAVAILABLE,
            response="""")

        ext_uri = 'ext_uri'
        host_uri = 'host_uri'
        api_uri = URI_FORMAT_GET_API_VERSIONS.format(host_uri, HOST_PLUGIN_PORT)
        art_uri = URI_FORMAT_GET_EXTENSION_ARTIFACT.format(host_uri, HOST_PLUGIN_PORT)
        mock_host = HostPluginProtocol(host_uri,
                                       'container_id',
                                       'role_config')

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri(uri=ext_uri))
        agent = GuestAgent(pkg=pkg)
        agent.host = mock_host

        # ensure fallback fails gracefully, no http
        self.assertRaises(UpdateError, agent._download)
        self.assertEqual(mock_http_get.call_count, 2)
        self.assertEqual(mock_http_get.call_args_list[0][0][0], ext_uri)
        self.assertEqual(mock_http_get.call_args_list[1][0][0], api_uri)

        # ensure fallback fails gracefully, artifact api failure
        with patch.object(HostPluginProtocol,
                          ""ensure_initialized"",
                          return_value=True):
            self.assertRaises(UpdateError, agent._download)
            self.assertEqual(mock_http_get.call_count, 4)

            self.assertEqual(mock_http_get.call_args_list[2][0][0], ext_uri)

            self.assertEqual(mock_http_get.call_args_list[3][0][0], art_uri)
            a, k = mock_http_get.call_args_list[3]
            self.assertEqual(False, k['use_proxy'])

            # ensure fallback works as expected
            with patch.object(HostPluginProtocol,
                              ""get_artifact_request"",
                              return_value=[art_uri, {}]):
                self.assertRaises(UpdateError, agent._download)
                self.assertEqual(mock_http_get.call_count, 6)

                a, k = mock_http_get.call_args_list[3]
                self.assertEqual(False, k['use_proxy'])

                self.assertEqual(mock_http_get.call_args_list[4][0][0], ext_uri)
                a, k = mock_http_get.call_args_list[4]

                self.assertEqual(mock_http_get.call_args_list[5][0][0], art_uri)
                a, k = mock_http_get.call_args_list[5]
                self.assertEqual(False, k['use_proxy'])

    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    def test_ensure_downloaded(self, mock_http_get):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        agent_pkg = load_bin_data(os.path.join(""ga"", get_agent_file_name()))
        mock_http_get.return_value= ResponseMock(response=agent_pkg)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))
        self.assertTrue(agent.is_downloaded)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"", side_effect=UpdateError)
    def test_ensure_downloaded_download_fails(self, mock_download):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertFalse(agent.error.was_fatal)
        self.assertFalse(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._unpack"", side_effect=UpdateError)
    def test_ensure_downloaded_unpack_fails(self, mock_unpack, mock_download):
        self.assertFalse(os.path.isdir(self.agent_path))

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.error.was_fatal)
        self.assertTrue(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._unpack"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._load_manifest"", side_effect=UpdateError)
    def test_ensure_downloaded_load_manifest_fails(self, mock_manifest, mock_unpack, mock_download):
        self.assertFalse(os.path.isdir(self.agent_path))

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.error.was_fatal)
        self.assertTrue(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._unpack"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._load_manifest"")
    def test_ensure_download_skips_blacklisted(self, mock_manifest, mock_unpack, mock_download):
        agent = GuestAgent(path=self.agent_path)
        self.assertEqual(0, mock_download.call_count)

        agent.clear_error()
        agent.mark_failure(is_fatal=True)
        self.assertTrue(agent.is_blacklisted)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.error.was_fatal)
        self.assertTrue(agent.is_blacklisted)
        self.assertEqual(0, mock_download.call_count)
        self.assertEqual(0, mock_unpack.call_count)


class TestUpdate(UpdateTestCase):
    def setUp(self):
        UpdateTestCase.setUp(self)
        self.event_patch = patch('azurelinuxagent.common.event.add_event')
        self.update_handler = get_update_handler()
        self.update_handler.protocol_util = Mock()

    def test_creation(self):
        self.assertTrue(self.update_handler.running)

        self.assertEqual(None, self.update_handler.last_attempt_time)

        self.assertEqual(0, len(self.update_handler.agents))

        self.assertEqual(None, self.update_handler.child_agent)
        self.assertEqual(None, self.update_handler.child_launch_time)
        self.assertEqual(0, self.update_handler.child_launch_attempts)
        self.assertEqual(None, self.update_handler.child_process)

        self.assertEqual(None, self.update_handler.signal_handler)

    def test_emit_restart_event_emits_event_if_not_clean_start(self):
        try:
            mock_event = self.event_patch.start()
            self.update_handler._set_sentinel()
            self.update_handler._emit_restart_event()
            self.assertEqual(1, mock_event.call_count)
        except Exception as e:
            pass
        self.event_patch.stop()

    def _create_protocol(self, count=20, versions=None):
        latest_version = self.prepare_agents(count=count)
        if versions is None or len(versions) <= 0:
            versions = [latest_version]
        return ProtocolMock(versions=versions)

    def _test_ensure_no_orphans(self, invocations=3, interval=ORPHAN_WAIT_INTERVAL, pid_count=0):
        with patch.object(self.update_handler, 'osutil') as mock_util:
            # Note:
            # - Python only allows mutations of objects to which a function has
            #   a reference. Incrementing an integer directly changes the
            #   reference. Incrementing an item of a list changes an item to
            #   which the code has a reference.
            #   See http://stackoverflow.com/questions/26408941/python-nested-functions-and-variable-scope
            iterations = [0]
            def iterator(*args, **kwargs):
                iterations[0] += 1
                return iterations[0] < invocations

            mock_util.check_pid_alive = Mock(side_effect=iterator)

            pid_files = self.update_handler._get_pid_files()
            self.assertEqual(pid_count, len(pid_files))

            with patch('os.getpid', return_value=42):
                with patch('time.sleep', return_value=None) as mock_sleep:
                    self.update_handler._ensure_no_orphans(orphan_wait_interval=interval)
                    for pid_file in pid_files:
                        self.assertFalse(os.path.exists(pid_file))
                    return mock_util.check_pid_alive.call_count, mock_sleep.call_count

    def test_ensure_no_orphans(self):
        fileutil.write_file(os.path.join(self.tmp_dir, ""0_waagent.pid""), ustr(41))
        calls, sleeps = self._test_ensure_no_orphans(invocations=3, pid_count=1)
        self.assertEqual(3, calls)
        self.assertEqual(2, sleeps)

    def test_ensure_no_orphans_skips_if_no_orphans(self):
        calls, sleeps = self._test_ensure_no_orphans(invocations=3)
        self.assertEqual(0, calls)
        self.assertEqual(0, sleeps)

    def test_ensure_no_orphans_ignores_exceptions(self):
        with patch('azurelinuxagent.common.utils.fileutil.read_file', side_effect=Exception):
            calls, sleeps = self._test_ensure_no_orphans(invocations=3)
            self.assertEqual(0, calls)
            self.assertEqual(0, sleeps)

    def test_ensure_no_orphans_kills_after_interval(self):
        fileutil.write_file(os.path.join(self.tmp_dir, ""0_waagent.pid""), ustr(41))
        with patch('os.kill') as mock_kill:
            calls, sleeps = self._test_ensure_no_orphans(
                                        invocations=4,
                                        interval=3*GOAL_STATE_INTERVAL,
                                        pid_count=1)
            self.assertEqual(3, calls)
            self.assertEqual(2, sleeps)
            self.assertEqual(1, mock_kill.call_count)

    @patch('azurelinuxagent.ga.update.datetime')
    def test_ensure_partition_assigned(self, mock_time):
        path = os.path.join(conf.get_lib_dir(), AGENT_PARTITION_FILE)
        mock_time.utcnow = Mock()

        self.assertFalse(os.path.exists(path))

        for n in range(0,99):
            mock_time.utcnow.return_value = Mock(microsecond=n* 10000)

            self.update_handler._ensure_partition_assigned()

            self.assertTrue(os.path.exists(path))
            s = fileutil.read_file(path)
            self.assertEqual(n, int(s))
            os.remove(path)

    def test_ensure_readonly_sets_readonly(self):
        test_files = [
            os.path.join(conf.get_lib_dir(), ""faux_certificate.crt""),
            os.path.join(conf.get_lib_dir(), ""faux_certificate.p7m""),
            os.path.join(conf.get_lib_dir(), ""faux_certificate.pem""),
            os.path.join(conf.get_lib_dir(), ""faux_certificate.prv""),
            os.path.join(conf.get_lib_dir(), ""ovf-env.xml"")
        ]
        for path in test_files:
            fileutil.write_file(path, ""Faux content"")
            os.chmod(path,
                stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)

        self.update_handler._ensure_readonly_files()

        for path in test_files:
            mode = os.stat(path).st_mode
            mode &= (stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            self.assertEqual(0, mode ^ stat.S_IRUSR)

    def test_ensure_readonly_leaves_unmodified(self):
        test_files = [
            os.path.join(conf.get_lib_dir(), ""faux.xml""),
            os.path.join(conf.get_lib_dir(), ""faux.json""),
            os.path.join(conf.get_lib_dir(), ""faux.txt""),
            os.path.join(conf.get_lib_dir(), ""faux"")
        ]
        for path in test_files:
            fileutil.write_file(path, ""Faux content"")
            os.chmod(path,
                stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)

        self.update_handler._ensure_readonly_files()

        for path in test_files:
            mode = os.stat(path).st_mode
            mode &= (stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            self.assertEqual(
                stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH,
                mode)

    def _test_evaluate_agent_health(self, child_agent_index=0):
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertFalse(latest_agent.is_blacklisted)
        self.assertTrue(len(self.update_handler.agents) > 1)

        child_agent = self.update_handler.agents[child_agent_index]
        self.assertTrue(child_agent.is_available)
        self.assertFalse(child_agent.is_blacklisted)
        self.update_handler.child_agent = child_agent

        self.update_handler._evaluate_agent_health(latest_agent)

    def test_evaluate_agent_health_ignores_installed_agent(self):
        self.update_handler._evaluate_agent_health(None)

    def test_evaluate_agent_health_raises_exception_for_restarting_agent(self):
        self.update_handler.child_launch_time = time.time() - (4 * 60)
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX - 1
        self.assertRaises(Exception, self._test_evaluate_agent_health)

    def test_evaluate_agent_health_will_not_raise_exception_for_long_restarts(self):
        self.update_handler.child_launch_time = time.time() - 24 * 60
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX
        self._test_evaluate_agent_health()

    def test_evaluate_agent_health_will_not_raise_exception_too_few_restarts(self):
        self.update_handler.child_launch_time = time.time()
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX - 2
        self._test_evaluate_agent_health()

    def test_evaluate_agent_health_resets_with_new_agent(self):
        self.update_handler.child_launch_time = time.time() - (4 * 60)
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX - 1
        self._test_evaluate_agent_health(child_agent_index=1)
        self.assertEqual(1, self.update_handler.child_launch_attempts)

    def test_filter_blacklisted_agents(self):
        self.prepare_agents()

        self.update_handler._set_agents([GuestAgent(path=path) for path in self.agent_dirs()])
        self.assertEqual(len(self.agent_dirs()), len(self.update_handler.agents))

        kept_agents = self.update_handler.agents[::2]
        blacklisted_agents = self.update_handler.agents[1::2]
        for agent in blacklisted_agents:
            agent.mark_failure(is_fatal=True)
        self.update_handler._filter_blacklisted_agents()
        self.assertEqual(kept_agents, self.update_handler.agents)

    def test_find_agents(self):
        self.prepare_agents()

        self.assertTrue(0 <= len(self.update_handler.agents))
        self.update_handler._find_agents()
        self.assertEqual(len(get_agents(self.tmp_dir)), len(self.update_handler.agents))

    def test_find_agents_does_reload(self):
        self.prepare_agents()

        self.update_handler._find_agents()
        agents = self.update_handler.agents

        self.update_handler._find_agents()
        self.assertNotEqual(agents, self.update_handler.agents)

    def test_find_agents_sorts(self):
        self.prepare_agents()
        self.update_handler._find_agents()

        v = FlexibleVersion(""100000"")
        for a in self.update_handler.agents:
            self.assertTrue(v > a.version)
            v = a.version

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_host_plugin')
    def test_get_host_plugin_returns_host_for_wireserver(self, mock_get_host):
        protocol = WireProtocol('12.34.56.78')
        mock_get_host.return_value = ""faux host""
        host = self.update_handler._get_host_plugin(protocol=protocol)
        print(""mock_get_host call cound={0}"".format(mock_get_host.call_count))
        self.assertEqual(1, mock_get_host.call_count)
        self.assertEqual(""faux host"", host)

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_host_plugin')
    def test_get_host_plugin_returns_none_otherwise(self, mock_get_host):
        protocol = MetadataProtocol()
        host = self.update_handler._get_host_plugin(protocol=protocol)
        mock_get_host.assert_not_called()
        self.assertEqual(None, host)

    def test_get_latest_agent(self):
        latest_version = self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertEqual(len(get_agents(self.tmp_dir)), len(self.update_handler.agents))
        self.assertEqual(latest_version, latest_agent.version)

    def test_get_latest_agent_excluded(self):
        self.prepare_agent(AGENT_VERSION)
        self.assertFalse(self._test_upgrade_available(
                                versions=self.agent_versions(),
                                count=1))
        self.assertEqual(None, self.update_handler.get_latest_agent())

    def test_get_latest_agent_no_updates(self):
        self.assertEqual(None, self.update_handler.get_latest_agent())

    def test_get_latest_agent_skip_updates(self):
        conf.get_autoupdate_enabled = Mock(return_value=False)
        self.assertEqual(None, self.update_handler.get_latest_agent())

    def test_get_latest_agent_skips_unavailable(self):
        self.prepare_agents()
        prior_agent = self.update_handler.get_latest_agent()

        latest_version = self.prepare_agents(count=self.agent_count()+1, is_available=False)
        latest_path = os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, latest_version))
        self.assertFalse(GuestAgent(latest_path).is_available)

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.version < latest_version)
        self.assertEqual(latest_agent.version, prior_agent.version)

    def test_get_pid_files(self):
        pid_files = self.update_handler._get_pid_files()
        self.assertEqual(0, len(pid_files))

    def test_get_pid_files_returns_previous(self):
        for n in range(1250):
            fileutil.write_file(os.path.join(self.tmp_dir, str(n)+""_waagent.pid""), ustr(n+1))
        pid_files = self.update_handler._get_pid_files()
        self.assertEqual(1250, len(pid_files))

        pid_dir, pid_name, pid_re = self.update_handler._get_pid_parts()
        for p in pid_files:
            self.assertTrue(pid_re.match(os.path.basename(p)))

    def test_is_clean_start_returns_true_when_no_sentinel(self):
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))
        self.assertTrue(self.update_handler._is_clean_start)

    def test_is_clean_start_returns_false_when_sentinel_exists(self):
        self.update_handler._set_sentinel(agent=CURRENT_AGENT)
        self.assertFalse(self.update_handler._is_clean_start)

    def test_is_clean_start_returns_false_for_exceptions(self):
        self.update_handler._set_sentinel()
        with patch(""azurelinuxagent.common.utils.fileutil.read_file"", side_effect=Exception):
            self.assertFalse(self.update_handler._is_clean_start)

    def test_is_orphaned_returns_false_if_parent_exists(self):
        fileutil.write_file(conf.get_agent_pid_file_path(), ustr(42))
        with patch('os.getppid', return_value=42):
            self.assertFalse(self.update_handler._is_orphaned)

    def test_is_orphaned_returns_true_if_parent_is_init(self):
        with patch('os.getppid', return_value=1):
            self.assertTrue(self.update_handler._is_orphaned)

    def test_is_orphaned_returns_true_if_parent_does_not_exist(self):
        fileutil.write_file(conf.get_agent_pid_file_path(), ustr(24))
        with patch('os.getppid', return_value=42):
            self.assertTrue(self.update_handler._is_orphaned)

    def test_is_version_available(self):
        self.prepare_agents(is_available=True)
        self.update_handler.agents = self.agents()

        for agent in self.agents():
            self.assertTrue(self.update_handler._is_version_eligible(agent.version))

    @patch(""azurelinuxagent.ga.update.is_current_agent_installed"", return_value=False)
    def test_is_version_available_rejects(self, mock_current):
        self.prepare_agents(is_available=True)
        self.update_handler.agents = self.agents()

        self.update_handler.agents[0].mark_failure(is_fatal=True)
        self.assertFalse(self.update_handler._is_version_eligible(self.agents()[0].version))

    @patch(""azurelinuxagent.ga.update.is_current_agent_installed"", return_value=True)
    def test_is_version_available_accepts_current(self, mock_current):
        self.update_handler.agents = []
        self.assertTrue(self.update_handler._is_version_eligible(CURRENT_VERSION))

    @patch(""azurelinuxagent.ga.update.is_current_agent_installed"", return_value=False)
    def test_is_version_available_rejects_by_default(self, mock_current):
        self.prepare_agents()
        self.update_handler.agents = []

        v = self.agents()[0].version
        self.assertFalse(self.update_handler._is_version_eligible(v))

    def test_purge_agents(self):
        self.prepare_agents()
        self.update_handler._find_agents()

        # Ensure at least three agents initially exist
        self.assertTrue(2 < len(self.update_handler.agents))

        # Purge every other agent
        kept_agents = self.update_handler.agents[1::2]
        purged_agents = self.update_handler.agents[::2]

        # Reload and assert only the kept agents remain on disk
        self.update_handler.agents = kept_agents
        self.update_handler._purge_agents()
        self.update_handler._find_agents()
        self.assertEqual(
            [agent.version for agent in kept_agents],
            [agent.version for agent in self.update_handler.agents])

        # Ensure both directories and packages are removed
        for agent in purged_agents:
            agent_path = os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, agent.version))
            self.assertFalse(os.path.exists(agent_path))
            self.assertFalse(os.path.exists(agent_path + "".zip""))

        # Ensure kept agent directories and packages remain
        for agent in kept_agents:
            agent_path = os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, agent.version))
            self.assertTrue(os.path.exists(agent_path))
            self.assertTrue(os.path.exists(agent_path + "".zip""))

    def _test_run_latest(self, mock_child=None, mock_time=None, child_args=None):
        if mock_child is None:
            mock_child = ChildMock()
        if mock_time is None:
            mock_time = TimeMock()

        with patch('subprocess.Popen', return_value=mock_child) as mock_popen:
            with patch('time.time', side_effect=mock_time.time):
                with patch('time.sleep', side_effect=mock_time.sleep):
                    self.update_handler.run_latest(child_args=child_args)
                    self.assertEqual(1, mock_popen.call_count)

                    return mock_popen.call_args

    def test_run_latest(self):
        self.prepare_agents()

        agent = self.update_handler.get_latest_agent()
        args, kwargs = self._test_run_latest()
        args = args[0]
        cmds = textutil.safe_shlex_split(agent.get_agent_cmd())
        if cmds[0].lower() == ""python"":
            cmds[0] = get_python_cmd()

        self.assertEqual(args, cmds)
        self.assertTrue(len(args) > 1)
        self.assertTrue(args[0].startswith(""python""))
        self.assertEqual(""-run-exthandlers"", args[len(args)-1])
        self.assertEqual(True, 'cwd' in kwargs)
        self.assertEqual(agent.get_agent_dir(), kwargs['cwd'])
        self.assertEqual(False, '\x00' in cmds[0])

    def test_run_latest_passes_child_args(self):
        self.prepare_agents()

        agent = self.update_handler.get_latest_agent()
        args, kwargs = self._test_run_latest(child_args=""AnArgument"")
        args = args[0]

        self.assertTrue(len(args) > 1)
        self.assertTrue(args[0].startswith(""python""))
        self.assertEqual(""AnArgument"", args[len(args)-1])

    def test_run_latest_polls_and_waits_for_success(self):
        mock_child = ChildMock(return_value=None)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/3)
        self._test_run_latest(mock_child=mock_child, mock_time=mock_time)
        self.assertEqual(2, mock_child.poll.call_count)
        self.assertEqual(1, mock_child.wait.call_count)

    def test_run_latest_polling_stops_at_success(self):
        mock_child = ChildMock(return_value=0)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/3)
        self._test_run_latest(mock_child=mock_child, mock_time=mock_time)
        self.assertEqual(1, mock_child.poll.call_count)
        self.assertEqual(0, mock_child.wait.call_count)

    def test_run_latest_polling_stops_at_failure(self):
        mock_child = ChildMock(return_value=42)
        mock_time = TimeMock()
        self._test_run_latest(mock_child=mock_child, mock_time=mock_time)
        self.assertEqual(1, mock_child.poll.call_count)
        self.assertEqual(0, mock_child.wait.call_count)

    def test_run_latest_polls_frequently_if_installed_is_latest(self):
        mock_child = ChildMock(return_value=0)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/2)
        self._test_run_latest(mock_time=mock_time)
        self.assertEqual(1, mock_time.sleep_interval)

    def test_run_latest_polls_moderately_if_installed_not_latest(self):
        self.prepare_agents()

        mock_child = ChildMock(return_value=0)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/2)
        self._test_run_latest(mock_time=mock_time)
        self.assertNotEqual(1, mock_time.sleep_interval)

    def test_run_latest_defaults_to_current(self):
        self.assertEqual(None, self.update_handler.get_latest_agent())

        args, kwargs = self._test_run_latest()

        self.assertEqual(args[0], [get_python_cmd(), ""-u"", sys.argv[0], ""-run-exthandlers""])
        self.assertEqual(True, 'cwd' in kwargs)
        self.assertEqual(os.getcwd(), kwargs['cwd'])

    def test_run_latest_forwards_output(self):
        try:
            tempdir = tempfile.mkdtemp()
            stdout_path = os.path.join(tempdir, ""stdout"")
            stderr_path = os.path.join(tempdir, ""stderr"")

            with open(stdout_path, ""w"") as stdout:
                with open(stderr_path, ""w"") as stderr:
                    saved_stdout, sys.stdout = sys.stdout, stdout
                    saved_stderr, sys.stderr = sys.stderr, stderr
                    try:
                        self._test_run_latest(mock_child=ChildMock(side_effect=faux_logger))
                    finally:
                        sys.stdout = saved_stdout
                        sys.stderr = saved_stderr

            with open(stdout_path, ""r"") as stdout:
                self.assertEqual(1, len(stdout.readlines()))
            with open(stderr_path, ""r"") as stderr:
                self.assertEqual(1, len(stderr.readlines()))
        finally:
            shutil.rmtree(tempdir, True)

    def test_run_latest_nonzero_code_marks_failures(self):
        # logger.add_logger_appender(logger.AppenderType.STDOUT)
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

        with patch('azurelinuxagent.ga.update.UpdateHandler.get_latest_agent', return_value=latest_agent):
            self._test_run_latest(mock_child=ChildMock(return_value=1))

        self.assertTrue(latest_agent.is_blacklisted)
        self.assertFalse(latest_agent.is_available)
        self.assertNotEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(1, latest_agent.error.failure_count)

    def test_run_latest_exception_blacklists(self):
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

        with patch('azurelinuxagent.ga.update.UpdateHandler.get_latest_agent', return_value=latest_agent):
            self._test_run_latest(mock_child=ChildMock(side_effect=Exception(""Force blacklisting"")))

        self.assertFalse(latest_agent.is_available)
        self.assertTrue(latest_agent.error.is_blacklisted)
        self.assertNotEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(1, latest_agent.error.failure_count)

    def test_run_latest_exception_does_not_blacklist_if_terminating(self):
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

        with patch('azurelinuxagent.ga.update.UpdateHandler.get_latest_agent', return_value=latest_agent):
            self.update_handler.running = False
            self._test_run_latest(mock_child=ChildMock(side_effect=Exception(""Attempt blacklisting"")))

        self.assertTrue(latest_agent.is_available)
        self.assertFalse(latest_agent.error.is_blacklisted)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

    @patch('signal.signal')
    def test_run_latest_captures_signals(self, mock_signal):
        self._test_run_latest()
        self.assertEqual(1, mock_signal.call_count)

    @patch('signal.signal')
    def test_run_latest_creates_only_one_signal_handler(self, mock_signal):
        self.update_handler.signal_handler = ""Not None""
        self._test_run_latest()
        self.assertEqual(0, mock_signal.call_count)

    def _test_run(self, invocations=1, calls=[call.run()], enable_updates=False):
        conf.get_autoupdate_enabled = Mock(return_value=enable_updates)

        # Note:
        # - Python only allows mutations of objects to which a function has
        #   a reference. Incrementing an integer directly changes the
        #   reference. Incrementing an item of a list changes an item to
        #   which the code has a reference.
        #   See http://stackoverflow.com/questions/26408941/python-nested-functions-and-variable-scope
        iterations = [0]
        def iterator(*args, **kwargs):
            iterations[0] += 1
            if iterations[0] >= invocations:
                self.update_handler.running = False
            return

        fileutil.write_file(conf.get_agent_pid_file_path(), ustr(42))

        with patch('azurelinuxagent.ga.exthandlers.get_exthandlers_handler') as mock_handler:
            with patch('azurelinuxagent.ga.remoteaccess.get_remote_access_handler') as mock_ra_handler:
                with patch('azurelinuxagent.ga.monitor.get_monitor_handler') as mock_monitor:
                    with patch('azurelinuxagent.ga.env.get_env_handler') as mock_env:
                        with patch('time.sleep', side_effect=iterator) as mock_sleep:
                            with patch('sys.exit') as mock_exit:
                                if isinstance(os.getppid, MagicMock):
                                    self.update_handler.run()
                                else:
                                    with patch('os.getppid', return_value=42):
                                        self.update_handler.run()

                                self.assertEqual(1, mock_handler.call_count)
                                self.assertEqual(mock_handler.return_value.method_calls, calls)
                                self.assertEqual(1, mock_ra_handler.call_count)
                                self.assertEqual(mock_ra_handler.return_value.method_calls, calls)
                                self.assertEqual(invocations, mock_sleep.call_count)
                                self.assertEqual(1, mock_monitor.call_count)
                                self.assertEqual(1, mock_env.call_count)
                                self.assertEqual(1, mock_exit.call_count)

    def test_run(self):
        self._test_run()

    def test_run_keeps_running(self):
        self._test_run(invocations=15, calls=[call.run()]*15)

    def test_run_stops_if_update_available(self):
        self.update_handler._upgrade_available = Mock(return_value=True)
        self._test_run(invocations=0, calls=[], enable_updates=True)

    def test_run_stops_if_orphaned(self):
        with patch('os.getppid', return_value=1):
            self._test_run(invocations=0, calls=[], enable_updates=True)

    def test_run_clears_sentinel_on_successful_exit(self):
        self._test_run()
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_run_leaves_sentinel_on_unsuccessful_exit(self):
        self.update_handler._upgrade_available = Mock(side_effect=Exception)
        self._test_run(invocations=0, calls=[], enable_updates=True)
        self.assertTrue(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_run_emits_restart_event(self):
        self.update_handler._emit_restart_event = Mock()
        self._test_run()
        self.assertEqual(1, self.update_handler._emit_restart_event.call_count)

    def test_set_agents_sets_agents(self):
        self.prepare_agents()

        self.update_handler._set_agents([GuestAgent(path=path) for path in self.agent_dirs()])
        self.assertTrue(len(self.update_handler.agents) > 0)
        self.assertEqual(len(self.agent_dirs()), len(self.update_handler.agents))

    def test_set_agents_sorts_agents(self):
        self.prepare_agents()

        self.update_handler._set_agents([GuestAgent(path=path) for path in self.agent_dirs()])

        v = FlexibleVersion(""100000"")
        for a in self.update_handler.agents:
            self.assertTrue(v > a.version)
            v = a.version

    def test_set_sentinel(self):
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))
        self.update_handler._set_sentinel()
        self.assertTrue(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_set_sentinel_writes_current_agent(self):
        self.update_handler._set_sentinel()
        self.assertTrue(
            fileutil.read_file(self.update_handler._sentinel_file_path()),
            CURRENT_AGENT)

    def test_shutdown(self):
        self.update_handler._set_sentinel()
        self.update_handler._shutdown()
        self.assertFalse(self.update_handler.running)
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_shutdown_ignores_missing_sentinel_file(self):
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))
        self.update_handler._shutdown()
        self.assertFalse(self.update_handler.running)
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_shutdown_ignores_exceptions(self):
        self.update_handler._set_sentinel()

        try:
            with patch(""os.remove"", side_effect=Exception):
                self.update_handler._shutdown()
        except Exception as e:
            self.assertTrue(False, ""Unexpected exception"")

    def _test_upgrade_available(
            self,
            base_version=FlexibleVersion(AGENT_VERSION),
            protocol=None,
            versions=None,
            count=20):

        if protocol is None:
            protocol = self._create_protocol(count=count, versions=versions)

        self.update_handler.protocol_util = protocol
        conf.get_autoupdate_gafamily = Mock(return_value=protocol.family)

        return self.update_handler._upgrade_available(base_version=base_version)

    def test_upgrade_available_returns_true_on_first_use(self):
        self.assertTrue(self._test_upgrade_available())

    def test_upgrade_available_will_refresh_goal_state(self):
        protocol = self._create_protocol()
        protocol.emulate_stale_goal_state()
        self.assertTrue(self._test_upgrade_available(protocol=protocol))
        self.assertEqual(2, protocol.call_counts[""get_vmagent_manifests""])
        self.assertEqual(1, protocol.call_counts[""get_vmagent_pkgs""])
        self.assertEqual(1, protocol.call_counts[""update_goal_state""])
        self.assertTrue(protocol.goal_state_forced)

    def test_upgrade_available_handles_missing_family(self):
        extensions_config = ExtensionsConfig(load_data(""wire/ext_conf_missing_family.xml""))
        protocol = ProtocolMock()
        protocol.family = ""Prod""
        protocol.agent_manifests = extensions_config.vmagent_manifests
        self.update_handler.protocol_util = protocol
        with patch('azurelinuxagent.common.logger.warn') as mock_logger:
            with patch('tests.ga.test_update.ProtocolMock.get_vmagent_pkgs', side_effect=ProtocolError):
                self.assertFalse(self.update_handler._upgrade_available(base_version=CURRENT_VERSION))
                self.assertEqual(0, mock_logger.call_count)

    def test_upgrade_available_includes_old_agents(self):
        self.prepare_agents()

        old_version = self.agent_versions()[-1]
        old_count = old_version.version[-1]

        self.replicate_agents(src_v=old_version, count=old_count, increment=-1)
        all_count = len(self.agent_versions())

        self.assertTrue(self._test_upgrade_available(versions=self.agent_versions()))
        self.assertEqual(all_count, len(self.update_handler.agents))

    def test_upgrade_available_purges_old_agents(self):
        self.prepare_agents()
        agent_count = self.agent_count()
        self.assertEqual(20, agent_count)

        agent_versions = self.agent_versions()[:3]
        self.assertTrue(self._test_upgrade_available(versions=agent_versions))
        self.assertEqual(len(agent_versions), len(self.update_handler.agents))

        # Purging always keeps the running agent
        if CURRENT_VERSION not in agent_versions:
            agent_versions.append(CURRENT_VERSION)
        self.assertEqual(agent_versions, self.agent_versions())

    def test_update_available_returns_true_if_current_gets_blacklisted(self):
        self.update_handler._is_version_eligible = Mock(return_value=False)
        self.assertTrue(self._test_upgrade_available())

    def test_upgrade_available_skips_if_too_frequent(self):
        conf.get_autoupdate_frequency = Mock(return_value=10000)
        self.update_handler.last_attempt_time = time.time()
        self.assertFalse(self._test_upgrade_available())

    def test_upgrade_available_skips_if_when_no_new_versions(self):
        self.prepare_agents()
        base_version = self.agent_versions()[0] + 1
        self.update_handler._is_version_eligible = lambda x: x == base_version
        self.assertFalse(self._test_upgrade_available(base_version=base_version))

    def test_upgrade_available_skips_when_no_versions(self):
        self.assertFalse(self._test_upgrade_available(protocol=ProtocolMock()))

    def test_upgrade_available_skips_when_updates_are_disabled(self):
        conf.get_autoupdate_enabled = Mock(return_value=False)
        self.assertFalse(self._test_upgrade_available())

    def test_upgrade_available_sorts(self):
        self.prepare_agents()
        self._test_upgrade_available()

        v = FlexibleVersion(""100000"")
        for a in self.update_handler.agents:
            self.assertTrue(v > a.version)
            v = a.version

    def test_write_pid_file(self):
        for n in range(1112):
            fileutil.write_file(os.path.join(self.tmp_dir, str(n)+""_waagent.pid""), ustr(n+1))
        with patch('os.getpid', return_value=1112):
            pid_files, pid_file = self.update_handler._write_pid_file()
            self.assertEqual(1112, len(pid_files))
            self.assertEqual(""1111_waagent.pid"", os.path.basename(pid_files[-1]))
            self.assertEqual(""1112_waagent.pid"", os.path.basename(pid_file))
            self.assertEqual(fileutil.read_file(pid_file), ustr(1112))

    def test_write_pid_file_ignores_exceptions(self):
        with patch('azurelinuxagent.common.utils.fileutil.write_file', side_effect=Exception):
            with patch('os.getpid', return_value=42):
                pid_files, pid_file = self.update_handler._write_pid_file()
                self.assertEqual(0, len(pid_files))
                self.assertEqual(None, pid_file)

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',
           return_value=GoalState(load_data('wire/goal_state.xml')))
    def test_package_filter_for_agent_manifest(self, _):

        protocol = WireProtocol('12.34.56.78')
        extension_config = ExtensionsConfig(load_data('wire/ext_conf.xml'))
        agent_manifest = extension_config.vmagent_manifests.vmAgentManifests[0]

        # has agent versions 13, 14
        ga_manifest_1 = ExtensionManifest(load_data('wire/ga_manifest_1.xml'))

        # has agent versions 13, 14, 15
        ga_manifest_2 = ExtensionManifest(load_data('wire/ga_manifest_2.xml'))

        goal_state = protocol.client.get_goal_state()
        disk_cache = os.path.join(conf.get_lib_dir(),
                                  AGENTS_MANIFEST_FILE_NAME.format(
                                      agent_manifest.family,
                                      goal_state.incarnation))

        self.assertFalse(os.path.exists(disk_cache))
        self.assertTrue(ga_manifest_1.allowed_versions is None)

        with patch(
                'azurelinuxagent.common.protocol.wire.WireClient'
                '.get_gafamily_manifest',
                return_value=ga_manifest_1):

            pkg_list_1 = protocol.get_vmagent_pkgs(agent_manifest)
            self.assertTrue(pkg_list_1 is not None)
            self.assertTrue(len(pkg_list_1.versions) == 2)
            self.assertTrue(pkg_list_1.versions[0].version == '2.2.13')
            self.assertTrue(pkg_list_1.versions[0].uris[0].uri == 'url1_13')
            self.assertTrue(pkg_list_1.versions[1].version == '2.2.14')
            self.assertTrue(pkg_list_1.versions[1].uris[0].uri == 'url1_14')

        self.assertTrue(os.path.exists(disk_cache))

        with patch(
                'azurelinuxagent.common.protocol.wire.WireClient'
                '.get_gafamily_manifest',
                return_value=ga_manifest_2):

            pkg_list_2 = protocol.get_vmagent_pkgs(agent_manifest)
            self.assertTrue(pkg_list_2 is not None)
            self.assertTrue(len(pkg_list_2.versions) == 2)
            self.assertTrue(pkg_list_2.versions[0].version == '2.2.13')
            self.assertTrue(pkg_list_2.versions[0].uris[0].uri == 'url2_13')
            self.assertTrue(pkg_list_2.versions[1].version == '2.2.14')
            self.assertTrue(pkg_list_2.versions[1].uris[0].uri == 'url2_14')
            # does not contain 2.2.15

        self.assertTrue(os.path.exists(disk_cache))
        self.assertTrue(ga_manifest_2.allowed_versions is not None)
        self.assertTrue(len(ga_manifest_2.allowed_versions) == 2)
        self.assertTrue(ga_manifest_2.allowed_versions[0] == '2.2.13')
        self.assertTrue(ga_manifest_2.allowed_versions[1] == '2.2.14')


class MonitorThreadTest(AgentTestCase):
    def setUp(self):
        AgentTestCase.setUp(self)
        self.event_patch = patch('azurelinuxagent.common.event.add_event')
        self.update_handler = get_update_handler()
        self.update_handler.protocol_util = Mock()

    def _test_run(self, invocations=1):
        iterations = [0]
        def iterator(*args, **kwargs):
            iterations[0] += 1
            if iterations[0] >= invocations:
                self.update_handler.running = False
            return

        with patch('os.getpid', return_value=42):
            with patch.object(UpdateHandler, '_is_orphaned') as mock_is_orphaned:
                mock_is_orphaned.__get__ = Mock(return_value=False)
                with patch('azurelinuxagent.ga.exthandlers.get_exthandlers_handler') as mock_handler:
                    with patch('azurelinuxagent.ga.remoteaccess.get_remote_access_handler') as mock_ra_handler:
                        with patch('time.sleep', side_effect=iterator) as mock_sleep:
                            with patch('sys.exit') as mock_exit:
                                self.update_handler.run()

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_start_threads(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=0)
        self.assertEqual(1, mock_monitor.call_count)
        self.assertEqual(1, mock_monitor_thread.run.call_count)
        self.assertEqual(1, mock_env.call_count)
        self.assertEqual(1, mock_env_thread.run.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_check_if_monitor_thread_is_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor_thread.is_alive = MagicMock(return_value=True)
        mock_monitor_thread.start = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        self._test_run(invocations=0)
        self.assertEqual(1, mock_monitor.call_count)
        self.assertEqual(1, mock_monitor_thread.run.call_count)
        self.assertEqual(1, mock_monitor_thread.is_alive.call_count)
        self.assertEqual(0, mock_monitor_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_check_if_env_thread_is_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env_thread.is_alive = MagicMock(return_value=True)
        mock_env_thread.start = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=1)
        self.assertEqual(1, mock_env.call_count)
        self.assertEqual(1, mock_env_thread.run.call_count)
        self.assertEqual(1, mock_env_thread.is_alive.call_count)
        self.assertEqual(0, mock_env_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_monitor_thread_if_not_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor_thread.is_alive = MagicMock(return_value=False)
        mock_monitor_thread.start = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        self._test_run(invocations=1)
        self.assertEqual(1, mock_monitor.call_count)
        self.assertEqual(1, mock_monitor_thread.run.call_count)
        self.assertEqual(1, mock_monitor_thread.is_alive.call_count)
        self.assertEqual(1, mock_monitor_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_env_thread_if_not_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env_thread.is_alive = MagicMock(return_value=False)
        mock_env_thread.start = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=1)
        self.assertEqual(1, mock_env.call_count)
        self.assertEqual(1, mock_env_thread.run.call_count)
        self.assertEqual(1, mock_env_thread.is_alive.call_count)
        self.assertEqual(1, mock_env_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_monitor_thread(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor_thread.is_alive = MagicMock(return_value=False)
        mock_monitor_thread.start = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        self._test_run(invocations=0)
        self.assertEqual(True, mock_monitor.called)
        self.assertEqual(True, mock_monitor_thread.run.called)
        self.assertEqual(True, mock_monitor_thread.is_alive.called)
        self.assertEqual(True, mock_monitor_thread.start.called)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_env_thread(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env_thread.is_alive = MagicMock(return_value=False)
        mock_env_thread.start = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=0)
        self.assertEqual(True, mock_env.called)
        self.assertEqual(True, mock_env_thread.run.called)
        self.assertEqual(True, mock_env_thread.is_alive.called)
        self.assertEqual(True, mock_env_thread.start.called)


class ChildMock(Mock):
    def __init__(self, return_value=0, side_effect=None):
        Mock.__init__(self, return_value=return_value, side_effect=side_effect)

        self.poll = Mock(return_value=return_value, side_effect=side_effect)
        self.wait = Mock(return_value=return_value, side_effect=side_effect)


class ProtocolMock(object):
    def __init__(self, family=""TestAgent"", etag=42, versions=None, client=None):
        self.family = family
        self.client = client
        self.call_counts = {
            ""get_vmagent_manifests"" : 0,
            ""get_vmagent_pkgs"" : 0,
            ""update_goal_state"" : 0
        }
        self.goal_state_is_stale = False
        self.goal_state_forced = False
        self.etag = etag
        self.versions = versions if versions is not None else []
        self.create_manifests()
        self.create_packages()

    def emulate_stale_goal_state(self):
        self.goal_state_is_stale = True

    def create_manifests(self):
        self.agent_manifests = VMAgentManifestList()
        if len(self.versions) <= 0:
            return

        if self.family is not None:
            manifest = VMAgentManifest(family=self.family)
            for i in range(0,10):
                manifest_uri = ""https://nowhere.msft/agent/{0}"".format(i)
                manifest.versionsManifestUris.append(VMAgentManifestUri(uri=manifest_uri))
            self.agent_manifests.vmAgentManifests.append(manifest)

    def create_packages(self):
        self.agent_packages = ExtHandlerPackageList()
        if len(self.versions) <= 0:
            return

        for version in self.versions:
            package = ExtHandlerPackage(str(version))
            for i in range(0,5):
                package_uri = ""https://nowhere.msft/agent_pkg/{0}"".format(i)
                package.uris.append(ExtHandlerPackageUri(uri=package_uri))
            self.agent_packages.versions.append(package)

    def get_protocol(self):
        return self

    def get_vmagent_manifests(self):
        self.call_counts[""get_vmagent_manifests""] += 1
        if self.goal_state_is_stale:
            self.goal_state_is_stale = False
            raise ResourceGoneError()
        return self.agent_manifests, self.etag

    def get_vmagent_pkgs(self, manifest):
        self.call_counts[""get_vmagent_pkgs""] += 1
        if self.goal_state_is_stale:
            self.goal_state_is_stale = False
            raise ResourceGoneError()
        return self.agent_packages

    def update_goal_state(self, forced=False, max_retry=3):
        self.call_counts[""update_goal_state""] += 1
        self.goal_state_forced = self.goal_state_forced or forced


class ResponseMock(Mock):
    def __init__(self, status=restutil.httpclient.OK, response=None, reason=None):
        Mock.__init__(self)
        self.status = status
        self.reason = reason
        self.response = response

    def read(self):
        return self.response


class TimeMock(Mock):
    def __init__(self, time_increment=1):
        Mock.__init__(self)
        self.next_time = time.time()
        self.time_call_count = 0
        self.time_increment = time_increment

        self.sleep_interval = None

    def sleep(self, n):
        self.sleep_interval = n

    def time(self):
        self.time_call_count += 1
        current_time = self.next_time
        self.next_time += self.time_increment
        return current_time


if __name__ == '__main__':
    unittest.main()
/n/n/ntests/test_import.py/n/nfrom tests.tools import *
import azurelinuxagent.common.osutil as osutil
import azurelinuxagent.common.dhcp as dhcp
import azurelinuxagent.common.protocol as protocol
import azurelinuxagent.pa.provision as provision
import azurelinuxagent.pa.deprovision as deprovision
import azurelinuxagent.daemon as daemon
import azurelinuxagent.daemon.resourcedisk as resourcedisk
import azurelinuxagent.daemon.scvmm as scvmm
import azurelinuxagent.ga.exthandlers as exthandlers
import azurelinuxagent.ga.monitor as monitor
import azurelinuxagent.ga.remoteaccess as remoteaccess
import azurelinuxagent.ga.update as update


class TestImportHandler(AgentTestCase):
    def test_get_handler(self):
        osutil.get_osutil()
        protocol.get_protocol_util()
        dhcp.get_dhcp_handler()
        provision.get_provision_handler()
        deprovision.get_deprovision_handler()
        daemon.get_daemon_handler()
        resourcedisk.get_resourcedisk_handler()
        scvmm.get_scvmm_handler()
        monitor.get_monitor_handler()
        update.get_update_handler()
        exthandlers.get_exthandlers_handler()
        remoteaccess.get_remote_access_handler()
/n/n/ntests/utils/test_crypt_util.py/n/n# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import base64
import binascii
import errno as errno
import glob
import random
import string
import subprocess
import tempfile
import uuid

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.exception import CryptError
from azurelinuxagent.common.version import PY_VERSION_MAJOR
from tests.tools import *
from subprocess import CalledProcessError

class TestCryptoUtilOperations(AgentTestCase):
    def test_decrypt_encrypted_text(self):
        encrypted_string = load_data(""wire/encrypted.enc"")
        prv_key = os.path.join(self.tmp_dir, ""TransportPrivate.pem"") 
        with open(prv_key, 'w+') as c:
            c.write(load_data(""wire/sample.pem""))
        secret = ']aPPEv}uNg1FPnl?'
        crypto = CryptUtil(conf.get_openssl_cmd())
        decrypted_string = crypto.decrypt_secret(encrypted_string, prv_key)
        self.assertEquals(secret, decrypted_string, ""decrypted string does not match expected"")

    def test_decrypt_encrypted_text_missing_private_key(self):
        encrypted_string = load_data(""wire/encrypted.enc"")
        prv_key = os.path.join(self.tmp_dir, ""TransportPrivate.pem"")
        crypto = CryptUtil(conf.get_openssl_cmd())
        self.assertRaises(CalledProcessError, crypto.decrypt_secret, encrypted_string, ""abc"" + prv_key)
    
    def test_decrypt_encrypted_text_wrong_private_key(self):
        encrypted_string = load_data(""wire/encrypted.enc"")
        prv_key = os.path.join(self.tmp_dir, ""wrong.pem"")
        with open(prv_key, 'w+') as c:
            c.write(load_data(""wire/trans_prv""))
        crypto = CryptUtil(conf.get_openssl_cmd())
        self.assertRaises(CalledProcessError, crypto.decrypt_secret, encrypted_string, prv_key)

    def test_decrypt_encrypted_text_text_not_encrypted(self):
        encrypted_string = ""abc@123""
        prv_key = os.path.join(self.tmp_dir, ""TransportPrivate.pem"") 
        with open(prv_key, 'w+') as c:
            c.write(load_data(""wire/sample.pem""))
        crypto = CryptUtil(conf.get_openssl_cmd())
        self.assertRaises(CryptError, crypto.decrypt_secret, encrypted_string, prv_key)

if __name__ == '__main__':
    unittest.main()
/n/n/n",0
41,ee20e7e1058d24191320e54f444a5f7c22adb1e8,"/azurelinuxagent/common/osutil/bigip.py/n/n# Copyright 2016 F5 Networks Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import array
import fcntl
import os
import platform
import re
import socket
import struct
import time

try:
    # WAAgent > 2.1.3
    import azurelinuxagent.common.logger as logger
    import azurelinuxagent.common.utils.shellutil as shellutil

    from azurelinuxagent.common.exception import OSUtilError
    from azurelinuxagent.common.osutil.default import DefaultOSUtil
except ImportError:
    # WAAgent <= 2.1.3
    import azurelinuxagent.logger as logger
    import azurelinuxagent.utils.shellutil as shellutil

    from azurelinuxagent.exception import OSUtilError
    from azurelinuxagent.distro.default.osutil import DefaultOSUtil


class BigIpOSUtil(DefaultOSUtil):
    def __init__(self):
        super(BigIpOSUtil, self).__init__()

    def _wait_until_mcpd_is_initialized(self):
        """"""Wait for mcpd to become available

        All configuration happens in mcpd so we need to wait that this is
        available before we go provisioning the system. I call this method
        at the first opportunity I have (during the DVD mounting call).
        This ensures that the rest of the provisioning does not need to wait
        for mcpd to be available unless it absolutely wants to.

        :return bool: Returns True upon success
        :raises OSUtilError: Raises exception if mcpd does not come up within
                             roughly 50 minutes (100 * 30 seconds)
        """"""
        for retries in range(1, 100):
            # Retry until mcpd completes startup:
            logger.info(""Checking to see if mcpd is up"")
            rc = shellutil.run(""/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running"", chk_err=False)
            if rc == 0:
                logger.info(""mcpd is up!"")
                break
            time.sleep(30)

        if rc is 0:
            return True

        raise OSUtilError(
            ""mcpd hasn't completed initialization! Cannot proceed!""
        )

    def _save_sys_config(self):
        cmd = ""/usr/bin/tmsh save sys config""
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""WARNING: Cannot save sys config on 1st boot."")
        return rc

    def restart_ssh_service(self):
        return shellutil.run(""/usr/bin/bigstart restart sshd"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --add waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --del waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""/sbin/pidof dhclient"")
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        """"""Set the static hostname of the device

        Normally, tmsh is used to set the hostname for the system. For our
        purposes at this time though, I would hesitate to trust this function.

        Azure(Stack) uses the name that you provide in the Web UI or ARM (for
        example) as the value of the hostname argument to this method. The
        problem is that there is nowhere in the UI that specifies the
        restrictions and checks that tmsh has for the hostname.

        For example, if you set the name ""bigip1"" in the Web UI, Azure(Stack)
        considers that a perfectly valid name. When WAAgent gets around to
        running though, tmsh will reject that value because it is not a fully
        qualified domain name. The proper value should have been bigip.xxx.yyy

        WAAgent will not fail if this command fails, but the hostname will not
        be what the user set either. Currently we do not set the hostname when
        WAAgent starts up, so I am passing on setting it here too.

        :param hostname: The hostname to set on the device
        """"""
        return None

    def set_dhcp_hostname(self, hostname):
        """"""Sets the DHCP hostname

        See `set_hostname` for an explanation of why I pass here

        :param hostname: The hostname to set on the device
        """"""
        return None

    def useradd(self, username, expiration=None):
        """"""Create user account using tmsh

        Our policy is to create two accounts when booting a BIG-IP instance.
        The first account is the one that the user specified when they did
        the instance creation. The second one is the admin account that is,
        or should be, built in to the system.

        :param username: The username that you want to add to the system
        :param expiration: The expiration date to use. We do not use this
                           value.
        """"""
        if self.get_userentry(username):
            logger.info(""User {0} already exists, skip useradd"", username)
            return None

        cmd = ""/usr/bin/tmsh create auth user %s partition-access add { all-partitions { role admin } } shell bash"" % (username)
        retcode, out = shellutil.run_get_output(cmd, log_cmd=True, chk_err=True)
        if retcode != 0:
            raise OSUtilError(
                ""Failed to create user account:{0}, retcode:{1}, output:{2}"".format(username, retcode, out)
            )
        self._save_sys_config()
        return retcode

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        """"""Change a user's password with tmsh

        Since we are creating the user specified account and additionally
        changing the password of the built-in 'admin' account, both must
        be modified in this method.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        :param username: The username whose password to change
        :param password: The unencrypted password to set for the user
        :param crypt_id: If encrypting the password, the crypt_id that was used
        :param salt_len: If encrypting the password, the length of the salt
                         value used to do it.
        """"""

        # Start by setting the password of the user provided account
        cmd = ""/usr/bin/tmsh modify auth user {0} password '{1}'"".format(username, password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for {0}: {1}"".format(username, output)
            )

        # Next, set the password of the built-in 'admin' account to be have
        # the same password as the user provided account
        userentry = self.get_userentry('admin')
        if userentry is None:
            raise OSUtilError(""The 'admin' user account was not found!"")

        cmd = ""/usr/bin/tmsh modify auth user 'admin' password '{0}'"".format(password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for 'admin': {0}"".format(output)
            )
        self._save_sys_config()
        return ret

    def del_account(self, username):
        """"""Deletes a user account.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        We also don't use sudo, so we remove that method call as well.

        :param username:
        :return:
        """"""
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""/usr/bin/tmsh delete auth user "" + username)

    def get_dvd_device(self, dev_dir='/dev'):
        """"""Find BIG-IP's CD/DVD device

        This device is almost certainly /dev/cdrom so I added the ? to this pattern.
        Note that this method will return upon the first device found, but in my
        tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the
        correct CD/DVD device though.

        :todo: Consider just always returning ""/dev/cdrom"" here if that device device
               exists on all platforms that are supported on Azure(Stack)
        :param dev_dir: The root directory from which to look for devices
        """"""
        patten = r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)'
        for dvd in [re.match(patten, dev) for dev in os.listdir(dev_dir)]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        raise OSUtilError(""Failed to get dvd device"")

    def mount_dvd(self, **kwargs):
        """"""Mount the DVD containing the provisioningiso.iso file

        This is the _first_ hook that WAAgent provides for us, so this is the
        point where we should wait for mcpd to load. I am just overloading
        this method to add the mcpd wait. Then I proceed with the stock code.

        :param max_retry: Maximum number of retries waagent will make when
                          mounting the provisioningiso.iso DVD
        :param chk_err: Whether to check for errors or not in the mounting
                        commands
        """"""
        self._wait_until_mcpd_is_initialized()
        return super(BigIpOSUtil, self).mount_dvd(**kwargs)

    def eject_dvd(self, chk_err=True):
        """"""Runs the eject command to eject the provisioning DVD

        BIG-IP does not include an eject command. It is sufficient to just
        umount the DVD disk. But I will log that we do not support this for
        future reference.

        :param chk_err: Whether or not to check for errors raised by the eject
                        command
        """"""
        logger.warn(""Eject is not supported on this platform"")

    def get_first_if(self):
        """"""Return the interface name, and ip addr of the management interface.

        We need to add a struct_size check here because, curiously, our 64bit
        platform is identified by python in Azure(Stack) as 32 bit and without
        adjusting the struct_size, we can't get the information we need.

        I believe this may be caused by only python i686 being shipped with
        BIG-IP instead of python x86_64??
        """"""
        iface = ''
        expected = 16  # how many devices should I expect...

        python_arc = platform.architecture()[0]
        if python_arc == '64bit':
            struct_size = 40  # for 64bit the size is 40 bytes
        else:
            struct_size = 32  # for 32bit the size is 32 bytes
        sock = socket.socket(socket.AF_INET,
                             socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        buff = array.array('B', b'\0' * (expected * struct_size))
        param = struct.pack('iL',
                            expected*struct_size,
                            buff.buffer_info()[0])
        ret = fcntl.ioctl(sock.fileno(), 0x8912, param)
        retsize = (struct.unpack('iL', ret)[0])
        if retsize == (expected * struct_size):
            logger.warn(('SIOCGIFCONF returned more than {0} up '
                         'network interfaces.'), expected)
        sock = buff.tostring()
        for i in range(0, struct_size * expected, struct_size):
            iface = self._format_single_interface_name(sock, i)

            # Azure public was returning ""lo:1"" when deploying WAF
            if b'lo' in iface:
                continue
            else:
                break
        return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24])

    def _format_single_interface_name(self, sock, offset):
        return sock[offset:offset+16].split(b'\0', 1)[0]

    def route_add(self, net, mask, gateway):
        """"""Add specified route using tmsh.

        :param net:
        :param mask:
        :param gateway:
        :return:
        """"""
        cmd = (""/usr/bin/tmsh create net route ""
               ""{0}/{1} gw {2}"").format(net, mask, gateway)
        return shellutil.run(cmd, chk_err=False)

    def device_for_ide_port(self, port_id):
        """"""Return device name attached to ide port 'n'.

        Include a wait in here because BIG-IP may not have yet initialized
        this list of devices.

        :param port_id:
        :return:
        """"""
        for retries in range(1, 100):
            # Retry until devices are ready
            if os.path.exists(""/sys/bus/vmbus/devices/""):
                break
            else:
                time.sleep(10)
        return super(BigIpOSUtil, self).device_for_ide_port(port_id)
/n/n/n/azurelinuxagent/common/osutil/freebsd.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+

import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.osutil.default import DefaultOSUtil
from azurelinuxagent.common.future import ustr

class FreeBSDOSUtil(DefaultOSUtil):
    def __init__(self):
        super(FreeBSDOSUtil, self).__init__()
        self._scsi_disks_timeout_set = False

    def set_hostname(self, hostname):
        rc_file_path = '/etc/rc.conf'
        conf_file = fileutil.read_file(rc_file_path).split(""\n"")
        textutil.set_ini_config(conf_file, ""hostname"", hostname)
        fileutil.write_file(rc_file_path, ""\n"".join(conf_file))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run('service sshd restart', chk_err=False)

    def useradd(self, username, expiration=None):
        """"""
        Create user account with 'username'
        """"""
        userentry = self.get_userentry(username)
        if userentry is not None:
            logger.warn(""User {0} already exists, skip useradd"", username)
            return

        if expiration is not None:
            cmd = ""pw useradd {0} -e {1} -m"".format(username, expiration)
        else:
            cmd = ""pw useradd {0} -m"".format(username)
        retcode, out = shellutil.run_get_output(cmd)
        if retcode != 0:
            raise OSUtilError((""Failed to create user account:{0}, ""
                               ""retcode:{1}, ""
                               ""output:{2}"").format(username, retcode, out))

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."", username)
        shellutil.run('> /var/run/utx.active')
        shellutil.run('rmuser -y ' + username)
        self.conf_sudoer(username, remove=True)

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user, ""
                               ""will not set password."").format(username))
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        cmd = ""echo '{0}'|pw usermod {1} -H 0 "".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))

    def del_root_password(self):
        err = shellutil.run('pw usermod root -h -')
        if err:
            raise OSUtilError(""Failed to delete root password: Failed to update password database."")

    def get_if_mac(self, ifname):
        data = self._get_net_info()
        if data[0] == ifname:
            return data[2].replace(':', '').upper()
        return None

    def get_first_if(self):
        return self._get_net_info()[:2]

    def route_add(self, net, mask, gateway):
        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)
        return shellutil.run(cmd, chk_err=False)

    def is_missing_default_route(self):
        """"""
        For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to
        specify the route manually to get it work in a VNET environment.
        SEE ALSO: man ip(4) IP_ONESBCAST,
        """"""
        return True

    def is_dhcp_enabled(self):
        return True

    def start_dhcp_service(self):
        shellutil.run(""/etc/rc.d/dhclient start {0}"".format(self.get_if_name()), chk_err=False)

    def allow_dhcp_broadcast(self):
        pass

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route delete 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pgrep -n dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""cdcontrol -f {0} eject"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject dvd: ret={0}"".format(retcode))

    def restart_if(self, ifname):
        # Restart dhclient only to publish hostname
        shellutil.run(""/etc/rc.d/dhclient restart {0}"".format(ifname), chk_err=False)

    def get_total_mem(self):
        cmd = ""sysctl hw.physmem |awk '{print $2}'""
        ret, output = shellutil.run_get_output(cmd)
        if ret:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))
        try:
            return int(output)/1024/1024
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def get_processor_cores(self):
        ret, output = shellutil.run_get_output(""sysctl hw.ncpu |awk '{print $2}'"")
        if ret:
            raise OSUtilError(""Failed to get processor cores."")

        try:
            return int(output)
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def set_scsi_disks_timeout(self, timeout):
        if self._scsi_disks_timeout_set:
            return

        ret, output = shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout))
        if ret:
            raise OSUtilError(""Failed set SCSI disks timeout: {0}"".format(output))
        self._scsi_disks_timeout_set = True

    def check_pid_alive(self, pid):
        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0

    @staticmethod
    def _get_net_info():
        """"""
        There is no SIOCGIFCONF
        on freeBSD - just parse ifconfig.
        Returns strings: iface, inet4_addr, and mac
        or 'None,None,None' if unable to parse.
        We will sleep and retry as the network must be up.
        """"""
        iface = ''
        inet = ''
        mac = ''

        err, output = shellutil.run_get_output('ifconfig -l ether', chk_err=False)
        if err:
            raise OSUtilError(""Can't find ether interface:{0}"".format(output))
        ifaces = output.split()
        if not ifaces:
            raise OSUtilError(""Can't find ether interface."")
        iface = ifaces[0]

        err, output = shellutil.run_get_output('ifconfig ' + iface, chk_err=False)
        if err:
            raise OSUtilError(""Can't get info for interface:{0}"".format(iface))

        for line in output.split('\n'):
            if line.find('inet ') != -1:
                inet = line.split()[1]
            elif line.find('ether ') != -1:
                mac = line.split()[1]
        logger.verbose(""Interface info: ({0},{1},{2})"", iface, inet, mac)

        return iface, inet, mac

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        if port_id > 3:
            return None
        g0 = ""00000000""
        if port_id > 1:
            g0 = ""00000001""
            port_id = port_id - 2
        err, output = shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=')
        if err:
            return None
        g1 = ""000"" + ustr(port_id)
        g0g1 = ""{0}-{1}"".format(g0, g1)
        """"""
        search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000'
        """"""
        cmd_search_ide = ""sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}"".format(g0g1)
        err, output = shellutil.run_get_output(cmd_search_ide)
        if err:
            return None
        cmd_extract_id = cmd_search_ide + ""|awk -F . '{print $3}'""
        err, output = shellutil.run_get_output(cmd_extract_id)
        """"""
        try to search 'blkvscX' and 'storvscX' to find device name
        """"""
        output = output.rstrip()
        cmd_search_blkvsc = ""camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_blkvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible

        cmd_search_storvsc = ""camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_storvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible
        return None

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        return 0
/n/n/n",1
42,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
43,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
44,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
45,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
46,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
47,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
48,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
49,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
50,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Kster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Kster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Kster""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
51,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
52,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Kster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Kster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Kster""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
53,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
54,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Kster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Kster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Kster""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
55,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
56,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Kster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Kster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Kster""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
57,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
58,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Kster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Kster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Kster""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
59,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
60,bb986000ed3cb222832e1e4535dd6316d32503f8,"tcms/core/ajax.py/n/n# -*- coding: utf-8 -*-
""""""
Shared functions for plan/case/run.

Most of these functions are use for Ajax.
""""""
import datetime
import json
from distutils.util import strtobool

from django import http
from django.db.models import Q, Count
from django.contrib.auth.models import User
from django.core import serializers
from django.core.exceptions import ObjectDoesNotExist
from django.apps import apps
from django.forms import ValidationError
from django.http import Http404
from django.http import HttpResponse
from django.shortcuts import render
from django.views.decorators.http import require_GET
from django.views.decorators.http import require_POST

from tcms.signals import POST_UPDATE_SIGNAL
from tcms.management.models import Component, Build, Version
from tcms.management.models import Priority
from tcms.management.models import Tag
from tcms.management.models import EnvGroup, EnvProperty, EnvValue
from tcms.testcases.models import TestCase, Bug
from tcms.testcases.models import Category
from tcms.testcases.models import TestCaseStatus, TestCaseTag
from tcms.testcases.views import plan_from_request_or_none
from tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag
from tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag
from tcms.core.helpers.comments import add_comment
from tcms.core.utils.validations import validate_bug_id


def check_permission(request, ctype):
    perm = '%s.change_%s' % tuple(ctype.split('.'))
    if request.user.has_perm(perm):
        return True
    return False


def strip_parameters(request_dict, skip_parameters):
    parameters = {}
    for key, value in request_dict.items():
        if key not in skip_parameters and value:
            parameters[str(key)] = value

    return parameters


@require_GET
def info(request):
    """"""Ajax responder for misc information""""""

    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))
    info_type = getattr(objects, request.GET.get('info_type'))

    if not info_type:
        return HttpResponse('Unrecognizable info-type')

    if request.GET.get('format') == 'ulli':
        field = request.GET.get('field', default='name')

        response_str = '<ul>'
        for obj_value in info_type().values(field):
            response_str += '<li>' + obj_value.get(field, None) + '</li>'
        response_str += '</ul>'

        return HttpResponse(response_str)

    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))


class _InfoObjects(object):

    def __init__(self, request, product_id=None):
        self.request = request
        try:
            self.product_id = int(product_id)
        except (ValueError, TypeError):
            self.product_id = 0

    def builds(self):
        try:
            is_active = strtobool(self.request.GET.get('is_active', default='False'))
        except (ValueError, TypeError):
            is_active = False

        return Build.objects.filter(product_id=self.product_id, is_active=is_active)

    def categories(self):
        return Category.objects.filter(product__id=self.product_id)

    def components(self):
        return Component.objects.filter(product__id=self.product_id)

    def env_groups(self):
        return EnvGroup.objects.all()

    def env_properties(self):
        if self.request.GET.get('env_group_id'):
            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()
        return EnvProperty.objects.all()

    def env_values(self):
        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))

    def users(self):
        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))
        return User.objects.filter(**query)

    def versions(self):
        return Version.objects.filter(product__id=self.product_id)


def tags(request):
    """""" Get tags for TestPlan, TestCase or TestRun """"""

    tag_objects = _TagObjects(request)
    template_name, obj = tag_objects.get()

    q_tag = request.GET.get('tags')
    q_action = request.GET.get('a')

    if q_action:
        tag_actions = _TagActions(obj=obj, tag_name=q_tag)
        getattr(tag_actions, q_action)()

    all_tags = obj.tag.all().order_by('pk')
    test_plan_tags = TestPlanTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')
    test_case_tags = TestCaseTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')
    test_run_tags = TestRunTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')

    plan_counter = _TagCounter('num_plans', test_plan_tags)
    case_counter = _TagCounter('num_cases', test_case_tags)
    run_counter = _TagCounter('num_runs', test_run_tags)

    for tag in all_tags:
        tag.num_plans = plan_counter.calculate_tag_count(tag)
        tag.num_cases = case_counter.calculate_tag_count(tag)
        tag.num_runs = run_counter.calculate_tag_count(tag)

    context_data = {
        'tags': all_tags,
        'object': obj,
    }
    return render(request, template_name, context_data)


class _TagObjects(object):
    """""" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database """"""

    def __init__(self, request):
        """"""
        :param request: An HTTP GET request, containing the primary key
                        and the type of object to be selected
        :type request: HttpRequest
        """"""
        for obj in ['plan', 'case', 'run']:
            if request.GET.get(obj):
                self.object = obj
                self.object_pk = request.GET.get(obj)
                break

    def get(self):
        func = getattr(self, self.object)
        return func()

    def plan(self):
        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)

    def case(self):
        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)

    def run(self):
        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)


class _TagActions(object):
    """""" Used for performing the 'add' and 'remove' actions on a given tag """"""

    def __init__(self, obj, tag_name):
        """"""
        :param obj: the object for which the tag actions would be performed
        :type obj: either a :class:`tcms.testplans.models.TestPlan`,
                          a :class:`tcms.testcases.models.TestCase` or
                          a :class:`tcms.testruns.models.TestRun`
        :param tag_name: The name of the tag to be manipulated
        :type tag_name: str
        """"""
        self.obj = obj
        self.tag_name = tag_name

    def add(self):
        tag, _ = Tag.objects.get_or_create(name=self.tag_name)
        self.obj.add_tag(tag)

    def remove(self):
        tag = Tag.objects.get(name=self.tag_name)
        self.obj.remove_tag(tag)


class _TagCounter(object):
    """""" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan """"""

    def __init__(self, key, test_tags):
        """"""
         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count
         :type key: str
         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and
                            annotated by key
            e.g. TestPlanTag, TestCaseTag ot TestRunTag
         :type test_tags: QuerySet
        """"""
        self.key = key
        self.test_tags = iter(test_tags)
        self.counter = {'tag': 0}

    def calculate_tag_count(self, tag):
        """"""
        :param tag: the tag you do the counting for
        :type tag: :class:`tcms.management.models.Tag`
        :return: the number of times a tag is assigned to object
        :rtype: int
        """"""
        if self.counter['tag'] != tag.pk:
            try:
                self.counter = self.test_tags.__next__()
            except StopIteration:
                return 0

        if tag.pk == self.counter['tag']:
            return self.counter[self.key]
        return 0


def get_value_by_type(val, v_type):
    """"""
    Exampls:
    1. get_value_by_type('True', 'bool')
    (1, None)
    2. get_value_by_type('19860624 123059', 'datetime')
    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)
    3. get_value_by_type('5', 'int')
    ('5', None)
    4. get_value_by_type('string', 'str')
    ('string', None)
    5. get_value_by_type('everything', 'None')
    (None, None)
    6. get_value_by_type('buggy', 'buggy')
    (None, 'Unsupported value type.')
    7. get_value_by_type('string', 'int')
    (None, ""invalid literal for int() with base 10: 'string'"")
    """"""
    value = error = None

    def get_time(time):
        date_time = datetime.datetime
        if time == 'NOW':
            return date_time.now()
        return date_time.strptime(time, '%Y%m%d %H%M%S')

    pipes = {
        # Temporary solution is convert all of data to str
        # 'bool': lambda x: x == 'True',
        'bool': lambda x: x == 'True' and 1 or 0,
        'datetime': get_time,
        'int': lambda x: str(int(x)),
        'str': lambda x: str(x),
        'None': lambda x: None,
    }
    pipe = pipes.get(v_type, None)
    if pipe is None:
        error = 'Unsupported value type.'
    else:
        try:
            value = pipe(val)
        except Exception as e:
            error = str(e)
    return value, error


def say_no(error_msg):
    ajax_response = {'rc': 1, 'response': error_msg}
    return HttpResponse(json.dumps(ajax_response))


def say_yes():
    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


# Deprecated. Not flexible.
@require_POST
def update(request):
    """"""
    Generic approach to update a model,\n
    based on contenttype.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), value
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)
    return say_yes()


@require_POST
def update_case_run_status(request):
    """"""
    Update Case Run status.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field {} changed from {} to {}.'.format(
                        field,
                        getattr(t, field),
                        TestCaseRunStatus.id_to_string(value),
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        from tcms.core.utils.mailto import mailto

        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)

    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


class ModelUpdateActions(object):
    """"""Abstract class defining interfaces to update a model properties""""""


class TestCaseUpdateActions(ModelUpdateActions):
    """"""Actions to update each possible proprety of TestCases

    Define your own method named _update_[property name] to hold specific
    update logic.
    """"""

    ctype = 'testcases.testcase'

    def __init__(self, request):
        self.request = request
        self.target_field = request.POST.get('target_field')
        self.new_value = request.POST.get('new_value')

    def get_update_action(self):
        return getattr(self, '_update_%s' % self.target_field, None)

    def update(self):
        has_perms = check_permission(self.request, self.ctype)
        if not has_perms:
            return say_no(""You don't have enough permission to update TestCases."")

        action = self.get_update_action()
        if action is not None:
            try:
                resp = action()
                self._sendmail()
            except ObjectDoesNotExist as err:
                return say_no(str(err))
            except Exception:
                # TODO: besides this message to users, what happening should be
                # recorded in the system log.
                return say_no('Update failed. Please try again or request '
                              'support from your organization.')
            else:
                if resp is None:
                    resp = say_yes()
                return resp
        return say_no('Not know what to update.')

    def get_update_targets(self):
        """"""Get selected cases to update their properties""""""
        case_ids = map(int, self.request.POST.getlist('case'))
        self._update_objects = TestCase.objects.filter(pk__in=case_ids)
        return self._update_objects

    def get_plan(self, pk_enough=True):
        try:
            return plan_from_request_or_none(self.request, pk_enough)
        except Http404:
            return None

    def _sendmail(self):
        mail_context = TestCase.mail_scene(objects=self._update_objects,
                                           field=self.target_field,
                                           value=self.new_value)
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = self.request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    def _update_priority(self):
        exists = Priority.objects.filter(pk=self.new_value).exists()
        if not exists:
            raise ObjectDoesNotExist('The priority you specified to change '
                                     'does not exist.')
        self.get_update_targets().update(**{str(self.target_field): self.new_value})

    def _update_default_tester(self):
        try:
            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))
        except User.DoesNotExist:
            raise ObjectDoesNotExist('Default tester not found!')
        self.get_update_targets().update(**{str(self.target_field): user.pk})

    def _update_case_status(self):
        try:
            new_status = TestCaseStatus.objects.get(pk=self.new_value)
        except TestCaseStatus.DoesNotExist:
            raise ObjectDoesNotExist('The status you choose does not exist.')

        update_object = self.get_update_targets()
        if not update_object:
            return say_no('No record(s) found')

        for testcase in update_object:
            if hasattr(testcase, 'log_action'):
                testcase.log_action(
                    who=self.request.user,
                    action='Field %s changed from %s to %s.' % (
                        self.target_field, testcase.case_status, new_status.name
                    )
                )
        update_object.update(**{str(self.target_field): self.new_value})

        # ###
        # Case is moved between Cases and Reviewing Cases tabs accoding to the
        # change of status. Meanwhile, the number of cases with each status
        # should be updated also.

        try:
            plan = plan_from_request_or_none(self.request)
        except Http404:
            return say_no(""No plan record found."")
        else:
            if plan is None:
                return say_no('No plan record found.')

        confirm_status_name = 'CONFIRMED'
        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)
        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)
        run_case_count = plan.run_case.count()
        case_count = plan.case.count()
        # FIXME: why not calculate review_case_count or run_case_count by using
        # substraction, which saves one SQL query.
        review_case_count = plan.review_case.count()

        return http.JsonResponse({
            'rc': 0, 'response': 'ok',
            'run_case_count': run_case_count,
            'case_count': case_count,
            'review_case_count': review_case_count,
        })

    def _update_sortkey(self):
        try:
            sortkey = int(self.new_value)
            if sortkey < 0 or sortkey > 32300:
                return say_no('New sortkey is out of range [0, 32300].')
        except ValueError:
            return say_no('New sortkey is not an integer.')
        plan = plan_from_request_or_none(self.request, pk_enough=True)
        if plan is None:
            return say_no('No plan record found.')
        update_targets = self.get_update_targets()

        # ##
        # MySQL does not allow to exeucte UPDATE statement that contains
        # subquery querying from same table. In this case, OperationError will
        # be raised.
        offset = 0
        step_length = 500
        queryset_filter = TestCasePlan.objects.filter
        data = {self.target_field: sortkey}
        while 1:
            sub_cases = update_targets[offset:offset + step_length]
            case_pks = [case.pk for case in sub_cases]
            if len(case_pks) == 0:
                break
            queryset_filter(plan=plan, case__in=case_pks).update(**data)
            # Move to next batch of cases to change.
            offset += step_length

    def _update_reviewer(self):
        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)
        if not reviewers:
            err_msg = 'Reviewer %s is not found' % self.new_value
            raise ObjectDoesNotExist(err_msg)
        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})


# NOTE: what permission is necessary
# FIXME: find a good chance to map all TestCase property change request to this
@require_POST
def update_cases_default_tester(request):
    """"""Update default tester upon selected TestCases""""""
    proxy = TestCaseUpdateActions(request)
    return proxy.update()


update_cases_priority = update_cases_default_tester
update_cases_case_status = update_cases_default_tester
update_cases_sortkey = update_cases_default_tester
update_cases_reviewer = update_cases_default_tester


@require_POST
def comment_case_runs(request):
    """"""
    Add comment to one or more caseruns at a time.
    """"""
    data = request.POST.copy()
    comment = data.get('comment', None)
    if not comment:
        return say_no('Comments needed')
    run_ids = [i for i in data.get('run', '').split(',') if i]
    if not run_ids:
        return say_no('No runs selected.')
    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')
    if not runs:
        return say_no('No caserun found.')
    add_comment(runs, comment, request.user)
    return say_yes()


def clean_bug_form(request):
    """"""
    Verify the form data, return a tuple\n
    (None, ERROR_MSG) on failure\n
    or\n
    (data_dict, '') on success.\n
    """"""
    data = {}
    try:
        data['bugs'] = request.GET.get('bug_id', '').split(',')
        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))
    except (TypeError, ValueError) as e:
        return (None, 'Please specify only integers for bugs, '
                      'caseruns(using comma to seperate IDs), '
                      'and bug_system. (DEBUG INFO: %s)' % str(e))

    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))

    if request.GET.get('a') not in ('add', 'remove'):
        return (None, 'Actions only allow ""add"" and ""remove"".')
    else:
        data['action'] = request.GET.get('a')
    data['bz_external_track'] = True if request.GET.get('bz_external_track',
                                                        False) else False

    return (data, '')


def update_bugs_to_caseruns(request):
    """"""
    Add one or more bugs to or remove that from\n
    one or more caserun at a time.
    """"""
    data, error = clean_bug_form(request)
    if error:
        return say_no(error)
    runs = TestCaseRun.objects.filter(pk__in=data['runs'])
    bug_system_id = data['bug_system_id']
    bug_ids = data['bugs']

    try:
        validate_bug_id(bug_ids, bug_system_id)
    except ValidationError as e:
        return say_no(str(e))

    bz_external_track = data['bz_external_track']
    action = data['action']
    try:
        if action == ""add"":
            for run in runs:
                for bug_id in bug_ids:
                    run.add_bug(bug_id=bug_id,
                                bug_system_id=bug_system_id,
                                bz_external_track=bz_external_track)
        else:
            bugs = Bug.objects.filter(bug_id__in=bug_ids)
            for run in runs:
                for bug in bugs:
                    if bug.case_run_id == run.pk:
                        run.remove_bug(bug.bug_id, run.pk)
    except Exception as e:
        return say_no(str(e))
    return say_yes()


def get_prod_related_objs(p_pks, target):
    """"""
    Get Component, Version, Category, and Build\n
    Return [(id, name), (id, name)]
    """"""
    ctypes = {
        'component': (Component, 'name'),
        'version': (Version, 'value'),
        'build': (Build, 'name'),
        'category': (Category, 'name'),
    }
    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)
    attr = ctypes[target][1]
    results = [(r.pk, getattr(r, attr)) for r in results]
    return results


def get_prod_related_obj_json(request):
    """"""
    View for updating product drop-down\n
    in a Ajax way.
    """"""
    data = request.GET.copy()
    target = data.get('target', None)
    p_pks = data.get('p_ids', None)
    sep = data.get('sep', None)
    # py2.6: all(*values) => boolean ANDs
    if target and p_pks and sep:
        p_pks = [k for k in p_pks.split(sep) if k]
        res = get_prod_related_objs(p_pks, target)
    else:
        res = []
    return HttpResponse(json.dumps(res))


def objects_update(objects, **kwargs):
    objects.update(**kwargs)
    kwargs['instances'] = objects
    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(
            'case_run_status', None):
        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)
/n/n/ntcms/core/tests/test_views.py/n/n# -*- coding: utf-8 -*-

import json
from http import HTTPStatus
from urllib.parse import urlencode

from django import test
from django.conf import settings
from django.contrib.contenttypes.models import ContentType
from django.core import serializers
from django.urls import reverse
from django_comments.models import Comment

from tcms.management.models import Priority
from tcms.management.models import EnvGroup
from tcms.management.models import EnvProperty
from tcms.testcases.forms import TestCase
from tcms.testplans.models import TestPlan
from tcms.testruns.models import TestCaseRun
from tcms.testruns.models import TestCaseRunStatus
from tcms.tests import BaseCaseRun
from tcms.tests import BasePlanCase
from tcms.tests import remove_perm_from_user
from tcms.tests import user_should_have_perm
from tcms.tests.factories import UserFactory
from tcms.tests.factories import EnvGroupFactory
from tcms.tests.factories import EnvGroupPropertyMapFactory
from tcms.tests.factories import EnvPropertyFactory


class TestNavigation(test.TestCase):
    @classmethod
    def setUpTestData(cls):
        super(TestNavigation, cls).setUpTestData()
        cls.user = UserFactory(email='user+1@example.com')
        cls.user.set_password('testing')
        cls.user.save()

    def test_urls_for_emails_with_pluses(self):
        # test for https://github.com/Nitrate/Nitrate/issues/262
        # when email contains + sign it needs to be properly urlencoded
        # before passing it as query string argument to the search views
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.user.username,
            password='testing')
        response = self.client.get(reverse('iframe-navigation'))

        self.assertContains(response, urlencode({'people': self.user.email}))
        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))


class TestIndex(BaseCaseRun):
    def test_when_not_logged_in_index_page_redirects_to_login(self):
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-login'),
            target_status_code=HTTPStatus.OK)

    def test_when_logged_in_index_page_redirects_to_dashboard(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-recent', args=[self.tester.username]),
            target_status_code=HTTPStatus.OK)


class TestCommentCaseRuns(BaseCaseRun):
    """"""Test case for ajax.comment_case_runs""""""

    @classmethod
    def setUpTestData(cls):
        super(TestCommentCaseRuns, cls).setUpTestData()
        cls.many_comments_url = reverse('ajax-comment_case_runs')

    def test_refuse_if_missing_comment(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Comments needed'})

    def test_refuse_if_missing_no_case_run_pk(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment', 'run': []})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

    def test_refuse_if_passed_case_run_pks_not_exist(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment',
                                     'run': '99999998,1009900'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No caserun found.'})

    def test_add_comment_to_case_runs(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        new_comment = 'new comment'
        response = self.client.post(
            self.many_comments_url,
            {'comment': new_comment,
             'run': ','.join([str(self.case_run_1.pk),
                              str(self.case_run_2.pk)])})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        # Assert comments are added
        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)

        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):
            comments = Comment.objects.filter(object_pk=case_run_pk,
                                              content_type=case_run_ct)
            self.assertEqual(new_comment, comments[0].comment)
            self.assertEqual(self.tester, comments[0].user)


class TestUpdateObject(BasePlanCase):
    """"""Test case for update""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateObject, cls).setUpTestData()

        cls.permission = 'testplans.change_testplan'
        cls.update_url = reverse('ajax-update')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        remove_perm_from_user(self.tester, self.permission)

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_update_plan_is_active(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        plan = TestPlan.objects.get(pk=self.plan.pk)
        self.assertFalse(plan.is_active)


class TestUpdateCaseRunStatus(BaseCaseRun):
    """"""Test case for update_case_run_status""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCaseRunStatus, cls).setUpTestData()

        cls.permission = 'testruns.change_testcaserun'
        cls.update_url = reverse('ajax-update_case_run_status')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_change_case_run_status(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        self.assertEqual(
            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)


class TestUpdateCasePriority(BasePlanCase):
    """"""Test case for update_cases_default_tester""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCasePriority, cls).setUpTestData()

        cls.permission = 'testcases.change_testcase'
        cls.case_update_url = reverse('ajax-update_cases_default_tester')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': ""You don't have enough permission to ""
                                  ""update TestCases.""})

    def test_update_case_priority(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        for pk in (self.case_1.pk, self.case_3.pk):
            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)


class TestGetObjectInfo(BasePlanCase):
    """"""Test case for info view method""""""

    @classmethod
    def setUpTestData(cls):
        super(TestGetObjectInfo, cls).setUpTestData()

        cls.get_info_url = reverse('ajax-info')

        cls.group_nitrate = EnvGroupFactory(name='nitrate')
        cls.group_new = EnvGroupFactory(name='NewGroup')

        cls.property_os = EnvPropertyFactory(name='os')
        cls.property_python = EnvPropertyFactory(name='python')
        cls.property_django = EnvPropertyFactory(name='django')

        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_os)
        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_python)
        EnvGroupPropertyMapFactory(group=cls.group_new,
                                   property=cls.property_django)

    def test_get_env_properties(self):
        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})

        expected_json = json.loads(
            serializers.serialize(
                'json',
                EnvProperty.objects.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)

    def test_get_env_properties_by_group(self):
        response = self.client.get(self.get_info_url,
                                   {'info_type': 'env_properties',
                                    'env_group_id': self.group_new.pk})

        group = EnvGroup.objects.get(pk=self.group_new.pk)
        expected_json = json.loads(
            serializers.serialize(
                'json',
                group.property.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)
/n/n/ntcms/testcases/tests/test_form_views.py/n/n# -*- coding: utf-8 -*-

from django import test
from django.urls import reverse
from django.conf import settings

from tcms.testcases.forms import CaseAutomatedForm


class TestForm_AutomatedView(test.TestCase):
    def test_get_form(self):
        """"""Verify the view renders the expected HTML""""""
        response = self.client.get(reverse('testcases-form-automated'))
        form = CaseAutomatedForm()
        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())
/n/n/ntcms/testcases/urls/cases_urls.py/n/n# -*- coding: utf-8 -*-

from django.conf.urls import url

from .. import views

urlpatterns = [
    url(r'^new/$', views.new, name='testcases-new'),
    url(r'^$', views.all, name='testcases-all'),
    url(r'^search/$', views.search, name='testcases-search'),
    url(r'^load-more/$', views.load_more_cases),
    url(r'^ajax/$', views.ajax_search, name='testcases-ajax_search'),
    url(r'^form/automated/$', views.form_automated, name='testcases-form-automated'),
    url(r'^automated/$', views.automated, name='testcases-automated'),
    url(r'^component/$', views.component, name='testcases-component'),
    url(r'^category/$', views.category, name='testcases-category'),
    url(r'^clone/$', views.clone, name='testcases-clone'),
    url(r'^printable/$', views.printable, name='testcases-printable'),
    url(r'^export/$', views.export, name='testcases-export'),
]
/n/n/ntcms/testcases/views.py/n/n# -*- coding: utf-8 -*-

import datetime
import json
import itertools

from django.conf import settings
from django.contrib import messages
from django.contrib.auth.decorators import permission_required
from django.contrib.contenttypes.models import ContentType
from django.core.exceptions import ObjectDoesNotExist
from django.urls import reverse
from django.db.models import Count
from django.http import HttpResponseRedirect, HttpResponse, Http404
from django.shortcuts import get_object_or_404, render
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _
from django.views.decorators.http import require_GET
from django.views.decorators.http import require_POST
from django.views.generic.base import TemplateView

from django_comments.models import Comment

from tcms.core.utils import form_errors_to_list
from tcms.core.logs.models import TCMSLogModel
from tcms.core.utils.raw_sql import RawSQL
from tcms.core.utils import DataTableResult
from tcms.search import remove_from_request_path
from tcms.search.order import order_case_queryset
from tcms.testcases import actions
from tcms.testcases import data
from tcms.testcases.models import TestCase, TestCaseStatus, \
    TestCasePlan, BugSystem, \
    Bug, TestCaseText, TestCaseComponent
from tcms.management.models import Priority, Tag
from tcms.testplans.models import TestPlan
from tcms.testruns.models import TestCaseRun
from tcms.testruns.models import TestCaseRunStatus
from tcms.testcases.forms import CaseAutomatedForm, NewCaseForm, \
    SearchCaseForm, EditCaseForm, CaseNotifyForm, \
    CloneCaseForm, CaseBugForm
from tcms.testplans.forms import SearchPlanForm
from tcms.utils.dict_utils import create_dict_from_query
from .fields import CC_LIST_DEFAULT_DELIMITER


TESTCASE_OPERATION_ACTIONS = (
    'search', 'sort', 'update',
    'remove',  # including remove tag from cases
    'add',  # including add tag to cases
    'change',
    'delete_cases',  # unlink cases from a TestPlan
)


# _____________________________________________________________________________
# helper functions


def plan_from_request_or_none(request, pk_enough=False):
    """"""Get TestPlan from REQUEST

    This method relies on the existence of from_plan within REQUEST.

    Arguments:
    - pk_enough: a choice for invoker to determine whether the ID is enough.
    """"""
    tp_id = request.POST.get(""from_plan"") or request.GET.get(""from_plan"")
    if tp_id:
        if pk_enough:
            try:
                tp = int(tp_id)
            except ValueError:
                tp = None
        else:
            tp = get_object_or_404(TestPlan, plan_id=tp_id)
    else:
        tp = None
    return tp


def update_case_email_settings(tc, n_form):
    """"""Update testcase's email settings.""""""

    tc.emailing.notify_on_case_update = n_form.cleaned_data[
        'notify_on_case_update']
    tc.emailing.notify_on_case_delete = n_form.cleaned_data[
        'notify_on_case_delete']
    tc.emailing.auto_to_case_author = n_form.cleaned_data[
        'author']
    tc.emailing.auto_to_case_tester = n_form.cleaned_data[
        'default_tester_of_case']
    tc.emailing.auto_to_run_manager = n_form.cleaned_data[
        'managers_of_runs']
    tc.emailing.auto_to_run_tester = n_form.cleaned_data[
        'default_testers_of_runs']
    tc.emailing.auto_to_case_run_assignee = n_form.cleaned_data[
        'assignees_of_case_runs']
    tc.emailing.save()

    default_tester = n_form.cleaned_data['default_tester_of_case']
    if (default_tester and tc.default_tester_id):
        tc.emailing.auto_to_case_tester = True

    # Continue to update CC list
    valid_emails = n_form.cleaned_data['cc_list']
    tc.emailing.update_cc_list(valid_emails)


def group_case_bugs(bugs):
    """"""Group bugs using bug_id.""""""
    bugs = itertools.groupby(bugs, lambda b: b.bug_id)
    bugs = [(pk, list(_bugs)) for pk, _bugs in bugs]
    return bugs


def create_testcase(request, form, tp):
    """"""Create testcase""""""
    tc = TestCase.create(author=request.user, values=form.cleaned_data)
    tc.add_text(case_text_version=1,
                author=request.user,
                action=form.cleaned_data['action'],
                effect=form.cleaned_data['effect'],
                setup=form.cleaned_data['setup'],
                breakdown=form.cleaned_data['breakdown'])

    # Assign the case to the plan
    if tp:
        tc.add_to_plan(plan=tp)

    # Add components into the case
    for component in form.cleaned_data['component']:
        tc.add_component(component=component)
    return tc


@require_GET
def form_automated(request):
    """"""
        Return HTML for the form which allows changing of automated status.
        Form submission is handled by automated() below.
    """"""
    form = CaseAutomatedForm()
    return HttpResponse(form.as_p())


@require_POST
@permission_required('testcases.change_testcase')
def automated(request):
    """"""Change the automated status for cases

    Parameters:
    - a: Actions
    - case: IDs for case_id
    - o_is_automated: Status for is_automated
    - o_is_automated_proposed: Status for is_automated_proposed

    Returns:
    - Serialized JSON

    """"""
    ajax_response = {'rc': 0, 'response': 'ok'}

    form = CaseAutomatedForm(request.POST)
    if form.is_valid():
        tcs = get_selected_testcases(request)

        if form.cleaned_data['a'] == 'change':
            if isinstance(form.cleaned_data['is_automated'], int):
                # FIXME: inconsistent operation updating automated property
                # upon TestCases. Other place to update property upon
                # TestCase via Model.save, that will trigger model
                #        singal handlers.
                tcs.update(is_automated=form.cleaned_data['is_automated'])
            if isinstance(form.cleaned_data['is_automated_proposed'], bool):
                tcs.update(is_automated_proposed=form.cleaned_data['is_automated_proposed'])
    else:
        ajax_response['rc'] = 1
        ajax_response['response'] = form_errors_to_list(form)

    return HttpResponse(json.dumps(ajax_response))


@permission_required('testcases.add_testcase')
def new(request, template_name='case/new.html'):
    """"""New testcase""""""
    tp = plan_from_request_or_none(request)
    # Initial the form parameters when write new case from plan
    if tp:
        default_form_parameters = {
            'product': tp.product_id,
            'is_automated': '0',
        }
    # Initial the form parameters when write new case directly
    else:
        default_form_parameters = {'is_automated': '0'}

    if request.method == ""POST"":
        form = NewCaseForm(request.POST)
        if request.POST.get('product'):
            form.populate(product_id=request.POST['product'])
        else:
            form.populate()

        if form.is_valid():
            tc = create_testcase(request, form, tp)

            class ReturnActions(object):
                def __init__(self, case, plan):
                    self.__all__ = ('_addanother', '_continue', '_returntocase', '_returntoplan')
                    self.case = case
                    self.plan = plan

                def _continue(self):
                    if self.plan:
                        return HttpResponseRedirect(
                            '%s?from_plan=%s' % (reverse('testcases-edit',
                                                         args=[self.case.case_id]),
                                                 self.plan.plan_id))

                    return HttpResponseRedirect(
                        reverse('testcases-edit', args=[tc.case_id]))

                def _addanother(self):
                    form = NewCaseForm(initial=default_form_parameters)

                    if tp:
                        form.populate(product_id=self.plan.product_id)

                    return form

                def _returntocase(self):
                    if self.plan:
                        return HttpResponseRedirect(
                            '%s?from_plan=%s' % (reverse('testcases-get',
                                                         args=[self.case.pk]),
                                                 self.plan.plan_id))

                    return HttpResponseRedirect(
                        reverse('testcases-get', args=[self.case.pk]))

                def _returntoplan(self):
                    if not self.plan:
                        raise Http404

                    return HttpResponseRedirect(
                        '%s#reviewcases' % reverse('test_plan_url_short',
                                                   args=[self.plan.pk]))

            # Genrate the instance of actions
            ras = ReturnActions(case=tc, plan=tp)
            for ra_str in ras.__all__:
                if request.POST.get(ra_str):
                    func = getattr(ras, ra_str)
                    break
            else:
                func = ras._returntocase

            # Get the function and return back
            result = func()
            if isinstance(result, HttpResponseRedirect):
                return result
            else:
                # Assume here is the form
                form = result

    # Initial NewCaseForm for submit
    else:
        tp = plan_from_request_or_none(request)
        form = NewCaseForm(initial=default_form_parameters)
        if tp:
            form.populate(product_id=tp.product_id)

    context_data = {
        'test_plan': tp,
        'form': form
    }
    return render(request, template_name, context_data)


def get_testcaseplan_sortkey_pk_for_testcases(plan, tc_ids):
    """"""Get each TestCase' sortkey and related TestCasePlan's pk""""""
    qs = TestCasePlan.objects.filter(case__in=tc_ids)
    if plan is not None:
        qs = qs.filter(plan__pk=plan.pk)
    qs = qs.values('pk', 'sortkey', 'case')
    return dict([(item['case'], {
        'testcaseplan_pk': item['pk'],
        'sortkey': item['sortkey']
    }) for item in qs])


def calculate_number_of_bugs_for_testcases(tc_ids):
    """"""Calculate the number of bugs for each TestCase

    Arguments:
    - tc_ids: a list of tuple of TestCases' IDs
    """"""
    qs = Bug.objects.filter(case__in=tc_ids)
    qs = qs.values('case').annotate(total_count=Count('pk'))
    return dict([(item['case'], item['total_count']) for item in qs])


def calculate_for_testcases(plan, testcases):
    """"""Calculate extra data for TestCases

    Attach TestCasePlan.sortkey, TestCasePlan.pk, and the number of bugs of
    each TestCase.

    Arguments:
    - plan: the TestPlan containing searched TestCases. None means testcases
      are not limited to a specific TestPlan.
    - testcases: a queryset of TestCases.
    """"""
    tc_ids = [tc.pk for tc in testcases]
    sortkey_tcpkan_pks = get_testcaseplan_sortkey_pk_for_testcases(
        plan, tc_ids)
    num_bugs = calculate_number_of_bugs_for_testcases(tc_ids)

    # FIXME: strongly recommended to upgrade to Python +2.6
    for tc in testcases:
        data = sortkey_tcpkan_pks.get(tc.pk, None)
        if data:
            setattr(tc, 'cal_sortkey', data['sortkey'])
        else:
            setattr(tc, 'cal_sortkey', None)
        if data:
            setattr(tc, 'cal_testcaseplan_pk', data['testcaseplan_pk'])
        else:
            setattr(tc, 'cal_testcaseplan_pk', None)
        setattr(tc, 'cal_num_bugs', num_bugs.get(tc.pk, None))

    return testcases


def get_case_status(template_type):
    """"""Get part or all TestCaseStatus according to template type""""""
    confirmed_status_name = 'CONFIRMED'
    if template_type == 'case':
        d_status = TestCaseStatus.objects.filter(name=confirmed_status_name)
    elif template_type == 'review_case':
        d_status = TestCaseStatus.objects.exclude(name=confirmed_status_name)
    else:
        d_status = TestCaseStatus.objects.all()
    return d_status


@require_POST
def build_cases_search_form(request, populate=None, plan=None):
    """"""Build search form preparing for quering TestCases""""""
    # Initial the form and template
    action = request.POST.get('a')
    if action in TESTCASE_OPERATION_ACTIONS:
        search_form = SearchCaseForm(request.POST)
        request.session['items_per_page'] = \
            request.POST.get('items_per_page', settings.DEFAULT_PAGE_SIZE)
    else:
        d_status = get_case_status(request.POST.get('template_type'))
        d_status_ids = d_status.values_list('pk', flat=True)
        items_per_page = request.session.get('items_per_page',
                                             settings.DEFAULT_PAGE_SIZE)
        search_form = SearchCaseForm(initial={
            'case_status': d_status_ids,
            'items_per_page': items_per_page})

    if populate:
        if request.POST.get('product'):
            search_form.populate(product_id=request.POST['product'])
        elif plan and plan.product_id:
            search_form.populate(product_id=plan.product_id)
        else:
            search_form.populate()

    return search_form


def paginate_testcases(request, testcases):
    """"""Paginate queried TestCases

    Arguments:
    - request: django's HttpRequest from which to get pagination data
    - testcases: an object queryset representing already queried TestCases

    Return value: return the queryset for chain call
    """"""
    DEFAULT_PAGE_INDEX = 1

    POST = request.POST
    page_index = int(POST.get('page_index', DEFAULT_PAGE_INDEX))
    page_size = int(POST.get('items_per_page',
                             request.session.get('items_per_page',
                                                 settings.DEFAULT_PAGE_SIZE)))
    offset = (page_index - 1) * page_size
    return testcases[offset:offset + page_size]


def sort_queried_testcases(request, testcases):
    """"""Sort querid TestCases according to sort key

    Arguments:
    - request: REQUEST object
    - testcases: object of QuerySet containing queried TestCases
    """"""
    order_by = request.POST.get('order_by', 'create_date')
    asc = bool(request.POST.get('asc', None))
    tcs = order_case_queryset(testcases, order_by, asc)
    # default sorted by sortkey
    tcs = tcs.order_by('testcaseplan__sortkey')
    # Resort the order
    # if sorted by 'sortkey'(foreign key field)
    case_sort_by = request.POST.get('case_sort_by')
    if case_sort_by:
        if case_sort_by not in ['sortkey', '-sortkey']:
            tcs = tcs.order_by(case_sort_by)
        elif case_sort_by == 'sortkey':
            tcs = tcs.order_by('testcaseplan__sortkey')
        else:
            tcs = tcs.order_by('-testcaseplan__sortkey')
    return tcs


def query_testcases_from_request(request, plan=None):
    """"""Query TestCases according to criterias coming within REQUEST

    Arguments:
    - request: the REQUEST object.
    - plan: instance of TestPlan to restrict only those TestCases belongs to
      the TestPlan. Can be None. As you know, query from all TestCases.
    """"""
    search_form = build_cases_search_form(request)

    action = request.POST.get('a')
    if action == 'initial':
        # todo: build_cases_search_form will also check TESTCASE_OPERATION_ACTIONS
        # and return slightly different values in case of initialization
        # move the check there and just execute the query here if the data
        # is valid
        d_status = get_case_status(request.POST.get('template_type'))
        tcs = TestCase.objects.filter(case_status__in=d_status)
    elif action in TESTCASE_OPERATION_ACTIONS and search_form.is_valid():
        tcs = TestCase.list(search_form.cleaned_data, plan)
    else:
        tcs = TestCase.objects.none()

    # Search the relationship
    if plan:
        tcs = tcs.filter(plan=plan)

    tcs = tcs.select_related('author',
                             'default_tester',
                             'case_status',
                             'priority',
                             'category',
                             'reviewer')
    return tcs, search_form


def get_selected_testcases(request):
    """"""Get selected TestCases from client side

    TestCases are selected in two cases. One is user selects part of displayed
    TestCases, where there should be at least one variable named case, whose
    value is the TestCase Id. Another one is user selects all TestCases based
    on previous filter criterias even through there are non-displayed ones. In
    this case, another variable selectAll appears in the REQUEST. Whatever its
    value is.

    If neither variables mentioned exists, empty query result is returned.

    Arguments:
    - request: REQUEST object.
    """"""
    REQ = request.POST or request.GET
    if REQ.get('selectAll', None):
        plan = plan_from_request_or_none(request)
        cases, _search_form = query_testcases_from_request(request, plan)
        return cases
    else:
        pks = [int(pk) for pk in REQ.getlist('case')]
        return TestCase.objects.filter(pk__in=pks)


def load_more_cases(request, template_name='plan/cases_rows.html'):
    """"""Loading more TestCases""""""
    plan = plan_from_request_or_none(request)
    cases = []
    selected_case_ids = []
    if plan is not None:
        cases, _search_form = query_testcases_from_request(request, plan)
        cases = sort_queried_testcases(request, cases)
        cases = paginate_testcases(request, cases)
        cases = calculate_for_testcases(plan, cases)
        selected_case_ids = [tc.pk for tc in cases]
    context_data = {
        'test_plan': plan,
        'test_cases': cases,
        'selected_case_ids': selected_case_ids,
        'case_status': TestCaseStatus.objects.all(),
    }
    return render(request, template_name, context_data)


def get_tags_from_cases(case_ids, plan=None):
    """"""Get all tags from test cases

    @param cases: an iterable object containing test cases' ids
    @type cases: list, tuple

    @param plan: TestPlan object

    @return: a list containing all found tags with id and name
    @rtype: list
    """"""
    query = Tag.objects.filter(case__in=case_ids).distinct().order_by('name')
    if plan:
        query = query.filter(case__plan=plan)

    return query


@require_POST
def all(request):
    """"""
    Generate the TestCase list for the UI tabs in TestPlan page view.

    POST Parameters:
    from_plan: Plan ID
       -- [number]: When the plan ID defined, it will build the case
    page in plan.

    """"""
    # Intial the plan in plan details page
    tp = plan_from_request_or_none(request)
    if not tp:
        messages.add_message(request,
                             messages.ERROR,
                             _('TestPlan not specified or does not exist'))
        return HttpResponseRedirect(reverse('core-views-index'))

    tcs, search_form = query_testcases_from_request(request, tp)
    tcs = sort_queried_testcases(request, tcs)
    total_cases_count = tcs.count()

    # Get the tags own by the cases
    ttags = get_tags_from_cases((case.pk for case in tcs), tp)

    tcs = paginate_testcases(request, tcs)

    # There are several extra information related to each TestCase to be shown
    # also. This step must be the very final one, because the calculation of
    # related data requires related TestCases' IDs, that is the queryset of
    # TestCases should be evaluated in advance.
    tcs = calculate_for_testcases(tp, tcs)

    # generating a query_url with order options
    #
    # FIXME: query_url is always equivlant to None&asc=True whatever what
    # criterias specified in filter form, or just with default filter
    # conditions during loading TestPlan page.
    query_url = remove_from_request_path(request, 'order_by')
    asc = bool(request.POST.get('asc', None))
    if asc:
        query_url = remove_from_request_path(query_url, 'asc')
    else:
        query_url = '%s&asc=True' % query_url

    context_data = {
        'test_cases': tcs,
        'test_plan': tp,
        'search_form': search_form,
        # selected_case_ids is used in template to decide whether or not this TestCase is selected
        'selected_case_ids': [test_case.pk for test_case in get_selected_testcases(request)],
        'case_status': TestCaseStatus.objects.all(),
        'priorities': Priority.objects.all(),
        'case_own_tags': ttags,
        'query_url': query_url,

        # Load more is a POST request, so POST parameters are required only.
        # Remember this for loading more cases with the same as criterias.
        'search_criterias': request.body.decode(),
        'total_cases_count': total_cases_count,
    }
    return render(request, 'plan/get_cases.html', context_data)


@require_GET
def search(request, template_name='case/all.html'):
    """"""
    generate the function of searching cases with search criteria
    """"""
    search_form = SearchCaseForm(request.GET)
    if request.GET.get('product'):
        search_form.populate(product_id=request.GET['product'])
    else:
        search_form.populate()

    context_data = {
        'search_form': search_form,
    }
    return render(request, template_name, context_data)


@require_GET
def ajax_search(request, template_name='case/common/json_cases.txt'):
    """"""Generate the case list in search case and case zone in plan
    """"""
    tp = plan_from_request_or_none(request)

    action = request.GET.get('a')

    # Initial the form and template
    if action in ('search', 'sort'):
        search_form = SearchCaseForm(request.GET)
    else:
        # Hacking for case plan
        confirmed_status_name = 'CONFIRMED'
        # 'c' is meaning component
        template_type = request.GET.get('template_type')
        if template_type == 'case':
            d_status = TestCaseStatus.objects.filter(name=confirmed_status_name)
        elif template_type == 'review_case':
            d_status = TestCaseStatus.objects.exclude(name=confirmed_status_name)
        else:
            d_status = TestCaseStatus.objects.all()

        d_status_ids = d_status.values_list('pk', flat=True)

        search_form = SearchCaseForm(initial={'case_status': d_status_ids})

    # Populate the form
    if request.GET.get('product'):
        search_form.populate(product_id=request.GET['product'])
    elif tp and tp.product_id:
        search_form.populate(product_id=tp.product_id)
    else:
        search_form.populate()

    # Query the database when search
    if action in ('search', 'sort') and search_form.is_valid():
        tcs = TestCase.list(search_form.cleaned_data)
    elif action == 'initial':
        tcs = TestCase.objects.filter(case_status__in=d_status)
    else:
        tcs = TestCase.objects.none()

    # Search the relationship
    if tp:
        tcs = tcs.filter(plan=tp)

    tcs = tcs.select_related(
        'author',
        'default_tester',
        'case_status',
        'priority',
        'category'
    ).only(
        'case_id',
        'summary',
        'create_date',
        'is_automated',
        'is_automated_proposed',
        'case_status__name',
        'category__name',
        'priority__value',
        'author__username',
        'default_tester__id',
        'default_tester__username'
    )
    tcs = tcs.extra(select={'num_bug': RawSQL.num_case_bugs, })

    # columnIndexNameMap is required for correct sorting behavior, 5 should be
    # product, but we use run.build.product
    column_names = [
        '',
        '',
        'case_id',
        'summary',
        'author__username',
        'default_tester__username',
        'is_automated',
        'case_status__name',
        'category__name',
        'priority__value',
        'create_date',
    ]
    return ajax_response(request, tcs, column_names, template_name)


def ajax_response(request, queryset, column_names, template_name):
    """"""json template for the ajax request for searching""""""
    dt = DataTableResult(request.GET, queryset, column_names)

    # todo: prepare the JSON with the response, consider using :
    # from django.template.defaultfilters import escapejs
    json_result = render_to_string(
        template_name,
        dt.get_response_data(),
        request=request)
    return HttpResponse(json_result, content_type='application/json')


class SimpleTestCaseView(TemplateView, data.TestCaseViewDataMixin):
    """"""Simple read-only TestCase View used in TestPlan page""""""

    template_name = 'case/get_details.html'

    # NOTES: what permission is proper for this request?
    def get(self, request, case_id):
        self.case_id = case_id
        self.review_mode = request.GET.get('review_mode')
        return super(SimpleTestCaseView, self).get(request, case_id)

    def get_case(self):
        cases = TestCase.objects.filter(pk=self.case_id).only('notes')
        cases = list(cases.iterator())
        return cases[0] if cases else None

    def get_context_data(self, **kwargs):
        data = super(SimpleTestCaseView, self).get_context_data(**kwargs)

        case = self.get_case()
        data['test_case'] = case
        if case is not None:
            data.update({
                'review_mode': self.review_mode,
                'test_case_text': case.latest_text(),
                'logs': self.get_case_logs(case),
                'components': case.component.only('name'),
                'tags': case.tag.only('name'),
                'case_comments': self.get_case_comments(case),
            })

        return data


class TestCaseCaseRunListPaneView(TemplateView):
    """"""Display case runs list when expand a plan from case page""""""

    template_name = 'case/get_case_runs_by_plan.html'

    # FIXME: what permission here?
    def get(self, request, case_id):
        self.case_id = case_id

        plan_id = self.request.GET.get('plan_id', None)
        self.plan_id = int(plan_id) if plan_id is not None else None

        this_cls = TestCaseCaseRunListPaneView
        return super(this_cls, self).get(request, case_id)

    def get_case_runs(self):
        qs = TestCaseRun.objects.filter(case=self.case_id,
                                        run__plan=self.plan_id)
        qs = qs.values(
            'pk', 'case_id', 'run_id', 'case_text_version',
            'close_date', 'sortkey',
            'tested_by__username', 'assignee__username',
            'run__plan_id', 'run__summary',
            'case__category__name', 'case__priority__value',
            'case_run_status__name',
        ).order_by('pk')
        return qs

    def get_comments_count(self, caserun_ids):
        ct = ContentType.objects.get_for_model(TestCaseRun)
        qs = Comment.objects.filter(content_type=ct,
                                    object_pk__in=caserun_ids,
                                    site_id=settings.SITE_ID,
                                    is_removed=False)
        qs = qs.values('object_pk').annotate(comment_count=Count('pk'))
        result = {}
        for item in qs.iterator():
            result[int(item['object_pk'])] = item['comment_count']
        return result

    def get_context_data(self, **kwargs):
        this_cls = TestCaseCaseRunListPaneView
        data = super(this_cls, self).get_context_data(**kwargs)

        case_runs = self.get_case_runs()

        # Get the number of each caserun's comments, and put the count into
        # comments query result.
        caserun_ids = [item['pk'] for item in case_runs]
        comments_count = self.get_comments_count(caserun_ids)
        for case_run in case_runs:
            case_run['comments_count'] = comments_count.get(case_run['pk'], 0)

        data.update({
            'case_runs': case_runs,
        })
        return data


class TestCaseSimpleCaseRunView(TemplateView, data.TestCaseRunViewDataMixin):
    """"""Display caserun information in Case Runs tab in case page

    This view only shows notes, comments and logs simply. So, call it simple.
    """"""

    template_name = 'case/get_details_case_case_run.html'

    # what permission here?
    def get(self, request, case_id):
        try:
            self.caserun_id = int(request.GET.get('case_run_id', None))
        except (TypeError, ValueError):
            raise Http404

        this_cls = TestCaseSimpleCaseRunView
        return super(this_cls, self).get(request, case_id)

    def get_caserun(self):
        try:
            return TestCaseRun.objects.filter(
                pk=self.caserun_id).only('notes')[0]
        except IndexError:
            raise Http404

    def get_context_data(self, **kwargs):
        this_cls = TestCaseSimpleCaseRunView
        data = super(this_cls, self).get_context_data(**kwargs)

        caserun = self.get_caserun()
        logs = self.get_case_run_logs(caserun)
        comments = self.get_case_run_comments(caserun)

        data.update({
            'test_caserun': caserun,
            'logs': logs.iterator(),
            'comments': comments.iterator(),
        })
        return data


class TestCaseCaseRunDetailPanelView(TemplateView,
                                     data.TestCaseViewDataMixin,
                                     data.TestCaseRunViewDataMixin):
    """"""Display case run detail in run page""""""

    template_name = 'case/get_details_case_run.html'

    def get(self, request, case_id):
        self.case_id = case_id
        try:
            self.caserun_id = int(request.GET.get('case_run_id'))
            self.case_text_version = int(request.GET.get('case_text_version'))
        except (TypeError, ValueError):
            raise Http404

        this_cls = TestCaseCaseRunDetailPanelView
        return super(this_cls, self).get(request, case_id)

    def get_context_data(self, **kwargs):
        this_cls = TestCaseCaseRunDetailPanelView
        data = super(this_cls, self).get_context_data(**kwargs)

        try:
            qs = TestCase.objects.filter(pk=self.case_id)
            qs = qs.prefetch_related('component',
                                     'tag').only('pk')
            case = qs[0]

            qs = TestCaseRun.objects.filter(pk=self.caserun_id).order_by('pk')
            case_run = qs[0]
        except IndexError:
            raise Http404

        # Data of TestCase
        test_case_text = case.get_text_with_version(self.case_text_version)

        # Data of TestCaseRun
        caserun_comments = self.get_case_run_comments(case_run)
        caserun_logs = self.get_case_run_logs(case_run)

        caserun_status = TestCaseRunStatus.objects.values('pk', 'name')
        caserun_status = caserun_status.order_by('sortkey')
        bugs = group_case_bugs(case_run.case.get_bugs().order_by('bug_id'))

        data.update({
            'test_case': case,
            'test_case_text': test_case_text,

            'test_case_run': case_run,
            'comments_count': len(caserun_comments),
            'caserun_comments': caserun_comments,
            'caserun_logs': caserun_logs,
            'test_case_run_status': caserun_status,
            'grouped_case_bugs': bugs,
        })

        return data


def get(request, case_id):
    """"""Get the case content""""""
    # Get the case
    try:
        tc = TestCase.objects.select_related(
            'author', 'default_tester',
            'category', 'category',
            'priority', 'case_status').get(case_id=case_id)
    except ObjectDoesNotExist:
        raise Http404

    # Get the test plans
    tps = tc.plan.select_related('author', 'product', 'type').all()

    # log
    log_id = str(case_id)
    logs = TCMSLogModel.get_logs_for_model(TestCase, log_id)

    logs = itertools.groupby(logs, lambda l: l.date)
    logs = [(day, list(log_actions)) for day, log_actions in logs]
    try:
        tp = tps.get(pk=request.GET.get('from_plan', 0))
    except (TestPlan.DoesNotExist, ValueError):
        # ValueError is raised when from_plan is empty string
        # not viewing TC from a Plan or specified Plan does not exist (e.g. broken link)
        tp = None

    # Get the test case runs
    tcrs = tc.case_run.select_related(
        'run', 'tested_by',
        'assignee', 'case',
        'case', 'case_run_status').all()
    tcrs = tcrs.extra(select={
        'num_bug': RawSQL.num_case_run_bugs,
    }).order_by('run__plan')
    runs_ordered_by_plan = itertools.groupby(tcrs, lambda t: t.run.plan)
    # FIXME: Just don't know why Django template does not evaluate a generator,
    # and had to evaluate the groupby generator manually like below.
    runs_ordered_by_plan = [(k, list(v)) for k, v in runs_ordered_by_plan]
    case_run_plans = [k for k, v in runs_ordered_by_plan]
    # Get the specific test case run
    if request.GET.get('case_run_id'):
        tcr = tcrs.get(pk=request.GET['case_run_id'])
    else:
        tcr = None
    case_run_plan_id = request.GET.get('case_run_plan_id', None)
    if case_run_plan_id:
        for item in runs_ordered_by_plan:
            if item[0].pk == int(case_run_plan_id):
                case_runs_by_plan = item[1]
                break
            else:
                continue
    else:
        case_runs_by_plan = None

    # Get the case texts
    tc_text = tc.get_text_with_version(request.GET.get('case_text_version'))

    grouped_case_bugs = tcr and group_case_bugs(tcr.case.get_bugs())
    # Render the page
    context_data = {
        'logs': logs,
        'test_case': tc,
        'test_plan': tp,
        'test_plans': tps,
        'test_case_runs': tcrs,
        'case_run_plans': case_run_plans,
        'test_case_runs_by_plan': case_runs_by_plan,
        'test_case_run': tcr,
        'grouped_case_bugs': grouped_case_bugs,
        'test_case_text': tc_text,
        'test_case_status': TestCaseStatus.objects.all(),
        'test_case_run_status': TestCaseRunStatus.objects.all(),
        'bug_trackers': BugSystem.objects.all(),
    }
    return render(request, 'case/get.html', context_data)


@require_POST
def printable(request, template_name='case/printable.html'):
    """"""
        Create the printable copy for plan/case.
        Only CONFIRMED TestCases are printed when printing a TestPlan!
    """"""
    # search only by case PK. Used when printing selected cases
    case_ids = request.POST.getlist('case')
    case_filter = {'case__in': case_ids}

    test_plan = None
    # plan_pk is passed from the TestPlan.printable function
    # but it doesn't pass IDs of individual cases to be printed
    if not case_ids:
        plan_pk = request.POST.get('plan', 0)
        try:
            test_plan = TestPlan.objects.get(pk=plan_pk)
            # search cases from a TestPlan, used when printing entire plan
            case_filter = {
                'case__plan': plan_pk,
                'case__case_status': TestCaseStatus.objects.get(name='CONFIRMED').pk,
            }
        except (ValueError, TestPlan.DoesNotExist):
            test_plan = None

    tcs = create_dict_from_query(
        TestCaseText.objects.filter(**case_filter).values(
            'case_id', 'case__summary', 'setup', 'action', 'effect', 'breakdown'
        ).order_by('case_id', '-case_text_version'),
        'case_id',
        True
    )

    context_data = {
        'test_plan': test_plan,
        'test_cases': tcs,
    }
    return render(request, template_name, context_data)


@require_POST
def export(request, template_name='case/export.xml'):
    """"""Export the plan""""""
    case_pks = request.POST.getlist('case')
    context_data = {
        'data_generator': generator_proxy(case_pks),
    }

    response = render(request, template_name, context_data)

    response['Content-Disposition'] = \
        'attachment; filename=tcms-testcases-%s.xml' % datetime.datetime.now().strftime('%Y-%m-%d')
    return response


def generator_proxy(case_pks):
    metas = TestCase.objects.filter(
        pk__in=case_pks
    ).exclude(
        case_status__name='DISABLED'
    ).values(
        'case_id', 'summary', 'is_automated', 'notes',
        'priority__value', 'case_status__name',
        'author__email', 'default_tester__email',
        'category__name')

    component_dict = create_dict_from_query(
        TestCaseComponent.objects.filter(
            case__in=case_pks
        ).values(
            'case_id', 'component_id', 'component__name', 'component__product__name'
        ).order_by('case_id'),
        'case_id'
    )

    tag_dict = create_dict_from_query(
        TestCase.objects.filter(
            pk__in=case_pks
        ).values('case_id', 'tag__name').order_by('case_id'),
        'case_id'
    )

    plan_text_dict = create_dict_from_query(
        TestCaseText.objects.filter(
            case__in=case_pks
        ).values(
            'case_id', 'setup', 'action', 'effect', 'breakdown'
        ).order_by('case_id', '-case_text_version'),
        'case_id',
        True
    )

    for meta in metas:
        case_id = meta['case_id']
        c_meta = component_dict.get(case_id, None)
        if c_meta:
            meta['c_meta'] = c_meta

        tag = tag_dict.get(case_id, None)
        if tag:
            meta['tag'] = tag

        plan_text = plan_text_dict.get(case_id, None)
        if plan_text:
            meta['latest_text'] = plan_text

        yield meta


def update_testcase(request, tc, tc_form):
    """"""Updating information of specific TestCase

    This is called by views.edit internally. Don't call this directly.

    Arguments:
    - tc: instance of a TestCase being updated
    - tc_form: instance of django.forms.Form, holding validated data.
    """"""

    # Modify the contents
    fields = ['summary',
              'case_status',
              'category',
              'priority',
              'notes',
              'is_automated',
              'is_automated_proposed',
              'script',
              'arguments',
              'extra_link',
              'requirement',
              'alias']

    for field in fields:
        if getattr(tc, field) != tc_form.cleaned_data[field]:
            tc.log_action(request.user,
                          'Case %s changed from %s to %s in edit page.' % (
                              field, getattr(tc, field),
                              tc_form.cleaned_data[field]
                          ))
            setattr(tc, field, tc_form.cleaned_data[field])
    try:
        if tc.default_tester != tc_form.cleaned_data['default_tester']:
            tc.log_action(
                request.user,
                'Case default tester changed from %s to %s in edit page.' % (
                    tc.default_tester_id and tc.default_tester,
                    tc_form.cleaned_data['default_tester']
                ))
            tc.default_tester = tc_form.cleaned_data['default_tester']
    except ObjectDoesNotExist:
        pass
    tc.update_tags(tc_form.cleaned_data.get('tag'))
    try:
        fields_text = ['action', 'effect', 'setup', 'breakdown']
        latest_text = tc.latest_text()

        for field in fields_text:
            form_cleaned = tc_form.cleaned_data[field]
            if not (getattr(latest_text, field) or form_cleaned):
                continue
            if getattr(latest_text, field) != form_cleaned:
                tc.log_action(
                    request.user,
                    ' Case %s changed from %s to %s in edit page.' % (
                        field, getattr(latest_text, field) or None,
                        form_cleaned or None
                    ))
    except ObjectDoesNotExist:
        pass

    # FIXME: Bug here, timedelta from form cleaned data need to convert.
    tc.estimated_time = tc_form.cleaned_data['estimated_time']
    # IMPORTANT! tc.current_user is an instance attribute,
    # added so that in post_save, current logged-in user info
    # can be accessed.
    # Instance attribute is usually not a desirable solution.
    tc.current_user = request.user
    tc.save()


@permission_required('testcases.change_testcase')
def edit(request, case_id, template_name='case/edit.html'):
    """"""Edit case detail""""""
    try:
        tc = TestCase.objects.select_related().get(case_id=case_id)
    except ObjectDoesNotExist:
        raise Http404

    tp = plan_from_request_or_none(request)

    if request.method == ""POST"":
        form = EditCaseForm(request.POST)
        if request.POST.get('product'):
            form.populate(product_id=request.POST['product'])
        elif tp:
            form.populate(product_id=tp.product_id)
        else:
            form.populate()

        n_form = CaseNotifyForm(request.POST)

        if form.is_valid() and n_form.is_valid():

            update_testcase(request, tc, form)

            tc.add_text(author=request.user,
                        action=form.cleaned_data['action'],
                        effect=form.cleaned_data['effect'],
                        setup=form.cleaned_data['setup'],
                        breakdown=form.cleaned_data['breakdown'])

            # Notification
            update_case_email_settings(tc, n_form)

            # Returns
            if request.POST.get('_continue'):
                return HttpResponseRedirect('%s?from_plan=%s' % (
                    reverse('testcases-edit', args=[case_id, ]),
                    request.POST.get('from_plan', None),
                ))

            if request.POST.get('_continuenext'):
                if not tp:
                    raise Http404

                # find out test case list which belong to the same
                # classification
                confirm_status_name = 'CONFIRMED'
                if tc.case_status.name == confirm_status_name:
                    pk_list = tp.case.filter(
                        case_status__name=confirm_status_name)
                else:
                    pk_list = tp.case.exclude(
                        case_status__name=confirm_status_name)
                pk_list = list(pk_list.defer('case_id').values_list('pk', flat=True))
                pk_list.sort()

                # Get the previous and next case
                p_tc, n_tc = tc.get_previous_and_next(pk_list=pk_list)
                return HttpResponseRedirect('%s?from_plan=%s' % (
                    reverse('testcases-edit', args=[n_tc.pk, ]),
                    tp.pk,
                ))

            if request.POST.get('_returntoplan'):
                if not tp:
                    raise Http404
                confirm_status_name = 'CONFIRMED'
                if tc.case_status.name == confirm_status_name:
                    return HttpResponseRedirect('%s#testcases' % (
                        reverse('test_plan_url_short', args=[tp.pk, ]),
                    ))
                else:
                    return HttpResponseRedirect('%s#reviewcases' % (
                        reverse('test_plan_url_short', args=[tp.pk, ]),
                    ))

            return HttpResponseRedirect('%s?from_plan=%s' % (
                reverse('testcases-get', args=[case_id, ]),
                request.POST.get('from_plan', None),
            ))

    else:
        tctxt = tc.latest_text()
        # Notification form initial
        n_form = CaseNotifyForm(initial={
            'notify_on_case_update': tc.emailing.notify_on_case_update,
            'notify_on_case_delete': tc.emailing.notify_on_case_delete,
            'author': tc.emailing.auto_to_case_author,
            'default_tester_of_case': tc.emailing.auto_to_case_tester,
            'managers_of_runs': tc.emailing.auto_to_run_manager,
            'default_testers_of_runs': tc.emailing.auto_to_run_tester,
            'assignees_of_case_runs': tc.emailing.auto_to_case_run_assignee,
            'cc_list': CC_LIST_DEFAULT_DELIMITER.join(
                tc.emailing.get_cc_list()),
        })
        default_tester = tc.default_tester_id and tc.default_tester.\
            email or None
        form = EditCaseForm(initial={
            'summary': tc.summary,
            'default_tester': default_tester,
            'requirement': tc.requirement,
            'is_automated': tc.get_is_automated_form_value(),
            'is_automated_proposed': tc.is_automated_proposed,
            'script': tc.script,
            'arguments': tc.arguments,
            'extra_link': tc.extra_link,
            'alias': tc.alias,
            'case_status': tc.case_status_id,
            'priority': tc.priority_id,
            'product': tc.category.product_id,
            'category': tc.category_id,
            'notes': tc.notes,
            'component': [c.pk for c in tc.component.all()],
            'estimated_time': tc.estimated_time,
            'setup': tctxt.setup,
            'action': tctxt.action,
            'effect': tctxt.effect,
            'breakdown': tctxt.breakdown,
            'tag': ','.join(tc.tag.values_list('name', flat=True)),
        })

        form.populate(product_id=tc.category.product_id)

    context_data = {
        'test_case': tc,
        'test_plan': tp,
        'form': form,
        'notify_form': n_form,
    }
    return render(request, template_name, context_data)


def text_history(request, case_id, template_name='case/history.html'):
    """"""View test plan text history""""""

    tc = get_object_or_404(TestCase, case_id=case_id)
    tp = plan_from_request_or_none(request)
    tctxts = tc.text.values('case_id',
                            'case_text_version',
                            'author__email',
                            'create_date').order_by('-case_text_version')

    context_data = {
        'testplan': tp,
        'testcase': tc,
        'test_case_texts': tctxts.iterator(),
    }

    try:
        case_text_version = int(request.GET.get('case_text_version'))
        text_to_show = tc.text.filter(case_text_version=case_text_version)
        text_to_show = text_to_show.values('action',
                                           'effect',
                                           'setup',
                                           'breakdown')

        context_data.update({
            'select_case_text_version': case_text_version,
            'text_to_show': text_to_show.iterator(),
        })
    except (TypeError, ValueError):
        # If case_text_version is not a valid number, no text to display for a
        # selected text history
        pass

    return render(request, template_name, context_data)


@permission_required('testcases.add_testcase')
def clone(request, template_name='case/clone.html'):
    """"""Clone one case or multiple case into other plan or plans""""""

    request_data = getattr(request, request.method)

    if 'selectAll' not in request_data and 'case' not in request_data:
        messages.add_message(request,
                             messages.ERROR,
                             _('At least one TestCase is required'))
        # redirect back where we came from
        return HttpResponseRedirect(request.META.get('HTTP_REFERER', '/'))

    tp_src = plan_from_request_or_none(request)
    tp = None
    search_plan_form = SearchPlanForm()

    # Do the clone action
    if request.method == 'POST':
        clone_form = CloneCaseForm(request.POST)
        clone_form.populate(case_ids=request.POST.getlist('case'))

        if clone_form.is_valid():
            tcs_src = clone_form.cleaned_data['case']
            for tc_src in tcs_src:
                if clone_form.cleaned_data['copy_case']:
                    tc_dest = TestCase.objects.create(
                        is_automated=tc_src.is_automated,
                        is_automated_proposed=tc_src.is_automated_proposed,
                        script=tc_src.script,
                        arguments=tc_src.arguments,
                        extra_link=tc_src.extra_link,
                        summary=tc_src.summary,
                        requirement=tc_src.requirement,
                        alias=tc_src.alias,
                        estimated_time=tc_src.estimated_time,
                        case_status=TestCaseStatus.get_PROPOSED(),
                        category=tc_src.category,
                        priority=tc_src.priority,
                        notes=tc_src.notes,
                        author=clone_form.cleaned_data[
                            'maintain_case_orignal_author'] and
                        tc_src.author or request.user,
                        default_tester=clone_form.cleaned_data[
                            'maintain_case_orignal_default_tester'] and
                        tc_src.author or request.user,
                    )

                    for tp in clone_form.cleaned_data['plan']:
                        # copy a case and keep origin case's sortkey
                        if tp_src:
                            try:
                                tcp = TestCasePlan.objects.get(plan=tp_src,
                                                               case=tc_src)
                                sortkey = tcp.sortkey
                            except ObjectDoesNotExist:
                                sortkey = tp.get_case_sortkey()
                        else:
                            sortkey = tp.get_case_sortkey()

                        tp.add_case(tc_dest, sortkey)

                    tc_dest.add_text(
                        author=clone_form.cleaned_data[
                            'maintain_case_orignal_author'] and
                        tc_src.author or request.user,
                        create_date=tc_src.latest_text().create_date,
                        action=tc_src.latest_text().action,
                        effect=tc_src.latest_text().effect,
                        setup=tc_src.latest_text().setup,
                        breakdown=tc_src.latest_text().breakdown
                    )

                    for tag in tc_src.tag.all():
                        tc_dest.add_tag(tag=tag)
                else:
                    tc_dest = tc_src
                    tc_dest.author = \
                        clone_form.cleaned_data[
                            'maintain_case_orignal_author'] \
                        and tc_src.author or request.user
                    tc_dest.default_tester = \
                        clone_form.cleaned_data[
                            'maintain_case_orignal_default_tester'] \
                        and tc_src.author or request.user
                    tc_dest.save()
                    for tp in clone_form.cleaned_data['plan']:
                        # create case link and keep origin plan's sortkey
                        if tp_src:
                            try:
                                tcp = TestCasePlan.objects.get(plan=tp_src,
                                                               case=tc_dest)
                                sortkey = tcp.sortkey
                            except ObjectDoesNotExist:
                                sortkey = tp.get_case_sortkey()
                        else:
                            sortkey = tp.get_case_sortkey()

                        tp.add_case(tc_dest, sortkey)

                # Add the cases to plan
                for tp in clone_form.cleaned_data['plan']:
                    # Clone the categories to new product
                    if clone_form.cleaned_data['copy_case']:
                        try:
                            tc_category = tp.product.category.get(
                                name=tc_src.category.name
                            )
                        except ObjectDoesNotExist:
                            tc_category = tp.product.category.create(
                                name=tc_src.category.name,
                                description=tc_src.category.description,
                            )

                        tc_dest.category = tc_category
                        tc_dest.save()
                        del tc_category

                    # Clone the components to new product
                    if clone_form.cleaned_data['copy_component'] and \
                            clone_form.cleaned_data['copy_case']:
                        for component in tc_src.component.all():
                            try:
                                new_c = tp.product.component.get(
                                    name=component.name
                                )
                            except ObjectDoesNotExist:
                                new_c = tp.product.component.create(
                                    name=component.name,
                                    initial_owner=request.user,
                                    description=component.description,
                                )

                            tc_dest.add_component(new_c)

            # Detect the number of items and redirect to correct one
            cases_count = len(clone_form.cleaned_data['case'])
            plans_count = len(clone_form.cleaned_data['plan'])

            if cases_count == 1 and plans_count == 1:
                return HttpResponseRedirect('%s?from_plan=%s' % (
                    reverse('testcases-get', args=[tc_dest.pk, ]),
                    tp.pk
                ))

            if cases_count == 1:
                return HttpResponseRedirect(
                    reverse('testcases-get', args=[tc_dest.pk, ])
                )

            if plans_count == 1:
                return HttpResponseRedirect(
                    reverse('test_plan_url_short', args=[tp.pk, ])
                )

            # Otherwise it will prompt to user the clone action is successful.
            messages.add_message(request,
                                 messages.SUCCESS,
                                 _('TestCase cloning was successful'))
            return HttpResponseRedirect(reverse('plans-all'))
    else:
        selected_cases = get_selected_testcases(request)
        # Initial the clone case form
        clone_form = CloneCaseForm(initial={
            'case': selected_cases,
            'copy_case': False,
            'maintain_case_orignal_author': False,
            'maintain_case_orignal_default_tester': False,
            'copy_component': True,
        })
        clone_form.populate(case_ids=selected_cases)

    # Generate search plan form
    if request_data.get('from_plan'):
        tp = TestPlan.objects.get(plan_id=request_data['from_plan'])
        search_plan_form = SearchPlanForm(
            initial={'product': tp.product_id, 'is_active': True})
        search_plan_form.populate(product_id=tp.product_id)

    submit_action = request_data.get('submit', None)
    context_data = {
        'test_plan': tp,
        'search_form': search_plan_form,
        'clone_form': clone_form,
        'submit_action': submit_action,
    }
    return render(request, template_name, context_data)


@require_POST
@permission_required('testcases.add_testcasecomponent')
def component(request):
    """"""
    Management test case components
    """"""
    # FIXME: It will update product/category/component at one time so far.
    # We may disconnect the component from case product in future.
    cas = actions.ComponentActions(request)
    action = request.POST.get('a', 'render_form')
    func = getattr(cas, action.lower())
    return func()


@require_POST
@permission_required('testcases.add_testcasecomponent')
def category(request):
    """"""Management test case categories""""""
    # FIXME: It will update product/category/component at one time so far.
    # We may disconnect the component from case product in future.
    cas = actions.CategoryActions(request)
    func = getattr(cas, request.POST.get('a', 'render_form').lower())
    return func()


@permission_required('testcases.add_testcaseattachment')
def attachment(request, case_id, template_name='case/attachment.html'):
    """"""Manage test case attachments""""""

    tc = get_object_or_404(TestCase, case_id=case_id)
    tp = plan_from_request_or_none(request)

    context_data = {
        'testplan': tp,
        'testcase': tc,
        'limit': settings.FILE_UPLOAD_MAX_SIZE,
    }
    return render(request, template_name, context_data)


def get_log(request, case_id, template_name=""management/get_log.html""):
    """"""Get the case log""""""
    tc = get_object_or_404(TestCase, case_id=case_id)

    context_data = {
        'object': tc
    }
    return render(request, template_name, context_data)


@permission_required('testcases.change_bug')
def bug(request, case_id, template_name='case/get_bug.html'):
    """"""Process the bugs for cases""""""
    # FIXME: Rewrite these codes for Ajax.Request
    tc = get_object_or_404(TestCase, case_id=case_id)

    class CaseBugActions(object):
        __all__ = ['get_form', 'render', 'add', 'remove']

        def __init__(self, request, case, template_name):
            self.request = request
            self.case = case
            self.template_name = template_name

        def render_form(self):
            form = CaseBugForm(initial={
                'case': self.case,
            })
            if request.GET.get('type') == 'table':
                return HttpResponse(form.as_table())

            return HttpResponse(form.as_p())

        def render(self, response=None):
            context_data = {
                'test_case': self.case,
                'response': response
            }
            return render(request, template_name, context_data)

        def add(self):
            # FIXME: It's may use ModelForm.save() method here.
            #        Maybe in future.
            if not self.request.user.has_perm('testcases.add_bug'):
                return self.render(response='Permission denied.')

            form = CaseBugForm(request.GET)
            if not form.is_valid():
                errors = []
                for field_name, error_messages in form.errors.items():
                    for item in error_messages:
                        errors.append(item)
                response = '\n'.join(errors)
                return self.render(response=response)

            try:
                self.case.add_bug(
                    bug_id=form.cleaned_data['bug_id'],
                    bug_system_id=form.cleaned_data['bug_system'].pk,
                    summary=form.cleaned_data['summary'],
                    description=form.cleaned_data['description'],
                )
            except Exception as e:
                return self.render(response=str(e))

            return self.render()

        def remove(self):
            if not request.user.has_perm('testcases.delete_bug'):
                return self.render(response='Permission denied.')

            try:
                self.case.remove_bug(request.GET.get('id'), request.GET.get('run_id'))
            except ObjectDoesNotExist as error:
                return self.render(response=error)

            return self.render()

    case_bug_actions = CaseBugActions(
        request=request,
        case=tc,
        template_name=template_name
    )

    if not request.GET.get('handle') in case_bug_actions.__all__:
        return case_bug_actions.render(response='Unrecognizable actions')

    func = getattr(case_bug_actions, request.GET['handle'])
    return func()


@require_GET
def plan(request, case_id):
    """"""Add and remove plan in plan tab""""""
    tc = get_object_or_404(TestCase, case_id=case_id)
    if request.GET.get('a'):
        # Search the plans from database
        if not request.GET.getlist('plan_id'):
            context_data = {
                'message': 'The case must specific one plan at leaset for '
                           'some action',
            }
            return render(
                request,
                'case/get_plan.html',
                context_data)

        tps = TestPlan.objects.filter(pk__in=request.GET.getlist('plan_id'))

        if not tps:
            context_data = {
                'testplans': tps,
                'message': 'The plan id are not exist in database at all.'
            }
            return render(
                request,
                'case/get_plan.html',
                context_data)

        # Add case plan action
        if request.GET['a'] == 'add':
            if not request.user.has_perm('testcases.add_testcaseplan'):
                context_data = {
                    'test_case': tc,
                    'test_plans': tps,
                    'message': 'Permission denied',
                }
                return render(
                    request,
                    'case/get_plan.html',
                    context_data)

            for tp in tps:
                tc.add_to_plan(tp)

        # Remove case plan action
        if request.GET['a'] == 'remove':
            if not request.user.has_perm('testcases.change_testcaseplan'):
                context_data = {
                    'test_case': tc,
                    'test_plans': tps,
                    'message': 'Permission denied',
                }
                return render(
                    request,
                    'case/get_plan.html',
                    context_data)

            for tp in tps:
                tc.remove_plan(tp)

    tps = tc.plan.all()
    tps = tps.select_related('author',
                             'type',
                             'product')

    context_data = {
        'test_case': tc,
        'test_plans': tps,
    }
    return render(
        request,
        'case/get_plan.html',
        context_data)
/n/n/ntcms/urls.py/n/n# -*- coding: utf-8 -*-

from django.conf import settings
from django.conf.urls import include, url
from django.conf.urls.static import static
from django.contrib import admin
from django.views.i18n import JavaScriptCatalog

from grappelli import urls as grappelli_urls
from attachments import urls as attachments_urls
from modernrpc.core import JSONRPC_PROTOCOL
from modernrpc.core import XMLRPC_PROTOCOL
from modernrpc.views import RPCEntryPoint
from tinymce import urls as tinymce_urls
from tcms.core import ajax
from tcms.core import views as core_views
from tcms.core.contrib.comments import views as comments_views
from tcms.core.contrib.linkreference import views as linkreference_views
from tcms.profiles import urls as profiles_urls
from tcms.testplans import urls as testplans_urls
from tcms.testcases import urls as testcases_urls
from tcms.testruns import urls as testruns_urls
from tcms.testruns import views as testruns_views
from tcms.management import views as management_views
from tcms.report import urls as report_urls
from tcms.search import advance_search


urlpatterns = [
    # iframe navigation workaround
    url(r'^navigation/', core_views.navigation, name='iframe-navigation'),

    url(r'^grappelli/', include(grappelli_urls)),
    url(r'^admin/', admin.site.urls),

    url(r'^attachments/', include(attachments_urls, namespace='attachments')),
    url(r'^tinymce/', include(tinymce_urls)),

    # Index and static zone
    url(r'^$', core_views.index, name='core-views-index'),
    url(r'^xml-rpc/', RPCEntryPoint.as_view(protocol=XMLRPC_PROTOCOL), name='xml-rpc'),
    url(r'^json-rpc/$', RPCEntryPoint.as_view(protocol=JSONRPC_PROTOCOL)),

    # Ajax call responder
    url(r'^ajax/update/$', ajax.update, name='ajax-update'),
    url(r'^ajax/update/case-status/$', ajax.update_cases_case_status),
    url(r'^ajax/update/case-run-status$', ajax.update_case_run_status,
        name='ajax-update_case_run_status'),
    url(r'^ajax/update/cases-priority/$', ajax.update_cases_priority),
    url(r'^ajax/update/cases-default-tester/$', ajax.update_cases_default_tester,
        name='ajax-update_cases_default_tester'),
    url(r'^ajax/update/cases-reviewer/$', ajax.update_cases_reviewer),
    url(r'^ajax/update/cases-sortkey/$', ajax.update_cases_sortkey),
    url(r'^ajax/get-prod-relate-obj/$', ajax.get_prod_related_obj_json),
    url(r'^management/getinfo/$', ajax.info, name='ajax-info'),
    url(r'^management/tags/$', ajax.tags, name='ajax-tags'),

    # comments
    url(r'^comments/post/', comments_views.post, name='comments-post'),
    url(r'^comments/delete/', comments_views.delete, name='comments-delete'),

    # Account information zone, such as login method
    url(r'^accounts/', include(profiles_urls)),

    # Testplans zone
    url(r'^plan/', include(testplans_urls.plan_urls)),
    url(r'^plans/', include(testplans_urls.plans_urls)),

    # Testcases zone
    url(r'^case/', include(testcases_urls.case_urls)),
    url(r'^cases/', include(testcases_urls.cases_urls)),

    # Testruns zone
    url(r'^run/', include(testruns_urls.run_urls)),
    url(r'^runs/', include(testruns_urls.runs_urls)),

    url(r'^caseruns/$', testruns_views.caseruns),
    url(r'^caserun/(?P<case_run_id>\d+)/bug/$', testruns_views.bug, name='testruns-bug'),
    url(r'^caserun/comment-many/', ajax.comment_case_runs, name='ajax-comment_case_runs'),
    url(r'^caserun/update-bugs-for-many/', ajax.update_bugs_to_caseruns),

    url(r'^linkref/add/$', linkreference_views.add, name='linkref-add'),
    url(r'^linkref/remove/(?P<link_id>\d+)/$', linkreference_views.remove),

    # Management zone
    url(r'^environment/groups/$', management_views.environment_groups,
        name='mgmt-environment_groups'),
    url(r'^environment/group/edit/$', management_views.environment_group_edit,
        name='mgmt-environment_group_edit'),
    url(r'^environment/properties/$', management_views.environment_properties,
        name='mgmt-environment_properties'),
    url(r'^environment/properties/values/$', management_views.environment_property_values,
        name='mgmt-environment_property_values'),

    # Report zone
    url(r'^report/', include(report_urls)),

    # Advance search
    url(r'^advance-search/$', advance_search, name='advance_search'),

    # TODO: do we need this at all ???
    # Using admin js without admin permission
    # https://docs.djangoproject.com/en/1.11/topics/i18n/translation/#django.views.i18n.JavaScriptCatalog
    url(r'^jsi18n/$', JavaScriptCatalog.as_view()),
]

# Debug zone

if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)

    try:
        import debug_toolbar

        urlpatterns += [
            url(r'^__debug__/', include(debug_toolbar.urls)),
        ]
    # in case we're trying to debug in production
    # and debug_toolbar is not installed
    except ImportError:
        pass

# Overwrite default 500 handler
# More details could see django.core.urlresolvers._resolve_special()
handler500 = 'tcms.core.views.error.server_error'
/n/n/n",0
61,bb986000ed3cb222832e1e4535dd6316d32503f8,"/tcms/core/ajax.py/n/n# -*- coding: utf-8 -*-
""""""
Shared functions for plan/case/run.

Most of these functions are use for Ajax.
""""""
import datetime
import sys
import json
from distutils.util import strtobool

from django import http
from django.db.models import Q, Count
from django.contrib.auth.models import User
from django.core import serializers
from django.core.exceptions import ObjectDoesNotExist
from django.apps import apps
from django.forms import ValidationError
from django.http import Http404
from django.http import HttpResponse
from django.shortcuts import render
from django.views.decorators.http import require_GET
from django.views.decorators.http import require_POST

from tcms.signals import POST_UPDATE_SIGNAL
from tcms.management.models import Component, Build, Version
from tcms.management.models import Priority
from tcms.management.models import Tag
from tcms.management.models import EnvGroup, EnvProperty, EnvValue
from tcms.testcases.models import TestCase, Bug
from tcms.testcases.models import Category
from tcms.testcases.models import TestCaseStatus, TestCaseTag
from tcms.testcases.views import plan_from_request_or_none
from tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag
from tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag
from tcms.core.helpers.comments import add_comment
from tcms.core.utils.validations import validate_bug_id


def check_permission(request, ctype):
    perm = '%s.change_%s' % tuple(ctype.split('.'))
    if request.user.has_perm(perm):
        return True
    return False


def strip_parameters(request_dict, skip_parameters):
    parameters = {}
    for key, value in request_dict.items():
        if key not in skip_parameters and value:
            parameters[str(key)] = value

    return parameters


@require_GET
def info(request):
    """"""Ajax responder for misc information""""""

    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))
    info_type = getattr(objects, request.GET.get('info_type'))

    if not info_type:
        return HttpResponse('Unrecognizable info-type')

    if request.GET.get('format') == 'ulli':
        field = request.GET.get('field', default='name')

        response_str = '<ul>'
        for obj_value in info_type().values(field):
            response_str += '<li>' + obj_value.get(field, None) + '</li>'
        response_str += '</ul>'

        return HttpResponse(response_str)

    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))


class _InfoObjects(object):

    def __init__(self, request, product_id=None):
        self.request = request
        try:
            self.product_id = int(product_id)
        except (ValueError, TypeError):
            self.product_id = 0

    def builds(self):
        try:
            is_active = strtobool(self.request.GET.get('is_active', default='False'))
        except (ValueError, TypeError):
            is_active = False

        return Build.objects.filter(product_id=self.product_id, is_active=is_active)

    def categories(self):
        return Category.objects.filter(product__id=self.product_id)

    def components(self):
        return Component.objects.filter(product__id=self.product_id)

    def env_groups(self):
        return EnvGroup.objects.all()

    def env_properties(self):
        if self.request.GET.get('env_group_id'):
            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()
        return EnvProperty.objects.all()

    def env_values(self):
        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))

    def users(self):
        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))
        return User.objects.filter(**query)

    def versions(self):
        return Version.objects.filter(product__id=self.product_id)


@require_GET
def form(request):
    """"""Response get form ajax call, most using in dialog""""""

    # The parameters in internal_parameters will delete from parameters
    internal_parameters = ['app_form', 'format']
    parameters = strip_parameters(request.GET, internal_parameters)
    q_app_form = request.GET.get('app_form')
    q_format = request.GET.get('format')
    if not q_format:
        q_format = 'p'

    if not q_app_form:
        return HttpResponse('Unrecognizable app_form')

    # Get the form
    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]
    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))
    __import__('tcms.%s.forms' % q_app)
    q_app_module = sys.modules['tcms.%s.forms' % q_app]
    form_class = getattr(q_app_module, q_form)
    form_params = form_class(initial=parameters)

    # Generate the HTML and reponse
    html = getattr(form_params, 'as_' + q_format)
    return HttpResponse(html())


def tags(request):
    """""" Get tags for TestPlan, TestCase or TestRun """"""

    tag_objects = _TagObjects(request)
    template_name, obj = tag_objects.get()

    q_tag = request.GET.get('tags')
    q_action = request.GET.get('a')

    if q_action:
        tag_actions = _TagActions(obj=obj, tag_name=q_tag)
        getattr(tag_actions, q_action)()

    all_tags = obj.tag.all().order_by('pk')
    test_plan_tags = TestPlanTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')
    test_case_tags = TestCaseTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')
    test_run_tags = TestRunTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')

    plan_counter = _TagCounter('num_plans', test_plan_tags)
    case_counter = _TagCounter('num_cases', test_case_tags)
    run_counter = _TagCounter('num_runs', test_run_tags)

    for tag in all_tags:
        tag.num_plans = plan_counter.calculate_tag_count(tag)
        tag.num_cases = case_counter.calculate_tag_count(tag)
        tag.num_runs = run_counter.calculate_tag_count(tag)

    context_data = {
        'tags': all_tags,
        'object': obj,
    }
    return render(request, template_name, context_data)


class _TagObjects(object):
    """""" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database """"""

    def __init__(self, request):
        """"""
        :param request: An HTTP GET request, containing the primary key
                        and the type of object to be selected
        :type request: HttpRequest
        """"""
        for obj in ['plan', 'case', 'run']:
            if request.GET.get(obj):
                self.object = obj
                self.object_pk = request.GET.get(obj)
                break

    def get(self):
        func = getattr(self, self.object)
        return func()

    def plan(self):
        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)

    def case(self):
        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)

    def run(self):
        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)


class _TagActions(object):
    """""" Used for performing the 'add' and 'remove' actions on a given tag """"""

    def __init__(self, obj, tag_name):
        """"""
        :param obj: the object for which the tag actions would be performed
        :type obj: either a :class:`tcms.testplans.models.TestPlan`,
                          a :class:`tcms.testcases.models.TestCase` or
                          a :class:`tcms.testruns.models.TestRun`
        :param tag_name: The name of the tag to be manipulated
        :type tag_name: str
        """"""
        self.obj = obj
        self.tag_name = tag_name

    def add(self):
        tag, _ = Tag.objects.get_or_create(name=self.tag_name)
        self.obj.add_tag(tag)

    def remove(self):
        tag = Tag.objects.get(name=self.tag_name)
        self.obj.remove_tag(tag)


class _TagCounter(object):
    """""" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan """"""

    def __init__(self, key, test_tags):
        """"""
         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count
         :type key: str
         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and
                            annotated by key
            e.g. TestPlanTag, TestCaseTag ot TestRunTag
         :type test_tags: QuerySet
        """"""
        self.key = key
        self.test_tags = iter(test_tags)
        self.counter = {'tag': 0}

    def calculate_tag_count(self, tag):
        """"""
        :param tag: the tag you do the counting for
        :type tag: :class:`tcms.management.models.Tag`
        :return: the number of times a tag is assigned to object
        :rtype: int
        """"""
        if self.counter['tag'] != tag.pk:
            try:
                self.counter = self.test_tags.__next__()
            except StopIteration:
                return 0

        if tag.pk == self.counter['tag']:
            return self.counter[self.key]
        return 0


def get_value_by_type(val, v_type):
    """"""
    Exampls:
    1. get_value_by_type('True', 'bool')
    (1, None)
    2. get_value_by_type('19860624 123059', 'datetime')
    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)
    3. get_value_by_type('5', 'int')
    ('5', None)
    4. get_value_by_type('string', 'str')
    ('string', None)
    5. get_value_by_type('everything', 'None')
    (None, None)
    6. get_value_by_type('buggy', 'buggy')
    (None, 'Unsupported value type.')
    7. get_value_by_type('string', 'int')
    (None, ""invalid literal for int() with base 10: 'string'"")
    """"""
    value = error = None

    def get_time(time):
        date_time = datetime.datetime
        if time == 'NOW':
            return date_time.now()
        return date_time.strptime(time, '%Y%m%d %H%M%S')

    pipes = {
        # Temporary solution is convert all of data to str
        # 'bool': lambda x: x == 'True',
        'bool': lambda x: x == 'True' and 1 or 0,
        'datetime': get_time,
        'int': lambda x: str(int(x)),
        'str': lambda x: str(x),
        'None': lambda x: None,
    }
    pipe = pipes.get(v_type, None)
    if pipe is None:
        error = 'Unsupported value type.'
    else:
        try:
            value = pipe(val)
        except Exception as e:
            error = str(e)
    return value, error


def say_no(error_msg):
    ajax_response = {'rc': 1, 'response': error_msg}
    return HttpResponse(json.dumps(ajax_response))


def say_yes():
    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


# Deprecated. Not flexible.
@require_POST
def update(request):
    """"""
    Generic approach to update a model,\n
    based on contenttype.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), value
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)
    return say_yes()


@require_POST
def update_case_run_status(request):
    """"""
    Update Case Run status.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field {} changed from {} to {}.'.format(
                        field,
                        getattr(t, field),
                        TestCaseRunStatus.id_to_string(value),
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        from tcms.core.utils.mailto import mailto

        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)

    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


class ModelUpdateActions(object):
    """"""Abstract class defining interfaces to update a model properties""""""


class TestCaseUpdateActions(ModelUpdateActions):
    """"""Actions to update each possible proprety of TestCases

    Define your own method named _update_[property name] to hold specific
    update logic.
    """"""

    ctype = 'testcases.testcase'

    def __init__(self, request):
        self.request = request
        self.target_field = request.POST.get('target_field')
        self.new_value = request.POST.get('new_value')

    def get_update_action(self):
        return getattr(self, '_update_%s' % self.target_field, None)

    def update(self):
        has_perms = check_permission(self.request, self.ctype)
        if not has_perms:
            return say_no(""You don't have enough permission to update TestCases."")

        action = self.get_update_action()
        if action is not None:
            try:
                resp = action()
                self._sendmail()
            except ObjectDoesNotExist as err:
                return say_no(str(err))
            except Exception:
                # TODO: besides this message to users, what happening should be
                # recorded in the system log.
                return say_no('Update failed. Please try again or request '
                              'support from your organization.')
            else:
                if resp is None:
                    resp = say_yes()
                return resp
        return say_no('Not know what to update.')

    def get_update_targets(self):
        """"""Get selected cases to update their properties""""""
        case_ids = map(int, self.request.POST.getlist('case'))
        self._update_objects = TestCase.objects.filter(pk__in=case_ids)
        return self._update_objects

    def get_plan(self, pk_enough=True):
        try:
            return plan_from_request_or_none(self.request, pk_enough)
        except Http404:
            return None

    def _sendmail(self):
        mail_context = TestCase.mail_scene(objects=self._update_objects,
                                           field=self.target_field,
                                           value=self.new_value)
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = self.request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    def _update_priority(self):
        exists = Priority.objects.filter(pk=self.new_value).exists()
        if not exists:
            raise ObjectDoesNotExist('The priority you specified to change '
                                     'does not exist.')
        self.get_update_targets().update(**{str(self.target_field): self.new_value})

    def _update_default_tester(self):
        try:
            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))
        except User.DoesNotExist:
            raise ObjectDoesNotExist('Default tester not found!')
        self.get_update_targets().update(**{str(self.target_field): user.pk})

    def _update_case_status(self):
        try:
            new_status = TestCaseStatus.objects.get(pk=self.new_value)
        except TestCaseStatus.DoesNotExist:
            raise ObjectDoesNotExist('The status you choose does not exist.')

        update_object = self.get_update_targets()
        if not update_object:
            return say_no('No record(s) found')

        for testcase in update_object:
            if hasattr(testcase, 'log_action'):
                testcase.log_action(
                    who=self.request.user,
                    action='Field %s changed from %s to %s.' % (
                        self.target_field, testcase.case_status, new_status.name
                    )
                )
        update_object.update(**{str(self.target_field): self.new_value})

        # ###
        # Case is moved between Cases and Reviewing Cases tabs accoding to the
        # change of status. Meanwhile, the number of cases with each status
        # should be updated also.

        try:
            plan = plan_from_request_or_none(self.request)
        except Http404:
            return say_no(""No plan record found."")
        else:
            if plan is None:
                return say_no('No plan record found.')

        confirm_status_name = 'CONFIRMED'
        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)
        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)
        run_case_count = plan.run_case.count()
        case_count = plan.case.count()
        # FIXME: why not calculate review_case_count or run_case_count by using
        # substraction, which saves one SQL query.
        review_case_count = plan.review_case.count()

        return http.JsonResponse({
            'rc': 0, 'response': 'ok',
            'run_case_count': run_case_count,
            'case_count': case_count,
            'review_case_count': review_case_count,
        })

    def _update_sortkey(self):
        try:
            sortkey = int(self.new_value)
            if sortkey < 0 or sortkey > 32300:
                return say_no('New sortkey is out of range [0, 32300].')
        except ValueError:
            return say_no('New sortkey is not an integer.')
        plan = plan_from_request_or_none(self.request, pk_enough=True)
        if plan is None:
            return say_no('No plan record found.')
        update_targets = self.get_update_targets()

        # ##
        # MySQL does not allow to exeucte UPDATE statement that contains
        # subquery querying from same table. In this case, OperationError will
        # be raised.
        offset = 0
        step_length = 500
        queryset_filter = TestCasePlan.objects.filter
        data = {self.target_field: sortkey}
        while 1:
            sub_cases = update_targets[offset:offset + step_length]
            case_pks = [case.pk for case in sub_cases]
            if len(case_pks) == 0:
                break
            queryset_filter(plan=plan, case__in=case_pks).update(**data)
            # Move to next batch of cases to change.
            offset += step_length

    def _update_reviewer(self):
        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)
        if not reviewers:
            err_msg = 'Reviewer %s is not found' % self.new_value
            raise ObjectDoesNotExist(err_msg)
        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})


# NOTE: what permission is necessary
# FIXME: find a good chance to map all TestCase property change request to this
@require_POST
def update_cases_default_tester(request):
    """"""Update default tester upon selected TestCases""""""
    proxy = TestCaseUpdateActions(request)
    return proxy.update()


update_cases_priority = update_cases_default_tester
update_cases_case_status = update_cases_default_tester
update_cases_sortkey = update_cases_default_tester
update_cases_reviewer = update_cases_default_tester


@require_POST
def comment_case_runs(request):
    """"""
    Add comment to one or more caseruns at a time.
    """"""
    data = request.POST.copy()
    comment = data.get('comment', None)
    if not comment:
        return say_no('Comments needed')
    run_ids = [i for i in data.get('run', '').split(',') if i]
    if not run_ids:
        return say_no('No runs selected.')
    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')
    if not runs:
        return say_no('No caserun found.')
    add_comment(runs, comment, request.user)
    return say_yes()


def clean_bug_form(request):
    """"""
    Verify the form data, return a tuple\n
    (None, ERROR_MSG) on failure\n
    or\n
    (data_dict, '') on success.\n
    """"""
    data = {}
    try:
        data['bugs'] = request.GET.get('bug_id', '').split(',')
        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))
    except (TypeError, ValueError) as e:
        return (None, 'Please specify only integers for bugs, '
                      'caseruns(using comma to seperate IDs), '
                      'and bug_system. (DEBUG INFO: %s)' % str(e))

    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))

    if request.GET.get('a') not in ('add', 'remove'):
        return (None, 'Actions only allow ""add"" and ""remove"".')
    else:
        data['action'] = request.GET.get('a')
    data['bz_external_track'] = True if request.GET.get('bz_external_track',
                                                        False) else False

    return (data, '')


def update_bugs_to_caseruns(request):
    """"""
    Add one or more bugs to or remove that from\n
    one or more caserun at a time.
    """"""
    data, error = clean_bug_form(request)
    if error:
        return say_no(error)
    runs = TestCaseRun.objects.filter(pk__in=data['runs'])
    bug_system_id = data['bug_system_id']
    bug_ids = data['bugs']

    try:
        validate_bug_id(bug_ids, bug_system_id)
    except ValidationError as e:
        return say_no(str(e))

    bz_external_track = data['bz_external_track']
    action = data['action']
    try:
        if action == ""add"":
            for run in runs:
                for bug_id in bug_ids:
                    run.add_bug(bug_id=bug_id,
                                bug_system_id=bug_system_id,
                                bz_external_track=bz_external_track)
        else:
            bugs = Bug.objects.filter(bug_id__in=bug_ids)
            for run in runs:
                for bug in bugs:
                    if bug.case_run_id == run.pk:
                        run.remove_bug(bug.bug_id, run.pk)
    except Exception as e:
        return say_no(str(e))
    return say_yes()


def get_prod_related_objs(p_pks, target):
    """"""
    Get Component, Version, Category, and Build\n
    Return [(id, name), (id, name)]
    """"""
    ctypes = {
        'component': (Component, 'name'),
        'version': (Version, 'value'),
        'build': (Build, 'name'),
        'category': (Category, 'name'),
    }
    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)
    attr = ctypes[target][1]
    results = [(r.pk, getattr(r, attr)) for r in results]
    return results


def get_prod_related_obj_json(request):
    """"""
    View for updating product drop-down\n
    in a Ajax way.
    """"""
    data = request.GET.copy()
    target = data.get('target', None)
    p_pks = data.get('p_ids', None)
    sep = data.get('sep', None)
    # py2.6: all(*values) => boolean ANDs
    if target and p_pks and sep:
        p_pks = [k for k in p_pks.split(sep) if k]
        res = get_prod_related_objs(p_pks, target)
    else:
        res = []
    return HttpResponse(json.dumps(res))


def objects_update(objects, **kwargs):
    objects.update(**kwargs)
    kwargs['instances'] = objects
    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(
            'case_run_status', None):
        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)
/n/n/n/tcms/core/tests/test_views.py/n/n# -*- coding: utf-8 -*-

import json
from http import HTTPStatus
from urllib.parse import urlencode

from django import test
from django.conf import settings
from django.contrib.contenttypes.models import ContentType
from django.core import serializers
from django.urls import reverse
from django_comments.models import Comment

from tcms.management.models import Priority
from tcms.management.models import EnvGroup
from tcms.management.models import EnvProperty
from tcms.testcases.forms import CaseAutomatedForm
from tcms.testcases.forms import TestCase
from tcms.testplans.models import TestPlan
from tcms.testruns.models import TestCaseRun
from tcms.testruns.models import TestCaseRunStatus
from tcms.tests import BaseCaseRun
from tcms.tests import BasePlanCase
from tcms.tests import remove_perm_from_user
from tcms.tests import user_should_have_perm
from tcms.tests.factories import UserFactory
from tcms.tests.factories import EnvGroupFactory
from tcms.tests.factories import EnvGroupPropertyMapFactory
from tcms.tests.factories import EnvPropertyFactory


class TestNavigation(test.TestCase):
    @classmethod
    def setUpTestData(cls):
        super(TestNavigation, cls).setUpTestData()
        cls.user = UserFactory(email='user+1@example.com')
        cls.user.set_password('testing')
        cls.user.save()

    def test_urls_for_emails_with_pluses(self):
        # test for https://github.com/Nitrate/Nitrate/issues/262
        # when email contains + sign it needs to be properly urlencoded
        # before passing it as query string argument to the search views
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.user.username,
            password='testing')
        response = self.client.get(reverse('iframe-navigation'))

        self.assertContains(response, urlencode({'people': self.user.email}))
        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))


class TestIndex(BaseCaseRun):
    def test_when_not_logged_in_index_page_redirects_to_login(self):
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-login'),
            target_status_code=HTTPStatus.OK)

    def test_when_logged_in_index_page_redirects_to_dashboard(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-recent', args=[self.tester.username]),
            target_status_code=HTTPStatus.OK)


class TestCommentCaseRuns(BaseCaseRun):
    """"""Test case for ajax.comment_case_runs""""""

    @classmethod
    def setUpTestData(cls):
        super(TestCommentCaseRuns, cls).setUpTestData()
        cls.many_comments_url = reverse('ajax-comment_case_runs')

    def test_refuse_if_missing_comment(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Comments needed'})

    def test_refuse_if_missing_no_case_run_pk(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment', 'run': []})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

    def test_refuse_if_passed_case_run_pks_not_exist(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment',
                                     'run': '99999998,1009900'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No caserun found.'})

    def test_add_comment_to_case_runs(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        new_comment = 'new comment'
        response = self.client.post(
            self.many_comments_url,
            {'comment': new_comment,
             'run': ','.join([str(self.case_run_1.pk),
                              str(self.case_run_2.pk)])})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        # Assert comments are added
        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)

        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):
            comments = Comment.objects.filter(object_pk=case_run_pk,
                                              content_type=case_run_ct)
            self.assertEqual(new_comment, comments[0].comment)
            self.assertEqual(self.tester, comments[0].user)


class TestUpdateObject(BasePlanCase):
    """"""Test case for update""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateObject, cls).setUpTestData()

        cls.permission = 'testplans.change_testplan'
        cls.update_url = reverse('ajax-update')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        remove_perm_from_user(self.tester, self.permission)

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_update_plan_is_active(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        plan = TestPlan.objects.get(pk=self.plan.pk)
        self.assertFalse(plan.is_active)


class TestUpdateCaseRunStatus(BaseCaseRun):
    """"""Test case for update_case_run_status""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCaseRunStatus, cls).setUpTestData()

        cls.permission = 'testruns.change_testcaserun'
        cls.update_url = reverse('ajax-update_case_run_status')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_change_case_run_status(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        self.assertEqual(
            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)


class TestGetForm(test.TestCase):
    """"""Test case for form""""""

    def test_get_form(self):
        response = self.client.get(reverse('ajax-form'),
                                   {'app_form': 'testcases.CaseAutomatedForm'})
        form = CaseAutomatedForm()
        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())


class TestUpdateCasePriority(BasePlanCase):
    """"""Test case for update_cases_default_tester""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCasePriority, cls).setUpTestData()

        cls.permission = 'testcases.change_testcase'
        cls.case_update_url = reverse('ajax-update_cases_default_tester')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': ""You don't have enough permission to ""
                                  ""update TestCases.""})

    def test_update_case_priority(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        for pk in (self.case_1.pk, self.case_3.pk):
            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)


class TestGetObjectInfo(BasePlanCase):
    """"""Test case for info view method""""""

    @classmethod
    def setUpTestData(cls):
        super(TestGetObjectInfo, cls).setUpTestData()

        cls.get_info_url = reverse('ajax-info')

        cls.group_nitrate = EnvGroupFactory(name='nitrate')
        cls.group_new = EnvGroupFactory(name='NewGroup')

        cls.property_os = EnvPropertyFactory(name='os')
        cls.property_python = EnvPropertyFactory(name='python')
        cls.property_django = EnvPropertyFactory(name='django')

        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_os)
        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_python)
        EnvGroupPropertyMapFactory(group=cls.group_new,
                                   property=cls.property_django)

    def test_get_env_properties(self):
        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})

        expected_json = json.loads(
            serializers.serialize(
                'json',
                EnvProperty.objects.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)

    def test_get_env_properties_by_group(self):
        response = self.client.get(self.get_info_url,
                                   {'info_type': 'env_properties',
                                    'env_group_id': self.group_new.pk})

        group = EnvGroup.objects.get(pk=self.group_new.pk)
        expected_json = json.loads(
            serializers.serialize(
                'json',
                group.property.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)
/n/n/n",1
62,e08c7a0b2dc5002a935737e661a6e8e8c9040de3,"openqml-pq/openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/nopenqml-sf/openqml_sf/fock.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CatState:': Catstate,
    'CoherentState': Coherent,
    'FockDensityMatrix': DensityMatrix,
    'DisplacedSqueezed': DisplacedSqueezed,
    'FockState': Fock,
    'FockStateVector': Ket,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'CrossKerr': CKgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'Kerr': Kgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeezing': Sgate,
    'CubicPhase': Vgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureFock': MeasureFock,
    # 'MeasureHomodyne': MeasureHomodyne
}


class StrawberryFieldsFock(Device):
    """"""StrawberryFields Fock device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):
        self.wires = wires
        self.cutoff = cutoff
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('fock', cutoff_dim=self.cutoff)

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self.observe.params)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/nopenqml-sf/openqml_sf/gaussian.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CoherentState': Coherent,
    'DisplacedSqueezed': DisplacedSqueezed,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeeze': Sgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureHomodyne': MeasureHomodyne,
    # 'MeasureHeterodyne': MeasureHeterodyne
}



class StrawberryFieldsGaussian(Device):
    """"""StrawberryFields Gaussian device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, hbar=2):
        self.wires = wires
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('gaussian')

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self._observe.params)
        elif self._observe.name == 'Displacement':
            ex = self.state.displacement(modes=reg)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/nopenqml/device.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""

import abc
import logging


logging.getLogger()


class MethodFactory(type):
    """"""Metaclass that allows derived classes to dynamically instantiate
    new objects based on undefined methods. The dynamic methods pass their arguments
    directly to __init__ of the inheriting class.""""""
    def __getattr__(cls, name):
        """"""Get the attribute call via name""""""
        def new_object(*args, **kwargs):
            """"""Return a new object of the same class, passing the attribute name
            as the first parameter, along with any additional parameters.""""""
            return cls(name, *args, **kwargs)
        return new_object


class DeviceError(Exception):
    """"""Exception raised by a :class:`Device` when it encounters an illegal
    operation in the quantum circuit.
    """"""
    pass


class Device(abc.ABC):
    """"""Abstract base class for devices.""""""
    _current_context = None
    name = ''          #: str: official device plugin name
    short_name = ''    #: str: name used to load device plugin
    api_version = ''   #: str: version of OpenQML for which the plugin was made
    version = ''       #: str: version of the device plugin itself
    author = ''        #: str: plugin author(s)
    _capabilities = {} #: dict[str->*]: plugin capabilities
    _gates = {}        #: dict[str->GateSpec]: specifications for supported gates
    _observables = {}  #: dict[str->GateSpec]: specifications for supported observables
    _circuits = {}     #: dict[str->Circuit]: circuit templates associated with this API class

    def __init__(self, name, shots):
        self.name = name # the name of the device

        # number of circuit evaluations used to estimate
        # expectation values of observables. 0 means the exact ev is returned.
        self.shots = shots

        self._out = None  # this attribute stores the expectation output
        self._queue = []  # this list stores the operations to be queued to the device
        self._observe = None # the measurement operation to be performed

    def __repr__(self):
        """"""String representation.""""""
        return self.__module__ +'.' +self.__class__.__name__ +'\nInstance: ' +self.name

    def __str__(self):
        """"""Verbose string representation.""""""
        return self.__repr__() +'\nName: ' +self.name +'\nAPI version: ' +self.api_version\
            +'\nPlugin version: ' +self.version +'\nAuthor: ' +self.author +'\n'

    def __enter__(self):
        if Device._current_context is None:
            Device._current_context = self
            self.reset()
        else:
            raise DeviceError('Only one device can be active at a time.')
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if self._observe is None:
            raise DeviceError('A qfunc must always conclude with a classical expectation value.')
        Device._current_context = None
        self.execute()

    @property
    def gates(self):
        """"""Get the supported gate set.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._gates

    @property
    def observables(self):
        """"""Get the supported observables.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._observables

    @property
    def templates(self):
        """"""Get the predefined circuit templates.

        .. todo:: rename to circuits?

        Returns:
          dict[str->Circuit]: circuit templates
        """"""
        return self._circuits

    @property
    def result(self):
        """"""Get the circuit result.

        Returns:
            float or int
        """"""
        return self._out

    @classmethod
    def capabilities(cls):
        """"""Get the other capabilities of the plugin.

        Measurements, batching etc.

        Returns:
          dict[str->*]: results
        """"""
        return cls._capabilities

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        self._out = self.execute_queued()

    @abc.abstractmethod
    def execute_queued(self):
        """"""Called during execute(). To be implemented by each plugin.

        Returns:
          float: expectation value(s) #todo: This should become an array type to handle multiple expectation values.
        """"""
        raise NotImplementedError

    @abc.abstractmethod
    def reset(self):
        """"""Reset the backend state.

        After the reset the backend should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        raise NotImplementedError
/n/n/nopenqml/plugins/default.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from scipy.linalg import expm, eigh

import openqml as qm
from openqml import Device, DeviceError, qfunc, QNode, Variable, __version__


# tolerance for numerical errors
tolerance = 1e-10


#========================================================
#  utilities
#========================================================

def spectral_decomposition_qubit(A):
    r""""""Spectral decomposition of a 2*2 Hermitian matrix.

    Args:
      A (array): 2*2 Hermitian matrix

    Returns:
      (vector[float], list[array[complex]]): (a, P): eigenvalues and hermitian projectors
        such that :math:`A = \sum_k a_k P_k`.
    """"""
    d, v = eigh(A)
    P = []
    for k in range(2):
        temp = v[:, k]
        P.append(np.outer(temp.conj(), temp))
    return d, P


#========================================================
#  fixed gates
#========================================================

I = np.eye(2)
# Pauli matrices
X = np.array([[0, 1], [1, 0]])
Y = np.array([[0, -1j], [1j, 0]])
Z = np.array([[1, 0], [0, -1]])
CNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])
SWAP = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])


#========================================================
#  parametrized gates
#========================================================


def frx(theta):
    r""""""One-qubit rotation about the x axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_x \theta/2}`
    """"""
    return expm(-1j * theta/2 * X)


def fry(theta):
    r""""""One-qubit rotation about the y axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_y \theta/2}`
    """"""
    return expm(-1j * theta/2 * Y)


def frz(theta):
    r""""""One-qubit rotation about the z axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_z \theta/2}`
    """"""
    return expm(-1j * theta/2 * Z)


def fr3(a, b, c):
    r""""""Arbitrary one-qubit rotation using three Euler angles.

    Args:
        a,b,c (float): rotation angles
    Returns:
        array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a)
    """"""
    return frz(c) @ (fry(b) @ frz(a))


#========================================================
#  Arbitrary states and operators
#========================================================

def ket(*args):
    r""""""Input validation for an arbitary state vector.

    Args:
        args (array): NumPy array.

    Returns:
        array: normalised array.
    """"""
    state = np.asarray(args)
    return state/np.linalg.norm(state)


def unitary(*args):
    r""""""Input validation for an arbitary unitary operation.

    Args:
        args (array): square unitary matrix.

    Returns:
        array: square unitary matrix.
    """"""
    U = np.asarray(args[0])

    if U.shape[0] != U.shape[1]:
        raise ValueError(""Operator must be a square matrix."")

    if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance):
        raise ValueError(""Operator must be unitary."")

    return U


def hermitian(*args):
    r""""""Input validation for an arbitary Hermitian observable.

    Args:
        args (array): square hermitian matrix.

    Returns:
        array: square hermitian matrix.
    """"""
    A = np.asarray(args[0])

    if A.shape[0] != A.shape[1]:
        raise ValueError(""Observable must be a square matrix."")

    if not np.allclose(A, A.conj().T, atol=tolerance):
        raise ValueError(""Observable must be Hermitian."")
    return A


#========================================================
#  operator map
#========================================================


operator_map = {
    'QubitStateVector': ket,
    'QubitUnitary': unitary,
    'Hermitian': hermitian,
    'Identity': I,
    'PauliX': X,
    'PauliY': Y,
    'PauliZ': Z,
    'CNOT': CNOT,
    'SWAP': SWAP,
    'RX': frx,
    'RY': fry,
    'RZ': frz,
    'Rot': fr3
}


#========================================================
#  device
#========================================================


class DefaultQubit(Device):
    """"""Default qubit device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Default OpenQML plugin'
    short_name = 'default.qubit'
    api_version = '0.1.0'
    version = '0.1.0'
    author = 'Xanadu Inc.'
    _gates = set(operator_map.keys())
    _observables = {}
    _circuits = {}

    def __init__(self, wires, *, shots=0):
        self.wires = wires
        self.eng = None
        self._state = None
        super().__init__(self.short_name, shots)

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self._state is None:
            # init the state vector to |00..0>
            self._state = np.zeros(2**self.wires, dtype=complex)
            self._state[0] = 1
            self._out = np.full(self.wires, np.nan)

        # apply unitary operations U
        for operation in self._queue:
            if operation.name == 'QubitStateVector':
                state = np.asarray(operation.params[0])
                if state.ndim == 1 and state.shape[0] == 2**self.wires:
                    self._state = state
                else:
                    raise ValueError('State vector must be of length 2**wires.')
                continue

            U = DefaultQubit._get_operator_matrix(operation)

            if len(operation.wires) == 1:
                U = self.expand_one(U, operation.wires)
            elif len(operation.wires) == 2:
                U = self.expand_two(U, operation.wires)
            else:
                raise ValueError('This plugin supports only one- and two-qubit gates.')
            self._state = U @ self._state

        # measurement/expectation value <psi|A|psi>
        A = DefaultQubit._get_operator_matrix(self._observe)
        if self.shots == 0:
            # exact expectation value
            ev = self.ev(A, [self._observe.wires])
        else:
            # estimate the ev
            if 0:
                # use central limit theorem, sample normal distribution once, only ok if n_eval is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
                ev = self.ev(A, self._observe.wires)
                var = self.ev(A**2, self._observe.wires) - ev**2  # variance
                ev = np.random.normal(ev, np.sqrt(var / self.shots))
            else:
                # sample Bernoulli distribution n_eval times / binomial distribution once
                a, P = spectral_decomposition_qubit(A)
                p0 = self.ev(P[0], self._observe.wires)  # probability of measuring a[0]
                n0 = np.random.binomial(self.shots, p0)
                ev = (n0*a[0] +(self.shots-n0)*a[1]) / self.shots

        self._out = ev  # store the result

    @classmethod
    def _get_operator_matrix(cls, A):
        """"""Get the operator matrix for a given operation.

        Args:
            A (openqml.Operation or openqml.Expectation): operation/observable.

        Returns:
            array: matrix representation.
        """"""
        if A.name not in operator_map:
            raise DeviceError(""{} not supported by device {}"".format(A.name, cls.short_name))

        if not callable(operator_map[A.name]):
            return operator_map[A.name]

        # unpack variables
        p = [x.val if isinstance(x, Variable) else x for x in A.params]
        return operator_map[A.name](*p)

    def ev(self, A, wires):
        r""""""Expectation value of a one-qubit observable in the current state.

        Args:
          A (array): 2*2 hermitian matrix corresponding to the observable
          wires (Sequence[int]): target subsystem

        Returns:
          float: expectation value :math:`\expect{A} = \bra{\psi}A\ket{\psi}`
        """"""
        if A.shape != (2, 2):
            raise ValueError('2x2 matrix required.')

        A = self.expand_one(A, wires)
        expectation = np.vdot(self._state, A @ self._state)

        if np.abs(expectation.imag) > tolerance:
            log.warning('Nonvanishing imaginary part {} in expectation value.'.format(expectation.imag))
        return expectation.real

    def reset(self):
        """"""Reset the device""""""
        self._state  = None  #: array: state vector
        self._out = None  #: array: measurement results

    def expand_one(self, U, wires):
        """"""Expand a one-qubit operator into a full system operator.

        Args:
          U (array): 2*2 matrix
          wires (Sequence[int]): target subsystem

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (2, 2):
            raise ValueError('2x2 matrix required.')
        if len(wires) != 1:
            raise ValueError('One target subsystem required.')
        wires = wires[0]
        before = 2**wires
        after  = 2**(self.wires-wires-1)
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U

    def expand_two(self, U, wires):
        """"""Expand a two-qubit operator into a full system operator.

        Args:
          U (array): 4x4 matrix
          wires (Sequence[int]): two target subsystems (order matters!)

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (4, 4):
            raise ValueError('4x4 matrix required.')
        if len(wires) != 2:
            raise ValueError('Two target subsystems required.')
        wires = np.asarray(wires)
        if np.any(wires < 0) or np.any(wires >= self.wires) or wires[0] == wires[1]:
            raise ValueError('Bad target subsystems.')

        a = np.min(wires)
        b = np.max(wires)
        n_between = b-a-1  # number of qubits between a and b
        # dimensions of the untouched subsystems
        before  = 2**a
        after   = 2**(self.wires-b-1)
        between = 2**n_between

        U = np.kron(U, np.eye(between))
        # how U should be reordered
        if wires[0] < wires[1]:
            p = [0, 2, 1]
        else:
            p = [1, 2, 0]
        dim = [2, 2, between]
        p = np.array(p)
        perm = np.r_[p, p+3]
        # reshape U into another array which has one index per subsystem, permute dimensions, back into original-shape array
        temp = np.prod(dim)
        U = U.reshape(dim * 2).transpose(perm).reshape([temp, temp])
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U


#====================
# Default circuits
#====================


dev = DefaultQubit(wires=2)

def node(x, y, z):
    qm.RX(x, [0])
    qm.CNOT([0, 1])
    qm.RY(-1.6, [0])
    qm.RY(y, [1])
    qm.CNOT([1, 0])
    qm.RX(z, [0])
    qm.CNOT([0, 1])
    qm.expectation.Hermitian(np.array([[0, 1], [1, 0]]), 0)

circuits = {'demo_ev': QNode(node, dev)}
/n/n/n",0
63,e08c7a0b2dc5002a935737e661a6e8e8c9040de3,"/openqml-pq/openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute(self):
        """""" """"""
        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18
        self._out = self.execute_queued()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/n/openqml-sf/openqml_sf/fock.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CatState:': Catstate,
    'CoherentState': Coherent,
    'FockDensityMatrix': DensityMatrix,
    'DisplacedSqueezed': DisplacedSqueezed,
    'FockState': Fock,
    'FockStateVector': Ket,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'CrossKerr': CKgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'Kerr': Kgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeezing': Sgate,
    'CubicPhase': Vgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureFock': MeasureFock,
    # 'MeasureHomodyne': MeasureHomodyne
}


class StrawberryFieldsFock(Device):
    """"""StrawberryFields Fock device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):
        self.wires = wires
        self.cutoff = cutoff
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('fock', cutoff_dim=self.cutoff)

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self.observe.params)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/n/openqml-sf/openqml_sf/gaussian.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CoherentState': Coherent,
    'DisplacedSqueezed': DisplacedSqueezed,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeeze': Sgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureHomodyne': MeasureHomodyne,
    # 'MeasureHeterodyne': MeasureHeterodyne
}



class StrawberryFieldsGaussian(Device):
    """"""StrawberryFields Gaussian device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, hbar=2):
        self.wires = wires
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('gaussian')

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self._observe.params)
        elif self._observe.name == 'Displacement':
            ex = self.state.displacement(modes=reg)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/n/openqml/device.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""

import abc
import logging


logging.getLogger()


class MethodFactory(type):
    """"""Metaclass that allows derived classes to dynamically instantiate
    new objects based on undefined methods. The dynamic methods pass their arguments
    directly to __init__ of the inheriting class.""""""
    def __getattr__(cls, name):
        """"""Get the attribute call via name""""""
        def new_object(*args, **kwargs):
            """"""Return a new object of the same class, passing the attribute name
            as the first parameter, along with any additional parameters.""""""
            return cls(name, *args, **kwargs)
        return new_object


class DeviceError(Exception):
    """"""Exception raised by a :class:`Device` when it encounters an illegal
    operation in the quantum circuit.
    """"""
    pass


class Device(abc.ABC):
    """"""Abstract base class for devices.""""""
    _current_context = None
    name = ''          #: str: official device plugin name
    short_name = ''    #: str: name used to load device plugin
    api_version = ''   #: str: version of OpenQML for which the plugin was made
    version = ''       #: str: version of the device plugin itself
    author = ''        #: str: plugin author(s)
    _capabilities = {} #: dict[str->*]: plugin capabilities
    _gates = {}        #: dict[str->GateSpec]: specifications for supported gates
    _observables = {}  #: dict[str->GateSpec]: specifications for supported observables
    _circuits = {}     #: dict[str->Circuit]: circuit templates associated with this API class

    def __init__(self, name, shots):
        self.name = name # the name of the device

        # number of circuit evaluations used to estimate
        # expectation values of observables. 0 means the exact ev is returned.
        self.shots = shots

        self._out = None  # this attribute stores the expectation output
        self._queue = []  # this list stores the operations to be queued to the device
        self._observe = None # the measurement operation to be performed

    def __repr__(self):
        """"""String representation.""""""
        return self.__module__ +'.' +self.__class__.__name__ +'\nInstance: ' +self.name

    def __str__(self):
        """"""Verbose string representation.""""""
        return self.__repr__() +'\nName: ' +self.name +'\nAPI version: ' +self.api_version\
            +'\nPlugin version: ' +self.version +'\nAuthor: ' +self.author +'\n'

    def __enter__(self):
        if Device._current_context is None:
            Device._current_context = self
            self.reset()
        else:
            raise DeviceError('Only one device can be active at a time.')
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if self._observe is None:
            raise DeviceError('A qfunc must always conclude with a classical expectation value.')
        Device._current_context = None
        self.execute()

    @property
    def gates(self):
        """"""Get the supported gate set.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._gates

    @property
    def observables(self):
        """"""Get the supported observables.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._observables

    @property
    def templates(self):
        """"""Get the predefined circuit templates.

        .. todo:: rename to circuits?

        Returns:
          dict[str->Circuit]: circuit templates
        """"""
        return self._circuits

    @property
    def result(self):
        """"""Get the circuit result.

        Returns:
            float or int
        """"""
        return self._out

    @classmethod
    def capabilities(cls):
        """"""Get the other capabilities of the plugin.

        Measurements, batching etc.

        Returns:
          dict[str->*]: results
        """"""
        return cls._capabilities

    @abc.abstractmethod
    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        raise NotImplementedError

    @abc.abstractmethod
    def reset(self):
        """"""Reset the backend state.

        After the reset the backend should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        raise NotImplementedError
/n/n/n/openqml/plugins/default.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from scipy.linalg import expm, eigh

import openqml as qm
from openqml import Device, DeviceError, qfunc, QNode, Variable, __version__


# tolerance for numerical errors
tolerance = 1e-10


#========================================================
#  utilities
#========================================================

def spectral_decomposition_qubit(A):
    r""""""Spectral decomposition of a 2*2 Hermitian matrix.

    Args:
      A (array): 2*2 Hermitian matrix

    Returns:
      (vector[float], list[array[complex]]): (a, P): eigenvalues and hermitian projectors
        such that :math:`A = \sum_k a_k P_k`.
    """"""
    d, v = eigh(A)
    P = []
    for k in range(2):
        temp = v[:, k]
        P.append(np.outer(temp.conj(), temp))
    return d, P


#========================================================
#  fixed gates
#========================================================

I = np.eye(2)
# Pauli matrices
X = np.array([[0, 1], [1, 0]])
Y = np.array([[0, -1j], [1j, 0]])
Z = np.array([[1, 0], [0, -1]])
CNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])
SWAP = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])


#========================================================
#  parametrized gates
#========================================================


def frx(theta):
    r""""""One-qubit rotation about the x axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_x \theta/2}`
    """"""
    return expm(-1j * theta/2 * X)


def fry(theta):
    r""""""One-qubit rotation about the y axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_y \theta/2}`
    """"""
    return expm(-1j * theta/2 * Y)


def frz(theta):
    r""""""One-qubit rotation about the z axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_z \theta/2}`
    """"""
    return expm(-1j * theta/2 * Z)


def fr3(a, b, c):
    r""""""Arbitrary one-qubit rotation using three Euler angles.

    Args:
        a,b,c (float): rotation angles
    Returns:
        array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a)
    """"""
    return frz(c) @ (fry(b) @ frz(a))


#========================================================
#  Arbitrary states and operators
#========================================================

def ket(*args):
    r""""""Input validation for an arbitary state vector.

    Args:
        args (array): NumPy array.

    Returns:
        array: normalised array.
    """"""
    state = np.asarray(args)
    return state/np.linalg.norm(state)


def unitary(*args):
    r""""""Input validation for an arbitary unitary operation.

    Args:
        args (array): square unitary matrix.

    Returns:
        array: square unitary matrix.
    """"""
    U = np.asarray(args[0])

    if U.shape[0] != U.shape[1]:
        raise ValueError(""Operator must be a square matrix."")

    if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance):
        raise ValueError(""Operator must be unitary."")

    return U


def hermitian(*args):
    r""""""Input validation for an arbitary Hermitian observable.

    Args:
        args (array): square hermitian matrix.

    Returns:
        array: square hermitian matrix.
    """"""
    A = np.asarray(args[0])

    if A.shape[0] != A.shape[1]:
        raise ValueError(""Observable must be a square matrix."")

    if not np.allclose(A, A.conj().T, atol=tolerance):
        raise ValueError(""Observable must be Hermitian."")
    return A


#========================================================
#  operator map
#========================================================


operator_map = {
    'QubitStateVector': ket,
    'QubitUnitary': unitary,
    'Hermitian': hermitian,
    'Identity': I,
    'PauliX': X,
    'PauliY': Y,
    'PauliZ': Z,
    'CNOT': CNOT,
    'SWAP': SWAP,
    'RX': frx,
    'RY': fry,
    'RZ': frz,
    'Rot': fr3
}


#========================================================
#  device
#========================================================


class DefaultQubit(Device):
    """"""Default qubit device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Default OpenQML plugin'
    short_name = 'default.qubit'
    api_version = '0.1.0'
    version = '0.1.0'
    author = 'Xanadu Inc.'
    _gates = set(operator_map.keys())
    _observables = {}
    _circuits = {}

    def __init__(self, wires, *, shots=0):
        self.wires = wires
        self.eng = None
        self._state = None
        super().__init__(self.short_name, shots)

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self._state is None:
            # init the state vector to |00..0>
            self._state = np.zeros(2**self.wires, dtype=complex)
            self._state[0] = 1
            self._out = np.full(self.wires, np.nan)

        # apply unitary operations U
        for operation in self._queue:
            if operation.name == 'QubitStateVector':
                state = np.asarray(operation.params[0])
                if state.ndim == 1 and state.shape[0] == 2**self.wires:
                    self._state = state
                else:
                    raise ValueError('State vector must be of length 2**wires.')
                continue

            U = DefaultQubit._get_operator_matrix(operation)

            if len(operation.wires) == 1:
                U = self.expand_one(U, operation.wires)
            elif len(operation.wires) == 2:
                U = self.expand_two(U, operation.wires)
            else:
                raise ValueError('This plugin supports only one- and two-qubit gates.')
            self._state = U @ self._state

        # measurement/expectation value <psi|A|psi>
        A = DefaultQubit._get_operator_matrix(self._observe)
        if self.shots == 0:
            # exact expectation value
            ev = self.ev(A, [self._observe.wires])
        else:
            # estimate the ev
            if 0:
                # use central limit theorem, sample normal distribution once, only ok if n_eval is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
                ev = self.ev(A, self._observe.wires)
                var = self.ev(A**2, self._observe.wires) - ev**2  # variance
                ev = np.random.normal(ev, np.sqrt(var / self.shots))
            else:
                # sample Bernoulli distribution n_eval times / binomial distribution once
                a, P = spectral_decomposition_qubit(A)
                p0 = self.ev(P[0], self._observe.wires)  # probability of measuring a[0]
                n0 = np.random.binomial(self.shots, p0)
                ev = (n0*a[0] +(self.shots-n0)*a[1]) / self.shots

        self._out = ev  # store the result

    @classmethod
    def _get_operator_matrix(cls, A):
        """"""Get the operator matrix for a given operation.

        Args:
            A (openqml.Operation or openqml.Expectation): operation/observable.

        Returns:
            array: matrix representation.
        """"""
        if A.name not in operator_map:
            raise DeviceError(""{} not supported by device {}"".format(A.name, cls.short_name))

        if not callable(operator_map[A.name]):
            return operator_map[A.name]

        # unpack variables
        p = [x.val if isinstance(x, Variable) else x for x in A.params]
        return operator_map[A.name](*p)

    def ev(self, A, wires):
        r""""""Expectation value of a one-qubit observable in the current state.

        Args:
          A (array): 2*2 hermitian matrix corresponding to the observable
          wires (Sequence[int]): target subsystem

        Returns:
          float: expectation value :math:`\expect{A} = \bra{\psi}A\ket{\psi}`
        """"""
        if A.shape != (2, 2):
            raise ValueError('2x2 matrix required.')

        A = self.expand_one(A, wires)
        expectation = np.vdot(self._state, A @ self._state)

        if np.abs(expectation.imag) > tolerance:
            log.warning('Nonvanishing imaginary part {} in expectation value.'.format(expectation.imag))
        return expectation.real

    def reset(self):
        """"""Reset the device""""""
        self._state  = None  #: array: state vector
        self._out = None  #: array: measurement results

    def expand_one(self, U, wires):
        """"""Expand a one-qubit operator into a full system operator.

        Args:
          U (array): 2*2 matrix
          wires (Sequence[int]): target subsystem

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (2, 2):
            raise ValueError('2x2 matrix required.')
        if len(wires) != 1:
            raise ValueError('One target subsystem required.')
        wires = wires[0]
        before = 2**wires
        after  = 2**(self.wires-wires-1)
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U

    def expand_two(self, U, wires):
        """"""Expand a two-qubit operator into a full system operator.

        Args:
          U (array): 4x4 matrix
          wires (Sequence[int]): two target subsystems (order matters!)

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (4, 4):
            raise ValueError('4x4 matrix required.')
        if len(wires) != 2:
            raise ValueError('Two target subsystems required.')
        wires = np.asarray(wires)
        if np.any(wires < 0) or np.any(wires >= self.wires) or wires[0] == wires[1]:
            raise ValueError('Bad target subsystems.')

        a = np.min(wires)
        b = np.max(wires)
        n_between = b-a-1  # number of qubits between a and b
        # dimensions of the untouched subsystems
        before  = 2**a
        after   = 2**(self.wires-b-1)
        between = 2**n_between

        U = np.kron(U, np.eye(between))
        # how U should be reordered
        if wires[0] < wires[1]:
            p = [0, 2, 1]
        else:
            p = [1, 2, 0]
        dim = [2, 2, between]
        p = np.array(p)
        perm = np.r_[p, p+3]
        # reshape U into another array which has one index per subsystem, permute dimensions, back into original-shape array
        temp = np.prod(dim)
        U = U.reshape(dim * 2).transpose(perm).reshape([temp, temp])
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U


#====================
# Default circuits
#====================


dev = DefaultQubit(wires=2)

def node(x, y, z):
    qm.RX(x, [0])
    qm.CNOT([0, 1])
    qm.RY(-1.6, [0])
    qm.RY(y, [1])
    qm.CNOT([1, 0])
    qm.RX(z, [0])
    qm.CNOT([0, 1])
    qm.expectation.Hermitian(np.array([[0, 1], [1, 0]]), 0)

circuits = {'demo_ev': QNode(node, dev)}
/n/n/n",1
64,a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef,"openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/n",0
65,a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef,"/openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute(self):
        """""" """"""
        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18
        self._out = self.execute_queued()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/n",1
66,9f02e2167b84f34a8c4a47702854304e5940dba6,"api/tests/python/end_points/test_sounds_like.py/n/nfrom namex.models import User
import requests
import json
import pytest
from tests.python import integration_solr, integration_synonym_api
import urllib
from hamcrest import *


token_header = {
                ""alg"": ""RS256"",
                ""typ"": ""JWT"",
                ""kid"": ""flask-jwt-oidc-test-client""
               }
claims = {
            ""iss"": ""https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc"",
            ""sub"": ""43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc"",
            ""aud"": ""NameX-Dev"",
            ""exp"": 31531718745,
            ""iat"": 1531718745,
            ""jti"": ""flask-jwt-oidc-test-support"",
            ""typ"": ""Bearer"",
            ""username"": ""test-user"",
            ""realm_access"": {
                ""roles"": [
                    ""{}"".format(User.EDITOR),
                    ""{}"".format(User.APPROVER),
                    ""viewer"",
                    ""user""
                ]
            }
         }


@pytest.fixture(scope=""session"", autouse=True)
def reload_schema(solr):
    url = solr + '/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json'
    r = requests.get(url)

    assert r.status_code == 200


@integration_solr
def test_solr_available(solr, app, client, jwt):
    url = solr + '/solr/possible.conflicts/admin/ping'
    r = requests.get(url)

    assert r.status_code == 200


def clean_database(solr):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'text/xml'}
    data = '<delete><query>id:*</query></delete>'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def seed_database_with(solr, name, id='1', source='CORP'):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'application/json'}
    data = '[{""source"":""' + source + '"", ""name"":""' + name + '"", ""id"":""'+ id +'""}]'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def verify(data, expected):

    print(""Expected: "", expected)

    # remove the search divider(s): ----<query term>
    actual = [{ 'name':doc['name_info']['name'] } for doc in data['names']]

    print(""Actual: "", actual)

    assert_that(len(actual), equal_to(len(expected)))
    for i in range(len(actual)):
        assert_that(actual[i]['name'], equal_to(expected[i]['name']))


def verify_results(client, jwt, query, expected):
    data = search(client, jwt, query)
    verify(data, expected)


def search(client, jwt, query):
    token = jwt.create_jwt(claims, token_header)
    headers = {'Authorization': 'Bearer ' + token}
    url = '/api/v1/requests/phonetics/' + urllib.parse.quote(query) + '/*'
    print(url)
    rv = client.get(url, headers=headers)

    assert rv.status_code == 200
    return json.loads(rv.data)


@integration_synonym_api
@integration_solr
def test_all_good(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'GOLDSTREAM ELECTRICAL LTD'}
       ]
    )


@pytest.mark.skip(reason=""Rhyming not implemented yet"")
@integration_synonym_api
@integration_solr
def test_sounds_like(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1')
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2')
    seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3')
    seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4')
    seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5')
    seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6')
    seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7')
    seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8')
    seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9')
    seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10')
    seed_database_with(solr, 'BLABLA ANYTHING', id='11')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'COLDSTREAM VENTURES INC.'},
           {'name': 'GOLDSPRING PROPERTIES LTD'},
           {'name': 'GOLDSTEIN HOLDINGS INC.'},
           {'name': 'GOLDSTREAM ELECTRICAL CORP'},
           {'name': 'GOLDSTRIPES AVIATION INC'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_liberti(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LIBERTI', id='1')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_deeper(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LABORATORY', id='1')
    seed_database_with(solr, 'LAPORTE', id='2')
    seed_database_with(solr, 'LIBERTI', id='3')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_jasmine(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'JASMINE', id='1')
    verify_results(client, jwt,
       query='OSMOND',
       expected=[
           {'name': '----OSMOND'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_fey(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEY', id='1')
    verify_results(client, jwt,
       query='FAY',
       expected=[
           {'name': '----FAY'},
           {'name': 'FEY'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_venizia(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'VENIZIA', id='1')
    seed_database_with(solr, 'VENEZIA', id='2')
    seed_database_with(solr, 'VANSEA', id='3')
    seed_database_with(solr, 'WENSO', id='4')
    verify_results(client, jwt,
       query='VENIZIA',
       expected=[
           {'name': '----VENIZIA'},
           {'name': 'VENEZIA'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_ys_and_is(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'CRYSTAL'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOLDSMITHS', id='1')
    verify_results(client, jwt,
       query='COLDSTREAM',
       expected=[
           {'name': '----COLDSTREAM'},
           {'name': 'KOLDSMITHS'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks_again(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRAZY', id='1')
    seed_database_with(solr, 'KAIZEN', id='2')
    verify_results(client, jwt,
       query='CAYZEN',
       expected=[
           {'name': '----CAYZEN'},
           {'name': 'KAIZEN'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_short_word(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FE', id='1')
    verify_results(client, jwt,
       query='FA',
       expected=[
           {'name': '----FA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_single_vowel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEDS', id='1')
    verify_results(client, jwt,
       query='FADS',
       expected=[
           {'name': '----FADS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_feel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEEL', id='1')
    verify_results(client, jwt,
       query='FILL',
       expected=[
           {'name': '----FILL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_bear(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEAR', id='1')
    verify_results(client, jwt,
       query='BARE',
       expected=[
           {'name': '----BARE'},
           {'name': 'BEAR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_corp(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_designation_in_query_is_ignored(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FINGER LIMATED', id='1')
    verify_results(client, jwt,
       query='SUN LIMITED',
       expected=[
           {'name': '----SUN'}
       ]
    )


@integration_synonym_api
@integration_solr
def leak(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LEAK', id='1')
    verify_results(client, jwt,
       query='LEEK',
       expected=[
           {'name': 'LEAK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_plank(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PLANCK', id='1')
    verify_results(client, jwt,
       query='PLANK',
       expected=[
           {'name': '----PLANK'},
           {'name': 'PLANCK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_krystal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_christal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CHRISTAL',
       expected=[
           {'name': '----CHRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kl(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KLASS', id='1')
    verify_results(client, jwt,
       query='CLASS',
       expected=[
           {'name': '----CLASS'},
           {'name': 'KLASS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pheel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PHEEL', id='1')
    verify_results(client, jwt,
       query='FEEL',
       expected=[
           {'name': '----FEEL'},
           {'name': 'PHEEL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ghable(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GHABLE', id='1')
    verify_results(client, jwt,
       query='GABLE',
       expected=[
           {'name': '----GABLE'},
           {'name': 'GHABLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_gnat(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'GNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'KNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PNEU', id='1')
    verify_results(client, jwt,
       query='NEU',
       expected=[
           {'name': '----NEU'},
           {'name': 'PNEU'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wr(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WREN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'WREN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_rh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'RHEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_soft_c_is_not_k(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KIRK', id='1')
    verify_results(client, jwt,
       query='CIRCLE',
       expected=[
           {'name': '----CIRCLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_oi_oy(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'OYSTER', id='1')
    verify_results(client, jwt,
       query='OISTER',
       expected=[
           {'name': '----OISTER'},
           {'name': 'OYSTER'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_dont_add_match_twice(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN GNAT', id='1')
    verify_results(client, jwt,
       query='REN NAT',
       expected=[
           {'name': '----REN NAT'},
           {'name': 'RHEN GNAT'},
           {'name': '----REN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_neighbour(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'NEIGHBOUR', id='1')
    verify_results(client, jwt,
       query='NAYBOR',
       expected=[
           {'name': '----NAYBOR'},
           {'name': 'NEIGHBOUR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_mac_mc(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'MCGREGOR', id='1')
    verify_results(client, jwt,
       query='MACGREGOR',
       expected=[
           {'name': '----MACGREGOR'},
           {'name': 'MCGREGOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ex_x(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'EXTREME', id='1')
    verify_results(client, jwt,
       query='XTREME',
       expected=[
           {'name': '----XTREME'},
           {'name': 'EXTREME'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WHITE', id='1')
    verify_results(client, jwt,
       query='WITE',
       expected=[
           {'name': '----WITE'},
           {'name': 'WHITE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_qu(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KWIK', id='1')
    verify_results(client, jwt,
       query='QUICK',
       expected=[
           {'name': '----QUICK'},
           {'name': 'KWIK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ps(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PSYCHO', id='1')
    verify_results(client, jwt,
       query='SYCHO',
       expected=[
           {'name': '----SYCHO'},
           {'name': 'PSYCHO'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_terra(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TERRA', id='1')
    verify_results(client, jwt,
       query='TARA',
       expected=[
           {'name': 'TERRA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ayaan(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AYAAN', id='1')
    verify_results(client, jwt,
       query='AYAN',
       expected=[
           {'name': '----AYAN'},
           {'name': 'AYAAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_aggri(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AGGRI', id='1')
    verify_results(client, jwt,
       query='AGRI',
       expected=[
           {'name': '----AGRI'},
           {'name': 'AGGRI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kofi(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOFI', id='1')
    verify_results(client, jwt,
       query='COFFI',
       expected=[
           {'name': '----COFFI'},
           {'name': 'KOFI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_tru(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TRU', id='1')
    verify_results(client, jwt,
       query='TRUE',
       expected=[
           {'name': '----TRUE'},
           {'name': 'TRU'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_dymond(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DYMOND', id='1')
    verify_results(client, jwt,
       query='DIAMOND',
       expected=[
           {'name': 'DYMOND'}
       ]
    )


@pytest.mark.skip(reason=""compound words not handled yet"")
@integration_synonym_api
@integration_solr
def test_bee_kleen(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEE KLEEN', id='1')
    verify_results(client, jwt,
       query='BE-CLEAN',
       expected=[
           {'name': 'BEE KLEEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_exact_match_keep_phonetic(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1')
    seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2')
    verify_results(client, jwt,
       query='BLUEPRINT BEAUTY',
       expected=[
           {'name': '----BLUEPRINT BEAUTY'},
           {'name': 'BLUEPRINT BEAUTEE'},
           {'name': '----BLUEPRINT synonyms:(BEAUTI)'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_both_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1')
    verify_results(client, jwt,
       query='INTERVENTION BEHAVIOUR',
       expected=[
           {'name': '----INTERVENTION BEHAVIOUR'},
           {'name': '----INTERVENTION'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_at_right_level(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resists_qword_matching_several_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR BEHAVIOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_a(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AILEEN ENTERPRISES', id='1')
    verify_results(client, jwt,
       query='ALAN HARGREAVES CORPORATION',
       expected=[
           {'name': '----ALAN HARGREAVES'},
           {'name': '----ALAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_e(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ACME', id='1')
    verify_results(client, jwt,
       query='EQUIOM',
       expected=[
           {'name': '----EQUIOM'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_not_match_consonant(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1')
    seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2')
    verify_results(client, jwt,
       query='EH',
       expected=[
           {'name': '----EH'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_unusual_result(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1')
    verify_results(client, jwt,
       query='TABLE',
       expected=[
           {'name': '----TABLE'}
       ]
    )

@integration_synonym_api
@integration_solr
def test_stack_ignores_wildcards(client, jwt, app):
    verify_results(client, jwt,
        query=""TESTING* @WILDCARDS"",
        expected=[
            {'name': '----TESTING WILDCARDS'},
            {'name': '----TESTING'}
        ]
    )

@integration_synonym_api
@integration_solr
@pytest.mark.parametrize(""query"", [
    ('T.H.E.'),
    ('COMPANY'),
    ('ASSN'),
    ('THAT'),
    ('LIMITED CORP.'),
])
def test_query_stripped_to_empty_string(solr,client, jwt, query):
    clean_database(solr)
    seed_database_with(solr, 'JM Van Damme inc', id='1')
    seed_database_with(solr, 'SOME RANDOM NAME', id='2')
    verify_results(client, jwt,
        query=query,
        expected=[{'name':'----*'}]
    )/n/n/n",0
67,9f02e2167b84f34a8c4a47702854304e5940dba6,"/api/tests/python/end_points/test_sounds_like.py/n/nfrom namex.models import User
import requests
import json
import pytest
from tests.python import integration_solr, integration_synonym_api
import urllib
from hamcrest import *


token_header = {
                ""alg"": ""RS256"",
                ""typ"": ""JWT"",
                ""kid"": ""flask-jwt-oidc-test-client""
               }
claims = {
            ""iss"": ""https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc"",
            ""sub"": ""43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc"",
            ""aud"": ""NameX-Dev"",
            ""exp"": 31531718745,
            ""iat"": 1531718745,
            ""jti"": ""flask-jwt-oidc-test-support"",
            ""typ"": ""Bearer"",
            ""username"": ""test-user"",
            ""realm_access"": {
                ""roles"": [
                    ""{}"".format(User.EDITOR),
                    ""{}"".format(User.APPROVER),
                    ""viewer"",
                    ""user""
                ]
            }
         }


@pytest.fixture(scope=""session"", autouse=True)
def reload_schema(solr):
    url = solr + '/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json'
    r = requests.get(url)

    assert r.status_code == 200


@integration_solr
def test_solr_available(solr, app, client, jwt):
    url = solr + '/solr/possible.conflicts/admin/ping'
    r = requests.get(url)

    assert r.status_code == 200


def clean_database(solr):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'text/xml'}
    data = '<delete><query>id:*</query></delete>'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def seed_database_with(solr, name, id='1', source='CORP'):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'application/json'}
    data = '[{""source"":""' + source + '"", ""name"":""' + name + '"", ""id"":""'+ id +'""}]'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def verify(data, expected):

    print(""Expected: "", expected)

    # remove the search divider(s): ----<query term>
    actual = [{ 'name':doc['name_info']['name'] } for doc in data['names']]

    print(""Actual: "", actual)

    assert_that(len(actual), equal_to(len(expected)))
    for i in range(len(actual)):
        assert_that(actual[i]['name'], equal_to(expected[i]['name']))


def verify_results(client, jwt, query, expected):
    data = search(client, jwt, query)
    verify(data, expected)


def search(client, jwt, query):
    token = jwt.create_jwt(claims, token_header)
    headers = {'Authorization': 'Bearer ' + token}
    url = '/api/v1/requests/phonetics/' + urllib.parse.quote(query) + '/*'
    print(url)
    rv = client.get(url, headers=headers)

    assert rv.status_code == 200
    return json.loads(rv.data)


@integration_synonym_api
@integration_solr
def test_all_good(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'GOLDSTREAM ELECTRICAL LTD'}
       ]
    )


@pytest.mark.skip(reason=""Rhyming not implemented yet"")
@integration_synonym_api
@integration_solr
def test_sounds_like(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1')
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2')
    seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3')
    seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4')
    seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5')
    seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6')
    seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7')
    seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8')
    seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9')
    seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10')
    seed_database_with(solr, 'BLABLA ANYTHING', id='11')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'COLDSTREAM VENTURES INC.'},
           {'name': 'GOLDSPRING PROPERTIES LTD'},
           {'name': 'GOLDSTEIN HOLDINGS INC.'},
           {'name': 'GOLDSTREAM ELECTRICAL CORP'},
           {'name': 'GOLDSTRIPES AVIATION INC'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_liberti(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LIBERTI', id='1')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_deeper(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LABORATORY', id='1')
    seed_database_with(solr, 'LAPORTE', id='2')
    seed_database_with(solr, 'LIBERTI', id='3')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_jasmine(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'JASMINE', id='1')
    verify_results(client, jwt,
       query='OSMOND',
       expected=[
           {'name': '----OSMOND'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_fey(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEY', id='1')
    verify_results(client, jwt,
       query='FAY',
       expected=[
           {'name': '----FAY'},
           {'name': 'FEY'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_venizia(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'VENIZIA', id='1')
    seed_database_with(solr, 'VENEZIA', id='2')
    seed_database_with(solr, 'VANSEA', id='3')
    seed_database_with(solr, 'WENSO', id='4')
    verify_results(client, jwt,
       query='VENIZIA',
       expected=[
           {'name': '----VENIZIA'},
           {'name': 'VENEZIA'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_ys_and_is(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'CRYSTAL'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOLDSMITHS', id='1')
    verify_results(client, jwt,
       query='COLDSTREAM',
       expected=[
           {'name': '----COLDSTREAM'},
           {'name': 'KOLDSMITHS'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks_again(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRAZY', id='1')
    seed_database_with(solr, 'KAIZEN', id='2')
    verify_results(client, jwt,
       query='CAYZEN',
       expected=[
           {'name': '----CAYZEN'},
           {'name': 'KAIZEN'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_short_word(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FE', id='1')
    verify_results(client, jwt,
       query='FA',
       expected=[
           {'name': '----FA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_single_vowel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEDS', id='1')
    verify_results(client, jwt,
       query='FADS',
       expected=[
           {'name': '----FADS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_feel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEEL', id='1')
    verify_results(client, jwt,
       query='FILL',
       expected=[
           {'name': '----FILL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_bear(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEAR', id='1')
    verify_results(client, jwt,
       query='BARE',
       expected=[
           {'name': '----BARE'},
           {'name': 'BEAR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_corp(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_designation_in_query_is_ignored(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FINGER LIMATED', id='1')
    verify_results(client, jwt,
       query='SUN LIMITED',
       expected=[
           {'name': '----SUN'}
       ]
    )


@integration_synonym_api
@integration_solr
def leak(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LEAK', id='1')
    verify_results(client, jwt,
       query='LEEK',
       expected=[
           {'name': 'LEAK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_plank(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PLANCK', id='1')
    verify_results(client, jwt,
       query='PLANK',
       expected=[
           {'name': '----PLANK'},
           {'name': 'PLANCK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_krystal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_christal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CHRISTAL',
       expected=[
           {'name': '----CHRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kl(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KLASS', id='1')
    verify_results(client, jwt,
       query='CLASS',
       expected=[
           {'name': '----CLASS'},
           {'name': 'KLASS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pheel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PHEEL', id='1')
    verify_results(client, jwt,
       query='FEEL',
       expected=[
           {'name': '----FEEL'},
           {'name': 'PHEEL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ghable(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GHABLE', id='1')
    verify_results(client, jwt,
       query='GABLE',
       expected=[
           {'name': '----GABLE'},
           {'name': 'GHABLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_gnat(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'GNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'KNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PNEU', id='1')
    verify_results(client, jwt,
       query='NEU',
       expected=[
           {'name': '----NEU'},
           {'name': 'PNEU'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wr(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WREN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'WREN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_rh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'RHEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_soft_c_is_not_k(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KIRK', id='1')
    verify_results(client, jwt,
       query='CIRCLE',
       expected=[
           {'name': '----CIRCLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_oi_oy(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'OYSTER', id='1')
    verify_results(client, jwt,
       query='OISTER',
       expected=[
           {'name': '----OISTER'},
           {'name': 'OYSTER'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_dont_add_match_twice(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN GNAT', id='1')
    verify_results(client, jwt,
       query='REN NAT',
       expected=[
           {'name': '----REN NAT'},
           {'name': 'RHEN GNAT'},
           {'name': '----REN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_neighbour(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'NEIGHBOUR', id='1')
    verify_results(client, jwt,
       query='NAYBOR',
       expected=[
           {'name': '----NAYBOR'},
           {'name': 'NEIGHBOUR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_mac_mc(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'MCGREGOR', id='1')
    verify_results(client, jwt,
       query='MACGREGOR',
       expected=[
           {'name': '----MACGREGOR'},
           {'name': 'MCGREGOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ex_x(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'EXTREME', id='1')
    verify_results(client, jwt,
       query='XTREME',
       expected=[
           {'name': '----XTREME'},
           {'name': 'EXTREME'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WHITE', id='1')
    verify_results(client, jwt,
       query='WITE',
       expected=[
           {'name': '----WITE'},
           {'name': 'WHITE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_qu(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KWIK', id='1')
    verify_results(client, jwt,
       query='QUICK',
       expected=[
           {'name': '----QUICK'},
           {'name': 'KWIK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ps(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PSYCHO', id='1')
    verify_results(client, jwt,
       query='SYCHO',
       expected=[
           {'name': '----SYCHO'},
           {'name': 'PSYCHO'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_terra(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TERRA', id='1')
    verify_results(client, jwt,
       query='TARA',
       expected=[
           {'name': 'TERRA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ayaan(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AYAAN', id='1')
    verify_results(client, jwt,
       query='AYAN',
       expected=[
           {'name': '----AYAN'},
           {'name': 'AYAAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_aggri(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AGGRI', id='1')
    verify_results(client, jwt,
       query='AGRI',
       expected=[
           {'name': '----AGRI'},
           {'name': 'AGGRI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kofi(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOFI', id='1')
    verify_results(client, jwt,
       query='COFFI',
       expected=[
           {'name': '----COFFI'},
           {'name': 'KOFI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_tru(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TRU', id='1')
    verify_results(client, jwt,
       query='TRUE',
       expected=[
           {'name': '----TRUE'},
           {'name': 'TRU'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_dymond(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DYMOND', id='1')
    verify_results(client, jwt,
       query='DIAMOND',
       expected=[
           {'name': 'DYMOND'}
       ]
    )


@pytest.mark.skip(reason=""compound words not handled yet"")
@integration_synonym_api
@integration_solr
def test_bee_kleen(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEE KLEEN', id='1')
    verify_results(client, jwt,
       query='BE-CLEAN',
       expected=[
           {'name': 'BEE KLEEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_exact_match_keep_phonetic(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1')
    seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2')
    verify_results(client, jwt,
       query='BLUEPRINT BEAUTY',
       expected=[
           {'name': '----BLUEPRINT BEAUTY'},
           {'name': 'BLUEPRINT BEAUTEE'},
           {'name': '----BLUEPRINT synonyms:(BEAUTI)'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_both_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1')
    verify_results(client, jwt,
       query='INTERVENTION BEHAVIOUR',
       expected=[
           {'name': '----INTERVENTION BEHAVIOUR'},
           {'name': '----INTERVENTION'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_at_right_level(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resists_qword_matching_several_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR BEHAVIOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_a(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AILEEN ENTERPRISES', id='1')
    verify_results(client, jwt,
       query='ALAN HARGREAVES CORPORATION',
       expected=[
           {'name': '----ALAN HARGREAVES'},
           {'name': '----ALAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_e(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ACME', id='1')
    verify_results(client, jwt,
       query='EQUIOM',
       expected=[
           {'name': '----EQUIOM'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_not_match_consonant(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1')
    seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2')
    verify_results(client, jwt,
       query='EH',
       expected=[
           {'name': '----EH'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_unusual_result(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1')
    verify_results(client, jwt,
       query='TABLE',
       expected=[
           {'name': '----TABLE'}
       ]
    )

@integration_synonym_api
@integration_solr
def test_stack_ignores_wildcards(client, jwt, app):
    verify_results(client, jwt,
        query=""TESTING* @WILDCARDS"",
        expected=[
            {'name': '----TESTING WILDCARDS'},
            {'name': '----TESTING'}
        ]
    )

@integration_synonym_api
@integration_solr
@pytest.mark.parametrize(""query"", [
    ('T.H.E.'),
    ('COMPANY'),
    ('ASSN'),
    ('THAT'),
    ('LIMITED CORP.'),
])
def test_query_stripped_to_empty_string(solr,client, jwt, query):
    clean_database(solr)
    seed_database_with(solr, 'JM Van Damme inc', id='1')
    seed_database_with(solr, 'SOME RANDOM NAME', id='2')
    verify_results(client, jwt,
        query=query,
        expected=[{'name':'----*'}]
    )
/n/n/n",1
68,22e3ab28b73a4de7a2a065d657b017ccbac352d8,"lib/galaxy/jobs/runners/lwr.py/n/nimport logging

from galaxy import model
from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner
from galaxy.jobs import ComputeEnvironment
from galaxy.jobs import JobDestination
from galaxy.jobs.command_factory import build_command
from galaxy.tools.deps import dependencies
from galaxy.util import string_as_bool_or_none
from galaxy.util.bunch import Bunch

import errno
from time import sleep
import os

from .lwr_client import build_client_manager
from .lwr_client import url_to_destination_params
from .lwr_client import finish_job as lwr_finish_job
from .lwr_client import submit_job as lwr_submit_job
from .lwr_client import ClientJobDescription
from .lwr_client import LwrOutputs
from .lwr_client import ClientOutputs
from .lwr_client import PathMapper

log = logging.getLogger( __name__ )

__all__ = [ 'LwrJobRunner' ]

NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = ""LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.""
NO_REMOTE_DATATYPES_CONFIG = ""LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.""

# Is there a good way to infer some default for this? Can only use
# url_for from web threads. https://gist.github.com/jmchilton/9098762
DEFAULT_GALAXY_URL = ""http://localhost:8080""


class LwrJobRunner( AsynchronousJobRunner ):
    """"""
    LWR Job Runner
    """"""
    runner_name = ""LWRRunner""

    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):
        """"""Start the job runner """"""
        super( LwrJobRunner, self ).__init__( app, nworkers )
        self.async_status_updates = dict()
        self._init_monitor_thread()
        self._init_worker_threads()
        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), ""url"": url}
        self.galaxy_url = galaxy_url
        self.client_manager = build_client_manager(**client_manager_kwargs)

    def url_to_destination( self, url ):
        """"""Convert a legacy URL to a job destination""""""
        return JobDestination( runner=""lwr"", params=url_to_destination_params( url ) )

    def check_watched_item(self, job_state):
        try:
            client = self.get_client_from_state(job_state)

            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):
                # Message queue implementation.

                # TODO: Very hacky now, refactor after Dannon merges in his
                # message queue work, runners need the ability to disable
                # check_watched_item like this and instead a callback needs to
                # be issued post job recovery allowing a message queue
                # consumer to be setup.
                self.client_manager.ensure_has_status_update_callback(self.__async_update)
                return job_state

            status = client.get_status()
        except Exception:
            # An orphaned job was put into the queue at app startup, so remote server went down
            # either way we are done I guess.
            self.mark_as_finished(job_state)
            return None
        job_state = self.__update_job_state_for_lwr_status(job_state, status)
        return job_state

    def __update_job_state_for_lwr_status(self, job_state, lwr_status):
        if lwr_status == ""complete"":
            self.mark_as_finished(job_state)
            return None
        if lwr_status == ""running"" and not job_state.running:
            job_state.running = True
            job_state.job_wrapper.change_state( model.Job.states.RUNNING )
        return job_state

    def __async_update( self, full_status ):
        job_id = full_status[ ""job_id"" ]
        job_state = self.__find_watched_job( job_id )
        if not job_state:
            # Probably finished too quickly, sleep and try again.
            # Kind of a hack, why does monitor queue need to no wait
            # get and sleep instead of doing a busy wait that would
            # respond immediately.
            sleep( 2 )
            job_state = self.__find_watched_job( job_id )
        if not job_state:
            log.warn( ""Failed to find job corresponding to final status %s in %s"" % ( full_status, self.watched ) )
        else:
            self.__update_job_state_for_lwr_status(job_state, full_status[""status""])

    def __find_watched_job( self, job_id ):
        found_job = None
        for async_job_state in self.watched:
            if str( async_job_state.job_id ) == job_id:
                found_job = async_job_state
                break
        return found_job

    def queue_job(self, job_wrapper):
        job_destination = job_wrapper.job_destination

        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )

        if not command_line:
            return

        try:
            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )
            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )
            unstructured_path_rewrites = {}
            if compute_environment:
                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites

            client_job_description = ClientJobDescription(
                command_line=command_line,
                input_files=self.get_input_files(job_wrapper),
                client_outputs=self.__client_outputs(client, job_wrapper),
                working_directory=job_wrapper.working_directory,
                tool=job_wrapper.tool,
                config_files=job_wrapper.extra_filenames,
                dependencies_description=dependencies_description,
                env=client.env,
                rewrite_paths=rewrite_paths,
                arbitrary_files=unstructured_path_rewrites,
            )
            job_id = lwr_submit_job(client, client_job_description, remote_job_config)
            log.info(""lwr job submitted with job_id %s"" % job_id)
            job_wrapper.set_job_destination( job_destination, job_id )
            job_wrapper.change_state( model.Job.states.QUEUED )
        except Exception:
            job_wrapper.fail( ""failure running job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)
            return

        lwr_job_state = AsynchronousJobState()
        lwr_job_state.job_wrapper = job_wrapper
        lwr_job_state.job_id = job_id
        lwr_job_state.old_state = True
        lwr_job_state.running = False
        lwr_job_state.job_destination = job_destination
        self.monitor_job(lwr_job_state)

    def __prepare_job(self, job_wrapper, job_destination):
        """""" Build command-line and LWR client for this job. """"""
        command_line = None
        client = None
        remote_job_config = None
        compute_environment = None
        try:
            client = self.get_client_from_wrapper(job_wrapper)
            tool = job_wrapper.tool
            remote_job_config = client.setup(tool.id, tool.version)
            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )
            prepare_kwds = {}
            if rewrite_parameters:
                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )
                prepare_kwds[ 'compute_environment' ] = compute_environment
            job_wrapper.prepare( **prepare_kwds )
            self.__prepare_input_files_locally(job_wrapper)
            remote_metadata = LwrJobRunner.__remote_metadata( client )
            dependency_resolution = LwrJobRunner.__dependency_resolution( client )
            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)
            remote_command_params = dict(
                working_directory=remote_job_config['working_directory'],
                metadata_kwds=metadata_kwds,
                dependency_resolution=dependency_resolution,
            )
            command_line = build_command(
                self,
                job_wrapper=job_wrapper,
                include_metadata=remote_metadata,
                include_work_dir_outputs=False,
                remote_command_params=remote_command_params,
            )
        except Exception:
            job_wrapper.fail( ""failure preparing job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)

        # If we were able to get a command line, run the job
        if not command_line:
            job_wrapper.finish( '', '' )

        return command_line, client, remote_job_config, compute_environment

    def __prepare_input_files_locally(self, job_wrapper):
        """"""Run task splitting commands locally.""""""
        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)
        if prepare_input_files_cmds is not None:
            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files
                if 0 != os.system(cmd):
                    raise Exception('Error running file staging command: %s' % cmd)
            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line

    def get_output_files(self, job_wrapper):
        output_paths = job_wrapper.get_output_fnames()
        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.

    def get_input_files(self, job_wrapper):
        input_paths = job_wrapper.get_input_paths()
        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.

    def get_client_from_wrapper(self, job_wrapper):
        job_id = job_wrapper.job_id
        if hasattr(job_wrapper, 'task_id'):
            job_id = ""%s_%s"" % (job_id, job_wrapper.task_id)
        params = job_wrapper.job_destination.params.copy()
        for key, value in params.iteritems():
            if value:
                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )
        env = getattr( job_wrapper.job_destination, ""env"", [] )
        return self.get_client( params, job_id, env )

    def get_client_from_state(self, job_state):
        job_destination_params = job_state.job_destination.params
        job_id = job_state.job_id
        return self.get_client( job_destination_params, job_id )

    def get_client( self, job_destination_params, job_id, env=[] ):
        # Cannot use url_for outside of web thread.
        #files_endpoint = url_for( controller=""job_files"", job_id=encoded_job_id )

        encoded_job_id = self.app.security.encode_id(job_id)
        job_key = self.app.security.encode_id( job_id, kind=""jobs_files"" )
        files_endpoint = ""%s/api/jobs/%s/files?job_key=%s"" % (
            self.galaxy_url,
            encoded_job_id,
            job_key
        )
        get_client_kwds = dict(
            job_id=str( job_id ),
            files_endpoint=files_endpoint,
            env=env
        )
        return self.client_manager.get_client( job_destination_params, **get_client_kwds )

    def finish_job( self, job_state ):
        stderr = stdout = ''
        job_wrapper = job_state.job_wrapper
        try:
            client = self.get_client_from_state(job_state)
            run_results = client.full_status()

            stdout = run_results.get('stdout', '')
            stderr = run_results.get('stderr', '')
            exit_code = run_results.get('returncode', None)
            lwr_outputs = LwrOutputs.from_status_response(run_results)
            # Use LWR client code to transfer/copy files back
            # and cleanup job if needed.
            completed_normally = \
                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]
            cleanup_job = self.app.config.cleanup_job
            client_outputs = self.__client_outputs(client, job_wrapper)
            finish_args = dict( client=client,
                                job_completed_normally=completed_normally,
                                cleanup_job=cleanup_job,
                                client_outputs=client_outputs,
                                lwr_outputs=lwr_outputs )
            failed = lwr_finish_job( **finish_args )

            if failed:
                job_wrapper.fail(""Failed to find or download one or more job outputs from remote server."", exception=True)
        except Exception:
            message = ""Failed to communicate with remote job server.""
            job_wrapper.fail( message, exception=True )
            log.exception(""failure finishing job %d"" % job_wrapper.job_id)
            return
        if not LwrJobRunner.__remote_metadata( client ):
            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )
        # Finish the job
        try:
            job_wrapper.finish( stdout, stderr, exit_code )
        except Exception:
            log.exception(""Job wrapper finish method failed"")
            job_wrapper.fail(""Unable to finish job"", exception=True)

    def fail_job( self, job_state ):
        """"""
        Seperated out so we can use the worker threads for it.
        """"""
        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )
        job_state.job_wrapper.fail( job_state.fail_message )

    def check_pid( self, pid ):
        try:
            os.kill( pid, 0 )
            return True
        except OSError, e:
            if e.errno == errno.ESRCH:
                log.debug( ""check_pid(): PID %d is dead"" % pid )
            else:
                log.warning( ""check_pid(): Got errno %s when attempting to check PID %d: %s"" % ( errno.errorcode[e.errno], pid, e.strerror ) )
            return False

    def stop_job( self, job ):
        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished
        job_ext_output_metadata = job.get_external_output_metadata()
        if job_ext_output_metadata:
            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them
            if pid in [ None, '' ]:
                log.warning( ""stop_job(): %s: no PID in database for job, unable to stop"" % job.id )
                return
            pid = int( pid )
            if not self.check_pid( pid ):
                log.warning( ""stop_job(): %s: PID %d was already dead or can't be signaled"" % ( job.id, pid ) )
                return
            for sig in [ 15, 9 ]:
                try:
                    os.killpg( pid, sig )
                except OSError, e:
                    log.warning( ""stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s"" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )
                    return  # give up
                sleep( 2 )
                if not self.check_pid( pid ):
                    log.debug( ""stop_job(): %s: PID %d successfully killed with signal %d"" % ( job.id, pid, sig ) )
                    return
                else:
                    log.warning( ""stop_job(): %s: PID %d refuses to die after signaling TERM/KILL"" % ( job.id, pid ) )
        else:
            # Remote kill
            lwr_url = job.job_runner_name
            job_id = job.job_runner_external_id
            log.debug(""Attempt remote lwr kill of job with url %s and id %s"" % (lwr_url, job_id))
            client = self.get_client(job.destination_params, job_id)
            client.kill()

    def recover( self, job, job_wrapper ):
        """"""Recovers jobs stuck in the queued/running state when Galaxy started""""""
        job_state = AsynchronousJobState()
        job_state.job_id = str( job.get_job_runner_external_id() )
        job_state.runner_url = job_wrapper.get_job_runner_url()
        job_state.job_destination = job_wrapper.job_destination
        job_wrapper.command_line = job.get_command_line()
        job_state.job_wrapper = job_wrapper
        state = job.get_state()
        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:
            log.debug( ""(LWR/%s) is still in running state, adding to the LWR queue"" % ( job.get_id()) )
            job_state.old_state = True
            job_state.running = state == model.Job.states.RUNNING
            self.monitor_queue.put( job_state )

    def shutdown( self ):
        super( LwrJobRunner, self ).shutdown()
        self.client_manager.shutdown()

    def __client_outputs( self, client, job_wrapper ):
        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )
        output_files = self.get_output_files( job_wrapper )
        client_outputs = ClientOutputs(
            working_directory=job_wrapper.working_directory,
            work_dir_outputs=work_dir_outputs,
            output_files=output_files,
            version_file=job_wrapper.get_version_string_path(),
        )
        return client_outputs

    @staticmethod
    def __dependencies_description( lwr_client, job_wrapper ):
        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )
        remote_dependency_resolution = dependency_resolution == ""remote""
        if not remote_dependency_resolution:
            return None
        requirements = job_wrapper.tool.requirements or []
        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []
        return dependencies.DependenciesDescription(
            requirements=requirements,
            installed_tool_dependencies=installed_tool_dependencies,
        )

    @staticmethod
    def __dependency_resolution( lwr_client ):
        dependency_resolution = lwr_client.destination_params.get( ""dependency_resolution"", ""local"" )
        if dependency_resolution not in [""none"", ""local"", ""remote""]:
            raise Exception(""Unknown dependency_resolution value encountered %s"" % dependency_resolution)
        return dependency_resolution

    @staticmethod
    def __remote_metadata( lwr_client ):
        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( ""remote_metadata"", False ) )
        return remote_metadata

    @staticmethod
    def __use_remote_datatypes_conf( lwr_client ):
        """""" When setting remote metadata, use integrated datatypes from this
        Galaxy instance or use the datatypes config configured via the remote
        LWR.

        Both options are broken in different ways for same reason - datatypes
        may not match. One can push the local datatypes config to the remote
        server - but there is no guarentee these datatypes will be defined
        there. Alternatively, one can use the remote datatype config - but
        there is no guarentee that it will contain all the datatypes available
        to this Galaxy.
        """"""
        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( ""use_remote_datatypes"", False ) )
        return use_remote_datatypes

    @staticmethod
    def __rewrite_parameters( lwr_client ):
        return string_as_bool_or_none( lwr_client.destination_params.get( ""rewrite_parameters"", False ) ) or False

    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):
        metadata_kwds = {}
        if remote_metadata:
            remote_system_properties = remote_job_config.get(""system_properties"", {})
            remote_galaxy_home = remote_system_properties.get(""galaxy_home"", None)
            if not remote_galaxy_home:
                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)
            metadata_kwds['exec_dir'] = remote_galaxy_home
            outputs_directory = remote_job_config['outputs_directory']
            configs_directory = remote_job_config['configs_directory']
            working_directory = remote_job_config['working_directory']
            # For metadata calculation, we need to build a list of of output
            # file objects with real path indicating location on Galaxy server
            # and false path indicating location on compute server. Since the
            # LWR disables from_work_dir copying as part of the job command
            # line we need to take the list of output locations on the LWR
            # server (produced by self.get_output_files(job_wrapper)) and for
            # each work_dir output substitute the effective path on the LWR
            # server relative to the remote working directory as the
            # false_path to send the metadata command generation module.
            work_dir_outputs = self.get_work_dir_outputs(job_wrapper, job_working_directory=working_directory)
            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]
            for output in outputs:
                for lwr_workdir_path, real_path in work_dir_outputs:
                    if real_path == output.real_path:
                        output.false_path = lwr_workdir_path
            metadata_kwds['output_fnames'] = outputs
            metadata_kwds['compute_tmp_dir'] = working_directory
            metadata_kwds['config_root'] = remote_galaxy_home
            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')
            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)
            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)
            if LwrJobRunner.__use_remote_datatypes_conf( client ):
                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)
                if not remote_datatypes_config:
                    log.warn(NO_REMOTE_DATATYPES_CONFIG)
                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')
                metadata_kwds['datatypes_config'] = remote_datatypes_config
            else:
                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs
                # Ensure this file gets pushed out to the remote config dir.
                job_wrapper.extra_filenames.append(integrates_datatypes_config)

                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))
        return metadata_kwds


class LwrComputeEnvironment( ComputeEnvironment ):

    def __init__( self, lwr_client, job_wrapper, remote_job_config ):
        self.lwr_client = lwr_client
        self.job_wrapper = job_wrapper
        self.local_path_config = job_wrapper.default_compute_environment()
        self.unstructured_path_rewrites = {}
        # job_wrapper.prepare is going to expunge the job backing the following
        # computations, so precalculate these paths.
        self._wrapper_input_paths = self.local_path_config.input_paths()
        self._wrapper_output_paths = self.local_path_config.output_paths()
        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())
        self._config_directory = remote_job_config[ ""configs_directory"" ]
        self._working_directory = remote_job_config[ ""working_directory"" ]
        self._sep = remote_job_config[ ""system_properties"" ][ ""separator"" ]
        self._tool_dir = remote_job_config[ ""tools_directory"" ]
        version_path = self.local_path_config.version_path()
        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)
        if new_version_path:
            version_path = new_version_path
        self._version_path = version_path

    def output_paths( self ):
        local_output_paths = self._wrapper_output_paths

        results = []
        for local_output_path in local_output_paths:
            wrapper_path = str( local_output_path )
            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_output_path, remote_path ) )
        return results

    def input_paths( self ):
        local_input_paths = self._wrapper_input_paths

        results = []
        for local_input_path in local_input_paths:
            wrapper_path = str( local_input_path )
            # This will over-copy in some cases. For instance in the case of task
            # splitting, this input will be copied even though only the work dir
            # input will actually be used.
            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_input_path, remote_path ) )
        return results

    def _dataset_path( self, local_dataset_path, remote_path ):
        remote_extra_files_path = None
        if remote_path:
            remote_extra_files_path = ""%s_files"" % remote_path[ 0:-len( "".dat"" ) ]
        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )

    def working_directory( self ):
        return self._working_directory

    def config_directory( self ):
        return self._config_directory

    def new_file_path( self ):
        return self.working_directory()  # Problems with doing this?

    def sep( self ):
        return self._sep

    def version_path( self ):
        return self._version_path

    def rewriter( self, parameter_value ):
        unstructured_path_rewrites = self.unstructured_path_rewrites
        if parameter_value in unstructured_path_rewrites:
            # Path previously mapped, use previous mapping.
            return unstructured_path_rewrites[ parameter_value ]
        if parameter_value in unstructured_path_rewrites.itervalues():
            # Path is a rewritten remote path (this might never occur,
            # consider dropping check...)
            return parameter_value

        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )
        if rewrite:
            unstructured_path_rewrites.update(new_unstructured_path_rewrites)
            return rewrite
        else:
            # Did need to rewrite, use original path or value.
            return parameter_value

    def unstructured_path_rewriter( self ):
        return self.rewriter
/n/n/n",0
69,22e3ab28b73a4de7a2a065d657b017ccbac352d8,"/lib/galaxy/jobs/runners/lwr.py/n/nimport logging

from galaxy import model
from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner
from galaxy.jobs import ComputeEnvironment
from galaxy.jobs import JobDestination
from galaxy.jobs.command_factory import build_command
from galaxy.tools.deps import dependencies
from galaxy.util import string_as_bool_or_none
from galaxy.util.bunch import Bunch

import errno
from time import sleep
import os

from .lwr_client import build_client_manager
from .lwr_client import url_to_destination_params
from .lwr_client import finish_job as lwr_finish_job
from .lwr_client import submit_job as lwr_submit_job
from .lwr_client import ClientJobDescription
from .lwr_client import LwrOutputs
from .lwr_client import ClientOutputs
from .lwr_client import PathMapper

log = logging.getLogger( __name__ )

__all__ = [ 'LwrJobRunner' ]

NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = ""LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.""
NO_REMOTE_DATATYPES_CONFIG = ""LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.""

# Is there a good way to infer some default for this? Can only use
# url_for from web threads. https://gist.github.com/jmchilton/9098762
DEFAULT_GALAXY_URL = ""http://localhost:8080""


class LwrJobRunner( AsynchronousJobRunner ):
    """"""
    LWR Job Runner
    """"""
    runner_name = ""LWRRunner""

    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):
        """"""Start the job runner """"""
        super( LwrJobRunner, self ).__init__( app, nworkers )
        self.async_status_updates = dict()
        self._init_monitor_thread()
        self._init_worker_threads()
        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), ""url"": url}
        self.galaxy_url = galaxy_url
        self.client_manager = build_client_manager(**client_manager_kwargs)

    def url_to_destination( self, url ):
        """"""Convert a legacy URL to a job destination""""""
        return JobDestination( runner=""lwr"", params=url_to_destination_params( url ) )

    def check_watched_item(self, job_state):
        try:
            client = self.get_client_from_state(job_state)

            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):
                # Message queue implementation.

                # TODO: Very hacky now, refactor after Dannon merges in his
                # message queue work, runners need the ability to disable
                # check_watched_item like this and instead a callback needs to
                # be issued post job recovery allowing a message queue
                # consumer to be setup.
                self.client_manager.ensure_has_status_update_callback(self.__async_update)
                return job_state

            status = client.get_status()
        except Exception:
            # An orphaned job was put into the queue at app startup, so remote server went down
            # either way we are done I guess.
            self.mark_as_finished(job_state)
            return None
        job_state = self.__update_job_state_for_lwr_status(job_state, status)
        return job_state

    def __update_job_state_for_lwr_status(self, job_state, lwr_status):
        if lwr_status == ""complete"":
            self.mark_as_finished(job_state)
            return None
        if lwr_status == ""running"" and not job_state.running:
            job_state.running = True
            job_state.job_wrapper.change_state( model.Job.states.RUNNING )
        return job_state

    def __async_update( self, full_status ):
        job_id = full_status[ ""job_id"" ]
        job_state = self.__find_watched_job( job_id )
        if not job_state:
            # Probably finished too quickly, sleep and try again.
            # Kind of a hack, why does monitor queue need to no wait
            # get and sleep instead of doing a busy wait that would
            # respond immediately.
            sleep( 2 )
            job_state = self.__find_watched_job( job_id )
        if not job_state:
            log.warn( ""Failed to find job corresponding to final status %s in %s"" % ( full_status, self.watched ) )
        else:
            self.__update_job_state_for_lwr_status(job_state, full_status[""status""])

    def __find_watched_job( self, job_id ):
        found_job = None
        for async_job_state in self.watched:
            if str( async_job_state.job_id ) == job_id:
                found_job = async_job_state
                break
        return found_job

    def queue_job(self, job_wrapper):
        job_destination = job_wrapper.job_destination

        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )

        if not command_line:
            return

        try:
            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )
            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )
            unstructured_path_rewrites = {}
            if compute_environment:
                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites

            client_job_description = ClientJobDescription(
                command_line=command_line,
                input_files=self.get_input_files(job_wrapper),
                client_outputs=self.__client_outputs(client, job_wrapper),
                working_directory=job_wrapper.working_directory,
                tool=job_wrapper.tool,
                config_files=job_wrapper.extra_filenames,
                dependencies_description=dependencies_description,
                env=client.env,
                rewrite_paths=rewrite_paths,
                arbitrary_files=unstructured_path_rewrites,
            )
            job_id = lwr_submit_job(client, client_job_description, remote_job_config)
            log.info(""lwr job submitted with job_id %s"" % job_id)
            job_wrapper.set_job_destination( job_destination, job_id )
            job_wrapper.change_state( model.Job.states.QUEUED )
        except Exception:
            job_wrapper.fail( ""failure running job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)
            return

        lwr_job_state = AsynchronousJobState()
        lwr_job_state.job_wrapper = job_wrapper
        lwr_job_state.job_id = job_id
        lwr_job_state.old_state = True
        lwr_job_state.running = False
        lwr_job_state.job_destination = job_destination
        self.monitor_job(lwr_job_state)

    def __prepare_job(self, job_wrapper, job_destination):
        """""" Build command-line and LWR client for this job. """"""
        command_line = None
        client = None
        remote_job_config = None
        compute_environment = None
        try:
            client = self.get_client_from_wrapper(job_wrapper)
            tool = job_wrapper.tool
            remote_job_config = client.setup(tool.id, tool.version)
            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )
            prepare_kwds = {}
            if rewrite_parameters:
                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )
                prepare_kwds[ 'compute_environment' ] = compute_environment
            job_wrapper.prepare( **prepare_kwds )
            self.__prepare_input_files_locally(job_wrapper)
            remote_metadata = LwrJobRunner.__remote_metadata( client )
            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
            dependency_resolution = LwrJobRunner.__dependency_resolution( client )
            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)
            remote_command_params = dict(
                working_directory=remote_job_config['working_directory'],
                metadata_kwds=metadata_kwds,
                dependency_resolution=dependency_resolution,
            )
            command_line = build_command(
                self,
                job_wrapper=job_wrapper,
                include_metadata=remote_metadata,
                include_work_dir_outputs=remote_work_dir_copy,
                remote_command_params=remote_command_params,
            )
        except Exception:
            job_wrapper.fail( ""failure preparing job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)

        # If we were able to get a command line, run the job
        if not command_line:
            job_wrapper.finish( '', '' )

        return command_line, client, remote_job_config, compute_environment

    def __prepare_input_files_locally(self, job_wrapper):
        """"""Run task splitting commands locally.""""""
        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)
        if prepare_input_files_cmds is not None:
            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files
                if 0 != os.system(cmd):
                    raise Exception('Error running file staging command: %s' % cmd)
            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line

    def get_output_files(self, job_wrapper):
        output_paths = job_wrapper.get_output_fnames()
        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.

    def get_input_files(self, job_wrapper):
        input_paths = job_wrapper.get_input_paths()
        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.

    def get_client_from_wrapper(self, job_wrapper):
        job_id = job_wrapper.job_id
        if hasattr(job_wrapper, 'task_id'):
            job_id = ""%s_%s"" % (job_id, job_wrapper.task_id)
        params = job_wrapper.job_destination.params.copy()
        for key, value in params.iteritems():
            if value:
                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )
        env = getattr( job_wrapper.job_destination, ""env"", [] )
        return self.get_client( params, job_id, env )

    def get_client_from_state(self, job_state):
        job_destination_params = job_state.job_destination.params
        job_id = job_state.job_id
        return self.get_client( job_destination_params, job_id )

    def get_client( self, job_destination_params, job_id, env=[] ):
        # Cannot use url_for outside of web thread.
        #files_endpoint = url_for( controller=""job_files"", job_id=encoded_job_id )

        encoded_job_id = self.app.security.encode_id(job_id)
        job_key = self.app.security.encode_id( job_id, kind=""jobs_files"" )
        files_endpoint = ""%s/api/jobs/%s/files?job_key=%s"" % (
            self.galaxy_url,
            encoded_job_id,
            job_key
        )
        get_client_kwds = dict(
            job_id=str( job_id ),
            files_endpoint=files_endpoint,
            env=env
        )
        return self.client_manager.get_client( job_destination_params, **get_client_kwds )

    def finish_job( self, job_state ):
        stderr = stdout = ''
        job_wrapper = job_state.job_wrapper
        try:
            client = self.get_client_from_state(job_state)
            run_results = client.full_status()

            stdout = run_results.get('stdout', '')
            stderr = run_results.get('stderr', '')
            exit_code = run_results.get('returncode', None)
            lwr_outputs = LwrOutputs.from_status_response(run_results)
            # Use LWR client code to transfer/copy files back
            # and cleanup job if needed.
            completed_normally = \
                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]
            cleanup_job = self.app.config.cleanup_job
            client_outputs = self.__client_outputs(client, job_wrapper)
            finish_args = dict( client=client,
                                job_completed_normally=completed_normally,
                                cleanup_job=cleanup_job,
                                client_outputs=client_outputs,
                                lwr_outputs=lwr_outputs )
            failed = lwr_finish_job( **finish_args )

            if failed:
                job_wrapper.fail(""Failed to find or download one or more job outputs from remote server."", exception=True)
        except Exception:
            message = ""Failed to communicate with remote job server.""
            job_wrapper.fail( message, exception=True )
            log.exception(""failure finishing job %d"" % job_wrapper.job_id)
            return
        if not LwrJobRunner.__remote_metadata( client ):
            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )
        # Finish the job
        try:
            job_wrapper.finish( stdout, stderr, exit_code )
        except Exception:
            log.exception(""Job wrapper finish method failed"")
            job_wrapper.fail(""Unable to finish job"", exception=True)

    def fail_job( self, job_state ):
        """"""
        Seperated out so we can use the worker threads for it.
        """"""
        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )
        job_state.job_wrapper.fail( job_state.fail_message )

    def check_pid( self, pid ):
        try:
            os.kill( pid, 0 )
            return True
        except OSError, e:
            if e.errno == errno.ESRCH:
                log.debug( ""check_pid(): PID %d is dead"" % pid )
            else:
                log.warning( ""check_pid(): Got errno %s when attempting to check PID %d: %s"" % ( errno.errorcode[e.errno], pid, e.strerror ) )
            return False

    def stop_job( self, job ):
        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished
        job_ext_output_metadata = job.get_external_output_metadata()
        if job_ext_output_metadata:
            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them
            if pid in [ None, '' ]:
                log.warning( ""stop_job(): %s: no PID in database for job, unable to stop"" % job.id )
                return
            pid = int( pid )
            if not self.check_pid( pid ):
                log.warning( ""stop_job(): %s: PID %d was already dead or can't be signaled"" % ( job.id, pid ) )
                return
            for sig in [ 15, 9 ]:
                try:
                    os.killpg( pid, sig )
                except OSError, e:
                    log.warning( ""stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s"" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )
                    return  # give up
                sleep( 2 )
                if not self.check_pid( pid ):
                    log.debug( ""stop_job(): %s: PID %d successfully killed with signal %d"" % ( job.id, pid, sig ) )
                    return
                else:
                    log.warning( ""stop_job(): %s: PID %d refuses to die after signaling TERM/KILL"" % ( job.id, pid ) )
        else:
            # Remote kill
            lwr_url = job.job_runner_name
            job_id = job.job_runner_external_id
            log.debug(""Attempt remote lwr kill of job with url %s and id %s"" % (lwr_url, job_id))
            client = self.get_client(job.destination_params, job_id)
            client.kill()

    def recover( self, job, job_wrapper ):
        """"""Recovers jobs stuck in the queued/running state when Galaxy started""""""
        job_state = AsynchronousJobState()
        job_state.job_id = str( job.get_job_runner_external_id() )
        job_state.runner_url = job_wrapper.get_job_runner_url()
        job_state.job_destination = job_wrapper.job_destination
        job_wrapper.command_line = job.get_command_line()
        job_state.job_wrapper = job_wrapper
        state = job.get_state()
        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:
            log.debug( ""(LWR/%s) is still in running state, adding to the LWR queue"" % ( job.get_id()) )
            job_state.old_state = True
            job_state.running = state == model.Job.states.RUNNING
            self.monitor_queue.put( job_state )

    def shutdown( self ):
        super( LwrJobRunner, self ).shutdown()
        self.client_manager.shutdown()

    def __client_outputs( self, client, job_wrapper ):
        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
        if not remote_work_dir_copy:
            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )
        else:
            # They have already been copied over to look like regular outputs remotely,
            # no need to handle them differently here.
            work_dir_outputs = []
        output_files = self.get_output_files( job_wrapper )
        client_outputs = ClientOutputs(
            working_directory=job_wrapper.working_directory,
            work_dir_outputs=work_dir_outputs,
            output_files=output_files,
            version_file=job_wrapper.get_version_string_path(),
        )
        return client_outputs

    @staticmethod
    def __dependencies_description( lwr_client, job_wrapper ):
        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )
        remote_dependency_resolution = dependency_resolution == ""remote""
        if not remote_dependency_resolution:
            return None
        requirements = job_wrapper.tool.requirements or []
        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []
        return dependencies.DependenciesDescription(
            requirements=requirements,
            installed_tool_dependencies=installed_tool_dependencies,
        )

    @staticmethod
    def __dependency_resolution( lwr_client ):
        dependency_resolution = lwr_client.destination_params.get( ""dependency_resolution"", ""local"" )
        if dependency_resolution not in [""none"", ""local"", ""remote""]:
            raise Exception(""Unknown dependency_resolution value encountered %s"" % dependency_resolution)
        return dependency_resolution

    @staticmethod
    def __remote_metadata( lwr_client ):
        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( ""remote_metadata"", False ) )
        return remote_metadata

    @staticmethod
    def __remote_work_dir_copy( lwr_client ):
        # Right now remote metadata handling assumes from_work_dir outputs
        # have been copied over before it runs. So do that remotely. This is
        # not the default though because adding it to the command line is not
        # cross-platform (no cp on Windows) and it's un-needed work outside
        # the context of metadata settting (just as easy to download from
        # either place.)
        return LwrJobRunner.__remote_metadata( lwr_client )

    @staticmethod
    def __use_remote_datatypes_conf( lwr_client ):
        """""" When setting remote metadata, use integrated datatypes from this
        Galaxy instance or use the datatypes config configured via the remote
        LWR.

        Both options are broken in different ways for same reason - datatypes
        may not match. One can push the local datatypes config to the remote
        server - but there is no guarentee these datatypes will be defined
        there. Alternatively, one can use the remote datatype config - but
        there is no guarentee that it will contain all the datatypes available
        to this Galaxy.
        """"""
        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( ""use_remote_datatypes"", False ) )
        return use_remote_datatypes

    @staticmethod
    def __rewrite_parameters( lwr_client ):
        return string_as_bool_or_none( lwr_client.destination_params.get( ""rewrite_parameters"", False ) ) or False

    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):
        metadata_kwds = {}
        if remote_metadata:
            remote_system_properties = remote_job_config.get(""system_properties"", {})
            remote_galaxy_home = remote_system_properties.get(""galaxy_home"", None)
            if not remote_galaxy_home:
                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)
            metadata_kwds['exec_dir'] = remote_galaxy_home
            outputs_directory = remote_job_config['outputs_directory']
            configs_directory = remote_job_config['configs_directory']
            working_directory = remote_job_config['working_directory']
            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]
            metadata_kwds['output_fnames'] = outputs
            metadata_kwds['compute_tmp_dir'] = working_directory
            metadata_kwds['config_root'] = remote_galaxy_home
            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')
            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)
            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)
            if LwrJobRunner.__use_remote_datatypes_conf( client ):
                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)
                if not remote_datatypes_config:
                    log.warn(NO_REMOTE_DATATYPES_CONFIG)
                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')
                metadata_kwds['datatypes_config'] = remote_datatypes_config
            else:
                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs
                # Ensure this file gets pushed out to the remote config dir.
                job_wrapper.extra_filenames.append(integrates_datatypes_config)

                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))
        return metadata_kwds


class LwrComputeEnvironment( ComputeEnvironment ):

    def __init__( self, lwr_client, job_wrapper, remote_job_config ):
        self.lwr_client = lwr_client
        self.job_wrapper = job_wrapper
        self.local_path_config = job_wrapper.default_compute_environment()
        self.unstructured_path_rewrites = {}
        # job_wrapper.prepare is going to expunge the job backing the following
        # computations, so precalculate these paths.
        self._wrapper_input_paths = self.local_path_config.input_paths()
        self._wrapper_output_paths = self.local_path_config.output_paths()
        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())
        self._config_directory = remote_job_config[ ""configs_directory"" ]
        self._working_directory = remote_job_config[ ""working_directory"" ]
        self._sep = remote_job_config[ ""system_properties"" ][ ""separator"" ]
        self._tool_dir = remote_job_config[ ""tools_directory"" ]
        version_path = self.local_path_config.version_path()
        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)
        if new_version_path:
            version_path = new_version_path
        self._version_path = version_path

    def output_paths( self ):
        local_output_paths = self._wrapper_output_paths

        results = []
        for local_output_path in local_output_paths:
            wrapper_path = str( local_output_path )
            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_output_path, remote_path ) )
        return results

    def input_paths( self ):
        local_input_paths = self._wrapper_input_paths

        results = []
        for local_input_path in local_input_paths:
            wrapper_path = str( local_input_path )
            # This will over-copy in some cases. For instance in the case of task
            # splitting, this input will be copied even though only the work dir
            # input will actually be used.
            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_input_path, remote_path ) )
        return results

    def _dataset_path( self, local_dataset_path, remote_path ):
        remote_extra_files_path = None
        if remote_path:
            remote_extra_files_path = ""%s_files"" % remote_path[ 0:-len( "".dat"" ) ]
        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )

    def working_directory( self ):
        return self._working_directory

    def config_directory( self ):
        return self._config_directory

    def new_file_path( self ):
        return self.working_directory()  # Problems with doing this?

    def sep( self ):
        return self._sep

    def version_path( self ):
        return self._version_path

    def rewriter( self, parameter_value ):
        unstructured_path_rewrites = self.unstructured_path_rewrites
        if parameter_value in unstructured_path_rewrites:
            # Path previously mapped, use previous mapping.
            return unstructured_path_rewrites[ parameter_value ]
        if parameter_value in unstructured_path_rewrites.itervalues():
            # Path is a rewritten remote path (this might never occur,
            # consider dropping check...)
            return parameter_value

        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )
        if rewrite:
            unstructured_path_rewrites.update(new_unstructured_path_rewrites)
            return rewrite
        else:
            # Did need to rewrite, use original path or value.
            return parameter_value

    def unstructured_path_rewriter( self ):
        return self.rewriter
/n/n/n",1
70,7ff203be36e439b535894764c37a8446351627ec,"lib/Shine/Commands/Base/Command.py/n/n# Command.py -- Base command class
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Support.Debug import Debug

from CommandRCDefs import *

import getopt


#
# Command exceptions are defined in Shine.Command.Exceptions
#

class Command(object):
    """"""
    The base class for command objects that can be added to the commands
    registry.
    """"""
    def __init__(self):
        self.options = {}
        self.getopt_string = """"
        self.params_desc = """"
        self.last_optional = 0
        self.arguments = None

        # All commands have debug support.
        self.debug_support = Debug(self)

    def is_hidden(self):
        """"""Return whether the command should not be displayed to user.""""""
        return False
    
    def get_name(self):
        raise NotImplementedError(""Derived classes must implement."")

    def get_desc(self):
        return ""Undocumented""

    def get_params_desc(self):
        pdesc = self.params_desc.strip()
        if self.has_subcommand():
            return ""%s %s"" % ('|'.join(self.get_subcommands()), pdesc)
        return pdesc

    def has_subcommand(self):
        """"""Return whether the command supports subcommand(s).""""""
        return False

    def get_subcommands(self):
        """"""Return the list of subcommand(s).""""""
        raise NotImplementedError(""Derived classes must implement."")
    
    def add_option(self, flag, arg, attr, cb=None):
        """"""
        Add an option for getopt with optional argument.
        """"""
        assert flag not in self.options

        optional = attr.get('optional', False)
        hidden = attr.get('hidden', False)

        if cb:
            self.options[flag] = cb

        object.__setattr__(self, ""opt_%s"" % flag, None)
            
        self.getopt_string += flag
        if optional:
            leftmark = '['
            rightmark = ']'
        else:
            leftmark = ''
            rightmark = ''

        if arg:
            self.getopt_string += "":""
            if not hidden:
                self.params_desc += ""%s-%s <%s>%s "" % (leftmark,
                    flag, arg, rightmark)
                self.last_optional = 0
        elif not hidden:
            if self.last_optional == 0:
                self.params_desc += ""%s-%s%s "" % (leftmark, flag, rightmark)
            else:
                self.params_desc = self.params_desc[:-2] + ""%s%s "" % (flag,
                    rightmark)
            
            if optional:
                self.last_optional = 1
            else:
                self.last_optional = 2

    def parse(self, args):
        """"""
        Parse command arguments.
        """"""
        options, arguments = getopt.gnu_getopt(args, self.getopt_string)
        self.arguments = arguments

        for opt, arg in options:
            trim_opt = opt[1:]
            callback = self.options.get(trim_opt)
            if callback:
                callback(trim_opt, arg)
            object.__setattr__(self, ""opt_%s"" % trim_opt, arg or True)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation.
        
        Return True when the user confirms the action, False otherwise.
        """"""
        i = raw_input(""%s (y)es/(N)o: "" % prompt)
        return i == 'y' or i == 'Y'


    def filter_rc(self, rc):
        """"""
        Allow derived classes to filter return codes.
        """"""
        # default is to not filter return code
        return rc

/n/n/nlib/Shine/Commands/Base/RemoteCommand.py/n/n# RemoteCommand.py -- Base command with remote capabilities
# Copyright (C) 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *
from Command import Command
from CommandRCDefs import *
from RemoteCallEventHandler import RemoteCallEventHandler
from Support.Nodes import Nodes
from Support.Yes import Yes

import socket


class RemoteCommand(Command):
    
    def __init__(self):
        Command.__init__(self)
        self.remote_call = False
        self.local_flag = False
        attr = { 'optional' : True, 'hidden' : True }
        self.add_option('L', None, attr, cb=self.parse_L)
        self.add_option('R', None, attr, cb=self.parse_R)
        self.nodes_support = Nodes(self)
        self.eventhandler = None

    def parse_L(self, opt, arg):
        self.local_flag = True

    def parse_R(self, opt, arg):
        self.remote_call = True

    def has_local_flag(self):
        return self.local_flag or self.remote_call

    def init_execute(self):
        """"""
        Initialize execution of remote command, if needed. Should be called
        first from derived classes before really executing the command.
        """"""
        # Limit the scope of the command if called with local flag (-L) or
        # called remotely (-R).
        if self.has_local_flag():
            self.opt_n = socket.gethostname().split('.', 1)[0]

    def install_eventhandler(self, local_eventhandler, global_eventhandler):
        """"""
        Select and install the appropriate event handler.
        """"""
        if self.remote_call:
            # When called remotely (-R), install a special event handler
            # that knows how to speak the Shine Proxy Protocol using pickle.
            self.eventhandler = RemoteCallEventHandler()
        elif self.local_flag:
            self.eventhandler = local_eventhandler
        else:
            self.eventhandler = global_eventhandler
        # return handler for convenience
        return self.eventhandler

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation. Overrides Command.ask_confirm to
        avoid confirmation when called remotely (-R).

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.remote_call or Command.ask_confirm(self, prompt)

    def filter_rc(self, rc):
        """"""
        When called remotely, return code are not used to handle shine action
        success or failure, nor for status info. To properly detect ssh or remote
        shine installation failures, we filter the return code here.
        """"""
        if self.remote_call:
            # Only errors of type RUNTIME ERROR are allowed to go up.
            rc &= RC_FLAG_RUNTIME_ERROR

        return Command.filter_rc(self, rc)


class RemoteCriticalCommand(RemoteCommand):

    def __init__(self):
        RemoteCommand.__init__(self)
        self.yes_support = Yes(self)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation if -y not specified.

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.yes_support.has_yes() or RemoteCommand.ask_confirm(self, prompt)

/n/n/nlib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        rc = command.execute()

        # Filter rc
        return command.filter_rc(rc)

/n/n/nlib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.CommandRCDefs import *
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes

from Exceptions import *

class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            raise CommandHelpException(""Lustre model file path (-m <model_file>) argument required."", self)
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return RC_OK

/n/n/nlib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Starting %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if not self.remote_call:
                if rc == RC_OK:
                    if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                        print ""Mount successful on %s"" % m_nodes
                elif rc == RC_RUNTIME_ERROR:
                    for nodes, msg in fs.proxy_errors:
                        print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.CommandRCDefs import *
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError %s"" % ex
            return RC_RUNTIME_ERROR

        return RC_OK
/n/n/nlib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

        return result
/n/n/nlib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)
            if not statusdict:
                continue

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if not self.remote_call and vlevel > 0:
                if view == ""fs"":
                    self.status_view_fs(fs)
                elif view.startswith(""target""):
                    self.status_view_targets(fs)
                elif view.startswith(""disk""):
                    self.status_view_disks(fs)
                else:
                    raise CommandBadParameterError(self.view_support.get_view(),
                            ""fs, targets, disks"")

        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/nlib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Stopping %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                    print ""Unmount successful on %s"" % m_nodes
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Configuration/FileSystem.py/n/n# FileSystem.py -- Lustre file system configuration
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$


from Globals import Globals
from Model import Model
from Exceptions import *
from TuningModel import TuningModel

from ClusterShell.NodeSet import NodeSet

from NidMap import NidMap
from TargetDevice import TargetDevice

import copy
import os
import sys


class FileSystem(Model):
    """"""
    Lustre File System Configuration class.
    """"""
    def __init__(self, fs_name=None, lmf=None, tuning_file=None):
        """""" Initialize File System config
        """"""
        self.backend = None

        globals = Globals()

        fs_conf_dir = os.path.expandvars(globals.get_conf_dir())
        fs_conf_dir = os.path.normpath(fs_conf_dir)

        # Load the file system from model or extended model
        if not fs_name and lmf:
            Model.__init__(self, lmf)

            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, self.get_one('fs_name'))

            self._setup_target_devices()

            # Reload
            self.set_filename(self.xmf_path)

        elif fs_name:
            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, fs_name)
            Model.__init__(self, self.xmf_path)

        self._setup_nid_map(self.get_one('nid_map'))

        self.fs_name = self.get_one('fs_name')
        
        # Initialize the tuning model to None if no special tuning configuration
        # is provided
        self.tuning_model = None
        
        if tuning_file:
            # It a tuning configuration file is provided load it
            self.tuning_model = TuningModel(tuning_file)
        else:
            self.tuning_model = TuningModel()

        #self._start_backend()

    def _start_backend(self):
        """"""
        Load and start backend subsystem once
        """"""
        if not self.backend:

            from Backend.BackendRegistry import BackendRegistry
            from Backend.Backend import Backend

            # Start the selected config backend system.
            self.backend = BackendRegistry().get_selected()
            if self.backend:
                self.backend.start()

        return self.backend

    def _setup_target_devices(self):
        """""" Generate the eXtended Model File XMF
        """"""
        self._start_backend()

        for target in [ 'mgt', 'mdt', 'ost' ]:

            if self.backend:

                # Returns a list of TargetDevices
                candidates = copy.copy(self.backend.get_target_devices(target))

                try:
                    # Save the model target selection
                    target_models = copy.copy(self.get(target))
                except KeyError, e:
                    raise ConfigException(""No %s target found"" %(target))

                # Delete it (to be replaced... see below)
                self.delete(target)
                 
                # Iterates on ModelDevices
                i = 0
                for target_model in target_models:
                    result = target_model.match_device(candidates)
                    if len(result) == 0 and not target == 'mgt' :
                        raise ConfigDeviceNotFoundError(target_model)
                    for matching in result:
                        candidates.remove(matching)
                        #
                        # target index is now mandatory in XMF files
                        if not matching.has_index():
                            matching.add_index(i)
                            i += 1

                        # `matching' is a TargetDevice, we want to add it to the
                        # underlying Model object. The current way to do this to
                        # create a configuration line string (performed by
                        # TargetDevice.getline()) and then call Model.add(). 
                        # TODO: add methods to Model/ModelDevice to avoid the use
                        #       of temporary configuration string line.
                        self.add(target, matching.getline())
            else:
                # no backend support

                devices = copy.copy(self.get_with_dict(target))

                self.delete(target)

                target_devices = []
                i = 0
                for dict in devices:
                    t = TargetDevice(target, dict)
                    if not t.has_index():
                        t.add_index(i)
                        i += 1
                    target_devices.append(TargetDevice(target, dict))
                    self.add(target, t.getline())

                if len(target_devices) == 0:
                    raise ConfigDeviceNotFoundError(self)




        # Save XMF
        self.save(self.xmf_path, ""Shine Lustre file system config file for %s"" % \
                self.get_one('fs_name'))
            
    def _setup_nid_map(self, maps):
        """"""
        Set self.nid_map using the NidMap helper class
        """"""
        #self.nid_map = NidMap().fromlist(maps)
        self.nid_map = NidMap(maps.get_one('nodes'), maps.get_one('nids'))

    def get_nid(self, node):
        try:
            return self.nid_map[node]
        except KeyError:
            raise ConfigException(""Cannot get NID for %s, aborting. Please verify `nid_map' configuration."" % node)

    def __str__(self):
        return "">> BACKEND:\n%s\n>> MODEL:\n%s"" % (self.backend, Model.__str__(self))

    def close(self):
        if self.backend:
            self.backend.stop()
            self.backend = None
    
    def register_client(self, node):
        """"""
        This function aims to register a new client that will be able to mount the
        file system.
        Parameters:
        @type node: string
        @param node : is the new client node name
        """"""
        if self._start_backend():
            self.backend.register_client(self.fs_name, node)
        
    def unregister_client(self, node):
        """"""
        This function aims to unregister a client of this  file system
        Parameters:
        @type node: string
        @param node : is name of the client node to unregister
        """"""
        if self._start_backend():
            self.backend.unregister_client(self.fs_name, node)
    
    def set_status_client_mount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                    self.backend.MOUNT_COMPLETE, options)

    def set_status_client_mount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_FAILED, options)

    def set_status_client_mount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_WARNING, options)

    def set_status_client_umount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_COMPLETE, options)

    def set_status_client_umount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_FAILED, options)

    def set_status_client_umount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_WARNING, options)

    def get_status_clients(self):
        if self._start_backend():
            return self.backend.get_status_clients(self.fs_name)

    def set_status_target_unknown(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNKNOWN
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, node, 
                self.backend.TARGET_UNKNOWN, options)

    def set_status_target_ko(self, target, options):
        """"""
        This function is used to set the specified target status
        to KO
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                backend.TARGET_KO, options)

    def set_status_target_available(self, target, options):
        """"""
        This function is used to set the specified target status
        to AVAILABLE
        """"""
        if self._start_backend():
            # Set the fs_name to Free since these targets are availble
            # which means not used by any file system.
            self.backend.set_status_target(None, target,
                self.backend.TARGET_AVAILABLE, options)

    def set_status_target_formating(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATING, options)

    def set_status_target_format_failed(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMAT_FAILED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMAT_FAILED, options)

    def set_status_target_formated(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATED, options)

    def set_status_target_offline(self, target, options):
        """"""
        This function is used to set the specified target status
        to OFFLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_OFFLINE, options)

    def set_status_target_starting(self, target, options):
        """"""
        This function is used to set the specified target status
        to STARTING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STARTING, options)

    def set_status_target_online(self, target, options):
        """"""
        This function is used to set the specified target status
        to ONLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_ONLINE, options)

    def set_status_target_critical(self, target, options):
        """"""
        This function is used to set the specified target status
        to CRITICAL
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_CRITICAL, options)

    def set_status_target_stopping(self, target, options):
        """"""
        This function is used to set the specified target status
        to STOPPING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STOPPING, options)

    def set_status_target_unreachable(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNREACHABLE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_UNREACHABLE, options)

    def get_status_targets(self):
        """"""
        This function returns the status of each targets
        involved in the current file system.
        """"""
        if self._start_backend():
            return self.backend.get_status_targets(self.fs_name)

    def register(self):
        """"""
        This function aims to register the file system configuration
        to the backend.
        """"""
        if self._start_backend():
            return self.backend.register_fs(self)

    def unregister(self):
        """"""
        This function aims to remove a file system configuration from
        the backend.        
        """"""
        result = 0
        if self._start_backend():
            result = self.backend.unregister_fs(self)

        if not result:
            os.unlink(self.xmf_path)

        return result
/n/n/nlib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
        except KeyError:
            raise
        
        return RC_RUNTIME_ERROR


/n/n/nlib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # some common remote errors:
            # rc 127 = command not found
            # rc 126 = found but not executable
            # rc 1 = python failure...
            if rc != 0:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/nlib/Shine/Lustre/FileSystem.py/n/n# FileSystem.py -- Lustre FS
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Lustre FileSystem class.

Represents a Lustre FS.
""""""

import copy
from sets import Set
import socket

from ClusterShell.NodeSet import NodeSet, RangeSet

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

# Action exceptions
from Actions.Action import ActionErrorException
from Actions.Proxies.ProxyAction import *

from Actions.Install import Install
from Actions.Proxies.Preinstall import Preinstall
from Actions.Proxies.FSProxyAction import FSProxyAction
from Actions.Proxies.FSClientProxyAction import FSClientProxyAction

from EventHandler import *
from Client import *
from Server import *
from Target import *


class FSException(Exception):
    def __init__(self, message):
        self.message = message
    def __str__(self):
        return self.message

class FSError(FSException):
    """"""
    Base FileSystem error exception.
    """"""

class FSSyntaxError(FSError):
    def __init__(self, message):
        self.message = ""Syntax error: \""%s\"""" % (message)
    def __str__(self):
        return self.message

class FSBadTargetError(FSSyntaxError):
    def __init__(self, target_name):
        self.message = ""Syntax error: unrecognized target \""%s\"""" % (target_name)

class FSStructureError(FSError):
    """"""
    Lustre file system structure error, raised after an invalid configuration
    is encountered. For example, you will get this error if you try to assign
    two targets `MGT' to a filesystem.
    """"""

class FSRemoteError(FSError):
    """"""
    Remote host(s) not available, or a remote operation failed.
    """"""
    def __init__(self, nodes, rc, message):
        FSError.__init__(self, message)
        self.nodes = nodes
        self.rc = int(rc)

    def __str__(self):
        return ""%s: %s [rc=%d]"" % (self.nodes, self.message, self.rc)


STATUS_SERVERS      = 0x01
STATUS_HASERVERS    = 0x02
STATUS_CLIENTS      = 0x10
STATUS_ANY          = 0xff


class FileSystem:
    """"""
    The Lustre FileSystem abstract class.
    """"""

    def __init__(self, fs_name, event_handler=None):
        self.fs_name = fs_name
        self.debug = False
        self.set_eventhandler(event_handler)
        self.proxy_errors = []

        self.local_hostname = socket.gethostname()
        self.local_hostname_short = self.local_hostname.split('.', 1)[0]

        # file system MGT
        self.mgt = None

        # All FS server targets (MGT, MDT, OST...)
        self.targets = []

        # All FS clients
        self.clients = []

        # filled after successful install
        self.mgt_servers = NodeSet()
        self.mgt_count = 0

        self.mdt_servers = NodeSet()
        self.mdt_count = 0

        self.ost_servers = NodeSet()
        self.ost_count = 0

        self.target_count = 0
        self.target_servers = NodeSet()

    def set_debug(self, debug):
        self.debug = debug

    #
    # file system event handling
    #

    def _invoke_event(self, event, **kwargs):
        if 'target' in kwargs or 'client' in kwargs:
            kwargs.setdefault('node', None)
        getattr(self.event_handler, event)(**kwargs)

    def _invoke_dummy(self, event, **kwargs):
        pass

    def set_eventhandler(self, event_handler):
        self.event_handler = event_handler
        if self.event_handler is None:
            self._invoke = self._invoke_dummy
        else:
            self._invoke = self._invoke_event

    def _handle_shine_event(self, event, node, **params):
        #print ""_handle_shine_event %s %s"" % (event, params)
        target = params.get('target')
        if target:
            found = False
            for t in self.targets:
                if t.match(target):
                    # perform sanity checks here
                    old_nids = t.get_nids()
                    if old_nids != target.get_nids():
                        print ""NIDs mismatch %s -> %s"" % \
                                (','.join(old.nids), ','.join(target.get_nids))
                    # update target from remote one
                    t.update(target)
                    # substitute target parameter by local one
                    params['target'] = t
                    found = True
            if not found:
                print ""Target Update FAILED (%s)"" % target
        
        client = params.get('client')
        if client:
            found = False
            for c in self.clients:
                if c.match(client):
                    # update client from remote one
                    c.update(client)
                    # substitute client parameter
                    params['client'] = c
                    found = True
            if not found:
                print ""Client Update FAILED (%s)"" % client

        self._invoke(event, node=node, **params)

    def _handle_shine_proxy_error(self, nodes, message):
        self.proxy_errors.append((NodeSet(nodes), message))

    #
    # file system construction
    #

    def _attach_target(self, target):
        self.targets.append(target)
        if target.type == 'mgt':
            self.mgt = target
        self._update_structure()

    def _attach_client(self, client):
        self.clients.append(client)
        self._update_structure()

    def new_target(self, server, type, index, dev, jdev=None, group=None,
            tag=None, enabled=True):
        """"""
        Create a new attached target.
        """"""
        #print ""new_target on %s type %s (enabled=%s)"" % (server, type, enabled)

        if type == 'mgt' and self.mgt and len(self.mgt.get_nids()) > 0:
            raise FSStructureError(""A Lustre FS has only one MGT."")

        # Instantiate matching target class (eg. 'ost' -> OST).
        target = getattr(sys.modules[self.__class__.__module__], type.upper())(fs=self,
                server=server, index=index, dev=dev, jdev=jdev, group=group, tag=tag,
                enabled=enabled)
        
        return target

    def new_client(self, server, mount_path, enabled=True):
        """"""
        Create a new attached client.
        """"""
        client = Client(self, server, mount_path, enabled)

        return client

    def get_mgs_nids(self):
        return self.mgt.get_nids()
    
    def get_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients])

    def get_enabled_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients if c.action_enabled])

    def get_enabled_target_servers(self):
        return NodeSet.fromlist([t.server for t in self.targets if t.action_enabled])

    def get_client_statecounters(self):
        """"""
        Get (ignored, offline, error, runtime_error, mounted) client state counters tuple.
        """"""
        ignored = 0
        states = {}
        for client in self.clients:
            if client.action_enabled:
                state = states.setdefault(client.state, 0)
                states[client.state] = state + 1
            else:
                ignored += 1
        
        return (ignored,
                states.get(OFFLINE, 0),
                states.get(CLIENT_ERROR, 0),
                states.get(RUNTIME_ERROR, 0),
                states.get(MOUNTED, 0))

    def targets_by_state(self, state):
        for target in self.targets:
            #print target, target.state
            if target.action_enabled and target.state == state:
                yield target

    def target_servers_by_state(self, state):
        servers = NodeSet()
        for target in self.targets_by_state(state):
            #print ""OK %s"" % target
            servers.add(target.servers[0])
        return servers

    def _distant_action_by_server(self, action_class, servers, **kwargs):

        task = task_self()

        # filter local server
        if self.local_hostname in servers:
            distant_servers = servers.difference(self.local_hostname)
        elif self.local_hostname_short in servers:
            distant_servers = servers.difference(self.local_hostname_short)
        else:
            distant_servers = servers

        # perform action on distant servers
        if len(distant_servers) > 0:
            action = action_class(nodes=distant_servers, fs=self, **kwargs)
            action.launch()
            task.resume()

    def install(self, fs_config_file, nodes=None):
        """"""
        Install FS config files.
        """"""
        servers = NodeSet()

        for target in self.targets:
            # install on failover partners too
            for s in target.servers:
                if not nodes or s in nodes:
                    servers.add(s)

        for client in self.clients:
            # install on failover partners too
            if not nodes or client.server in nodes:
                servers.add(client.server)

        assert len(servers) > 0, ""no servers?""

        try:
            self._distant_action_by_server(Preinstall, servers)
            self._distant_action_by_server(Install, servers, config_file=fs_config_file)
        except ProxyActionError, e:
            # switch to public exception
            raise FSRemoteError(e.nodes, e.rc, e.message)
        
    def remove(self):
        """"""
        Remove FS config files.
        """"""

        result = 0

        servers = NodeSet()

        self.action_refcnt = 0
        self.proxy_errors = []

        # iterate over lustre servers
        for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
            if not e_s_targets:
                continue

            if server.is_local():
                # remove local fs configuration file
                conf_dir_path = Globals().get_conf_dir()
                fs_file = os.path.join(Globals().get_conf_dir(), ""%s.xmf"" % self.fs_name)
                rc = os.unlink(fs_file)
                result = max(result, rc)
            else:
                servers.add(server)

        if len(servers) > 0:
            # Perform the remove operations on all targets for these nodes.
            action = FSProxyAction(self, 'remove', servers, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR
        
        return result

    def _update_structure(self):
        # convenience
        for type, targets, servers in self._iter_targets_servers_by_type():
            if type == 'ost':
                self.ost_count = len(targets)
                self.ost_servers = NodeSet(servers)
            elif type == 'mdt':
                self.mdt_count = len(targets)
                self.mdt_servers = NodeSet(servers)
            elif type == 'mgt':
                self.mgt_count = len(targets)
                self.mgt_servers = NodeSet(servers)

        self.target_count = self.mgt_count + self.mdt_count + self.ost_count
        self.target_servers = self.mgt_servers | self.mdt_servers | self.ost_servers

    def _iter_targets_servers_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns a tuple (list of targets,
        list of servers) per target type.
        """"""
        last_target_type = None
        servers = NodeSet()
        targets = Set()

        #self.targets.sort()

        if reverse:
            self.targets.reverse()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(targets) > 0:
                    yield last_target_type, targets, servers
                    servers.clear()     # ClusterShell 1.1+ needed (sorry)
                    targets.clear()

            if target.action_enabled:
                targets.add(target)
                # select server: change master_server for -F node
                servers.add(target.get_selected_server())
            last_target_type = target.type

        if len(targets) > 0:
            yield last_target_type, targets, servers

    def targets_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns the following tuple:
        (type, (list of all targets of this type, list of enabled targets))
        per target type.
        """"""
        last_target_type = None
        a_targets = Set()
        e_targets = Set()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(a_targets) > 0:
                    yield last_target_type, (a_targets, e_targets)
                    a_targets.clear()
                    e_targets.clear()

            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)
            last_target_type = target.type

        if len(a_targets) > 0:
            yield last_target_type, (a_targets, e_targets)

    def _iter_targets_by_server(self):
        """"""
        Per server of target iterator : returns the following tuple:
        (server, (list of all server targets, list of enabled targets))
        per target server.
        """"""
        servers = {}
        for target in self.targets:
            a_targets, e_targets = servers.setdefault(target.get_selected_server(), (Set(), Set()))
            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)

        return servers.iteritems()


    def _iter_type_idx_for_targets(self, targets):
        last_target_type = None

        indexes = RangeSet(autostep=3)

        #self.targets.sort()

        for target in targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(indexes) > 0:
                    yield last_target_type, indexes
                    indexes.clear()     # CS 1.1+
            indexes.add(int(target.index))
            last_target_type = target.type

        if len(indexes) > 0:
            yield last_target_type, indexes

    def format(self, **kwargs):

        # Remember format launched, so we can check their status once
        # all operations are done.
        format_launched = Set()

        servers_formatall = NodeSet()

        self.proxy_errors = []
        self.action_refcnt = 0

        for server, (a_targets, e_targets) in self._iter_targets_by_server():

            if server.is_local():
                # local server
                for target in e_targets:
                    target.format(**kwargs)
                    self.action_refcnt += 1

                format_launched.update(e_targets)

            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action if ""format all targets on this server""
                    # is detected
                    servers_formatall.add(server)
                else:
                    # otherwise, format per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'format',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

                format_launched.update(e_targets)

        if len(servers_formatall) > 0:
            action = FSProxyAction(self, 'format', servers_formatall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check.
        for target in format_launched:
            if target.state != OFFLINE:
                return target.state

        return OFFLINE

    def status(self, flags=STATUS_ANY):
        """"""
        Get status of filesystem.
        """"""

        status_target_launched = Set()
        status_client_launched = Set()
        servers_statusall = NodeSet()
        self.action_refcnt = 0
        self.proxy_errors = []

        # prepare servers status checks
        if flags & STATUS_SERVERS:
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                if len(e_s_targets) == 0:
                    continue

                if server.is_local():
                    for target in e_s_targets:
                        target.status()
                        self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)
                else:
                    # distant server: check if all server targets have been selected
                    if len(a_s_targets) == len(e_s_targets):
                        # ""status on all targets for this server"" detected
                        servers_statusall.add(server)
                    else:
                        # status per selected targets on this server
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(e_s_targets):
                            action = FSProxyAction(self, 'status',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)

        # prepare clients status checks
        if flags & STATUS_CLIENTS:
            for client in self.clients:
                if client.action_enabled:
                    server = client.server
                    if server.is_local():
                        client.status()
                        self.action_refcnt += 1
                    elif server not in servers_statusall:
                        servers_statusall.add(server)
                    status_client_launched.add(client)

        # launch distant actions
        if len(servers_statusall) > 0:
            action = FSProxyAction(self, 'status', servers_statusall, self.debug)
            action.launch()
            self.action_refcnt += 1

        # run loop
        task_self().resume()
        
        # return a dict of {state : target list}
        rdict = {}

        # all launched targets+clients
        launched = (status_target_launched | status_client_launched)
        if self.proxy_errors:
            # find targets/clients affected by the runtime error(s)
            for target in launched:
                for nodes, msg in self.proxy_errors:
                    if target.server in nodes:
                        target.state = RUNTIME_ERROR

        for target in launched:
            if target.state == None:
                print target, target.server
            assert target.state != None
            targets = rdict.setdefault(target.state, [])
            targets.append(target)
        return rdict

    def status_target(self, target):
        """"""
        Launch a status request for a specific local or remote target.
        """"""

        # Don't call me if the target itself is not enabled.
        assert target.action_enabled

        server = target.get_selected_server()

        if server.is_local():
            # Target is local
            target.status()
        else:
            action = FSProxyAction(self, 'status', NodeSet(server), self.debug,
                    target.type, RangeSet(str(target.index)))
            action.launch()

        self.action_refcnt = 1
        task_self().resume()

    def start(self, **kwargs):
        """"""
        Start Lustre file system servers.
        """"""
        self.proxy_errors = []

        # What starting order to use?
        for target in self.targets:
            if isinstance(target, MDT) and target.action_enabled:
                # Found enabled MDT: perform writeconf check.
                self.status_target(target)
                if target.has_first_time_flag() or target.has_writeconf_flag():
                    # first_time or writeconf flag found, start MDT before OSTs
                    MDT.target_order = 2 # change MDT class variable order

        self.targets.sort()

        # servers_startall is used for optimization, it contains nodes
        # where we have to perform the start operation on all targets
        # found for this FS. This will limit the number of FSProxyAction
        # to spawn.
        servers_startall = NodeSet()

        # Remember targets launched, so we can check their status once
        # all operations are done (here, status are checked after all
        # targets of the same type have completed the start operation -
        # with possible failure).
        targets_launched = Set()

        # Keep number of actions in order to abort task correctly in
        # action's ev_close.
        self.action_refcnt = 0

        result = 0

        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():
            
            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():

                # To summary, we keep targets that are:
                # 1. enabled
                # 2. of according type
                # 3. on this server
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Start targets if we are on the good server.
                    for target in type_e_targets:
                        # Note that target.start() should never block here:
                        # it will perform necessary non-blocking actions and
                        # (when needed) will start local ClusterShell workers.
                        target.start(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""start all FS targets on this server"" detected
                        servers_startall.add(server)
                    else:
                        # Start per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'start',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched targets of this server for late status check.
                targets_launched.update(type_e_targets)

            if len(servers_startall) > 0:
                # Perform the start operations on all targets for these nodes.
                action = FSProxyAction(self, 'start', servers_startall, self.debug)
                action.launch()
                self.action_refcnt += 1

            # Resume current task, ie. start runloop, process workers events
            # and also act as a target-type barrier.
            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_launched:
                if target.state > result:
                    result = target.state
                    if result > RECOVERING:
                        # Avoid broken cascading starts, so we break now if
                        # a target of the previous type failed to start.
                        return result

            # Some needed cleanup before next target type.
            servers_startall.clear()
            targets_launched.clear()

        return result


    def stop(self, **kwargs):
        """"""
        Stop file system.
        """"""
        rc = MOUNTED

        # Stop: reverse order
        self.targets.sort()
        self.targets.reverse()

        # servers_stopall is used for optimization, see the comment in
        # start() for servers_startall.
        servers_stopall = NodeSet()

        # Remember targets when stop was launched.
        targets_stopping = Set()

        self.action_refcnt = 0
        self.proxy_errors = []

        # We use a similar logic than start(): see start() for comments.
        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():

            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Stop targets if we are on the good server.
                    for target in type_e_targets:
                        target.stop(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""stop all FS targets on this server"" detected
                        servers_stopall.add(server)
                    else:
                        # Stop per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'stop',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched stopping targets of this server for late status check.
                targets_stopping.update(type_e_targets)

            if len(servers_stopall) > 0:
                # Perform the stop operations on all targets for these nodes.
                action = FSProxyAction(self, 'stop', servers_stopall, self.debug)
                action.launch()
                self.action_refcnt += 1

            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_stopping:
                if target.state > rc:
                    rc = target.state

            # Some needed cleanup before next target type.
            servers_stopall.clear()
            targets_stopping.clear()

        return rc

    def mount(self, **kwargs):
        """"""
        Mount FS clients.
        """"""
        servers_mountall = NodeSet()
        clients_mounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.start(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_mountall.add(client.server)

            clients_mounting.add(client)

        if len(servers_mountall) > 0:
            action = FSClientProxyAction(self, 'mount', servers_mountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_mounting:
            if client.state != MOUNTED:
                return client.state

        return MOUNTED

    def umount(self, **kwargs):
        """"""
        Unmount FS clients.
        """"""
        servers_umountall = NodeSet()
        clients_umounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.stop(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_umountall.add(client.server)

            clients_umounting.add(client)

        if len(servers_umountall) > 0:
            action = FSClientProxyAction(self, 'umount', servers_umountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_umounting:
            if client.state != OFFLINE:
                return client.state

        return OFFLINE

    def info(self):
        pass

    def tune(self, tuning_model):
        """"""
        Tune server.
        """"""
        task = task_self()
        tune_all = NodeSet()
        type_map = { 'mgt': 'mgs', 'mdt': 'mds', 'ost' : 'oss' }
        self.action_refcnt = 0
        self.proxy_errors = []
        result = 0

        # Install tuning.conf on enabled distant servers
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if e_targets and not server.is_local():
                tune_all.add(server)
        if len(tune_all) > 0:
            self._distant_action_by_server(Install, tune_all, config_file=Globals().get_tuning_file())
            self.action_refcnt += 1
            task.resume()
            tune_all.clear()

        # Apply tunings
        self.action_refcnt = 0
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if not e_targets:
                continue
            if server.is_local():
                types = Set()
                for t in e_targets:
                    types.add(type_map[t.type])

                rc = server.tune(tuning_model, types, self.fs_name)
                result = max(result, rc)
            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action
                    tune_all.add(server)
                else:
                    # otherwise, tune per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'tune',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

        if len(tune_all) > 0:
            action = FSProxyAction(self, 'tune', tune_all, self.debug)
            action.launch()
            self.action_refcnt += 1

        task.resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        return result

/n/n/n",0
71,7ff203be36e439b535894764c37a8446351627ec,"/lib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()

/n/n/n/lib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            print ""Bad argument""
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return 0

/n/n/n/lib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Mount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError""
            raise

/n/n/n/lib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc
/n/n/n/lib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == ""fs"":
                self.status_view_fs(fs)
            elif view.startswith(""target""):
                self.status_view_targets(fs)
            elif view.startswith(""disk""):
                self.status_view_disks(fs)
            else:
                raise CommandBadParameterError(self.view_support.get_view(),
                        ""fs, targets, disks"")
        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/n/lib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Unmount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
            return RC_USER_ERROR
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
            return RC_RUNTIME_ERROR
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except KeyError:
            print ""Error - Unrecognized action""
            print
            raise
        
        return 1


/n/n/n/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/n",1
72,7ff203be36e439b535894764c37a8446351627ec,"lib/Shine/Commands/Base/Command.py/n/n# Command.py -- Base command class
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Support.Debug import Debug

from CommandRCDefs import *

import getopt


#
# Command exceptions are defined in Shine.Command.Exceptions
#

class Command(object):
    """"""
    The base class for command objects that can be added to the commands
    registry.
    """"""
    def __init__(self):
        self.options = {}
        self.getopt_string = """"
        self.params_desc = """"
        self.last_optional = 0
        self.arguments = None

        # All commands have debug support.
        self.debug_support = Debug(self)

    def is_hidden(self):
        """"""Return whether the command should not be displayed to user.""""""
        return False
    
    def get_name(self):
        raise NotImplementedError(""Derived classes must implement."")

    def get_desc(self):
        return ""Undocumented""

    def get_params_desc(self):
        pdesc = self.params_desc.strip()
        if self.has_subcommand():
            return ""%s %s"" % ('|'.join(self.get_subcommands()), pdesc)
        return pdesc

    def has_subcommand(self):
        """"""Return whether the command supports subcommand(s).""""""
        return False

    def get_subcommands(self):
        """"""Return the list of subcommand(s).""""""
        raise NotImplementedError(""Derived classes must implement."")
    
    def add_option(self, flag, arg, attr, cb=None):
        """"""
        Add an option for getopt with optional argument.
        """"""
        assert flag not in self.options

        optional = attr.get('optional', False)
        hidden = attr.get('hidden', False)

        if cb:
            self.options[flag] = cb

        object.__setattr__(self, ""opt_%s"" % flag, None)
            
        self.getopt_string += flag
        if optional:
            leftmark = '['
            rightmark = ']'
        else:
            leftmark = ''
            rightmark = ''

        if arg:
            self.getopt_string += "":""
            if not hidden:
                self.params_desc += ""%s-%s <%s>%s "" % (leftmark,
                    flag, arg, rightmark)
                self.last_optional = 0
        elif not hidden:
            if self.last_optional == 0:
                self.params_desc += ""%s-%s%s "" % (leftmark, flag, rightmark)
            else:
                self.params_desc = self.params_desc[:-2] + ""%s%s "" % (flag,
                    rightmark)
            
            if optional:
                self.last_optional = 1
            else:
                self.last_optional = 2

    def parse(self, args):
        """"""
        Parse command arguments.
        """"""
        options, arguments = getopt.gnu_getopt(args, self.getopt_string)
        self.arguments = arguments

        for opt, arg in options:
            trim_opt = opt[1:]
            callback = self.options.get(trim_opt)
            if callback:
                callback(trim_opt, arg)
            object.__setattr__(self, ""opt_%s"" % trim_opt, arg or True)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation.
        
        Return True when the user confirms the action, False otherwise.
        """"""
        i = raw_input(""%s (y)es/(N)o: "" % prompt)
        return i == 'y' or i == 'Y'


    def filter_rc(self, rc):
        """"""
        Allow derived classes to filter return codes.
        """"""
        # default is to not filter return code
        return rc

/n/n/nlib/Shine/Commands/Base/RemoteCommand.py/n/n# RemoteCommand.py -- Base command with remote capabilities
# Copyright (C) 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *
from Command import Command
from CommandRCDefs import *
from RemoteCallEventHandler import RemoteCallEventHandler
from Support.Nodes import Nodes
from Support.Yes import Yes

import socket


class RemoteCommand(Command):
    
    def __init__(self):
        Command.__init__(self)
        self.remote_call = False
        self.local_flag = False
        attr = { 'optional' : True, 'hidden' : True }
        self.add_option('L', None, attr, cb=self.parse_L)
        self.add_option('R', None, attr, cb=self.parse_R)
        self.nodes_support = Nodes(self)
        self.eventhandler = None

    def parse_L(self, opt, arg):
        self.local_flag = True

    def parse_R(self, opt, arg):
        self.remote_call = True

    def has_local_flag(self):
        return self.local_flag or self.remote_call

    def init_execute(self):
        """"""
        Initialize execution of remote command, if needed. Should be called
        first from derived classes before really executing the command.
        """"""
        # Limit the scope of the command if called with local flag (-L) or
        # called remotely (-R).
        if self.has_local_flag():
            self.opt_n = socket.gethostname().split('.', 1)[0]

    def install_eventhandler(self, local_eventhandler, global_eventhandler):
        """"""
        Select and install the appropriate event handler.
        """"""
        if self.remote_call:
            # When called remotely (-R), install a special event handler
            # that knows how to speak the Shine Proxy Protocol using pickle.
            self.eventhandler = RemoteCallEventHandler()
        elif self.local_flag:
            self.eventhandler = local_eventhandler
        else:
            self.eventhandler = global_eventhandler
        # return handler for convenience
        return self.eventhandler

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation. Overrides Command.ask_confirm to
        avoid confirmation when called remotely (-R).

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.remote_call or Command.ask_confirm(self, prompt)

    def filter_rc(self, rc):
        """"""
        When called remotely, return code are not used to handle shine action
        success or failure, nor for status info. To properly detect ssh or remote
        shine installation failures, we filter the return code here.
        """"""
        if self.remote_call:
            # Only errors of type RUNTIME ERROR are allowed to go up.
            rc &= RC_FLAG_RUNTIME_ERROR

        return Command.filter_rc(self, rc)


class RemoteCriticalCommand(RemoteCommand):

    def __init__(self):
        RemoteCommand.__init__(self)
        self.yes_support = Yes(self)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation if -y not specified.

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.yes_support.has_yes() or RemoteCommand.ask_confirm(self, prompt)

/n/n/nlib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        rc = command.execute()

        # Filter rc
        return command.filter_rc(rc)

/n/n/nlib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.CommandRCDefs import *
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes

from Exceptions import *

class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            raise CommandHelpException(""Lustre model file path (-m <model_file>) argument required."", self)
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return RC_OK

/n/n/nlib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Starting %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if not self.remote_call:
                if rc == RC_OK:
                    if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                        print ""Mount successful on %s"" % m_nodes
                elif rc == RC_RUNTIME_ERROR:
                    for nodes, msg in fs.proxy_errors:
                        print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.CommandRCDefs import *
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError %s"" % ex
            return RC_RUNTIME_ERROR

        return RC_OK
/n/n/nlib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

        return result
/n/n/nlib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)
            if not statusdict:
                continue

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if not self.remote_call and vlevel > 0:
                if view == ""fs"":
                    self.status_view_fs(fs)
                elif view.startswith(""target""):
                    self.status_view_targets(fs)
                elif view.startswith(""disk""):
                    self.status_view_disks(fs)
                else:
                    raise CommandBadParameterError(self.view_support.get_view(),
                            ""fs, targets, disks"")

        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/nlib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Stopping %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                    print ""Unmount successful on %s"" % m_nodes
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Configuration/FileSystem.py/n/n# FileSystem.py -- Lustre file system configuration
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$


from Globals import Globals
from Model import Model
from Exceptions import *
from TuningModel import TuningModel

from ClusterShell.NodeSet import NodeSet

from NidMap import NidMap
from TargetDevice import TargetDevice

import copy
import os
import sys


class FileSystem(Model):
    """"""
    Lustre File System Configuration class.
    """"""
    def __init__(self, fs_name=None, lmf=None, tuning_file=None):
        """""" Initialize File System config
        """"""
        self.backend = None

        globals = Globals()

        fs_conf_dir = os.path.expandvars(globals.get_conf_dir())
        fs_conf_dir = os.path.normpath(fs_conf_dir)

        # Load the file system from model or extended model
        if not fs_name and lmf:
            Model.__init__(self, lmf)

            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, self.get_one('fs_name'))

            self._setup_target_devices()

            # Reload
            self.set_filename(self.xmf_path)

        elif fs_name:
            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, fs_name)
            Model.__init__(self, self.xmf_path)

        self._setup_nid_map(self.get_one('nid_map'))

        self.fs_name = self.get_one('fs_name')
        
        # Initialize the tuning model to None if no special tuning configuration
        # is provided
        self.tuning_model = None
        
        if tuning_file:
            # It a tuning configuration file is provided load it
            self.tuning_model = TuningModel(tuning_file)
        else:
            self.tuning_model = TuningModel()

        #self._start_backend()

    def _start_backend(self):
        """"""
        Load and start backend subsystem once
        """"""
        if not self.backend:

            from Backend.BackendRegistry import BackendRegistry
            from Backend.Backend import Backend

            # Start the selected config backend system.
            self.backend = BackendRegistry().get_selected()
            if self.backend:
                self.backend.start()

        return self.backend

    def _setup_target_devices(self):
        """""" Generate the eXtended Model File XMF
        """"""
        self._start_backend()

        for target in [ 'mgt', 'mdt', 'ost' ]:

            if self.backend:

                # Returns a list of TargetDevices
                candidates = copy.copy(self.backend.get_target_devices(target))

                try:
                    # Save the model target selection
                    target_models = copy.copy(self.get(target))
                except KeyError, e:
                    raise ConfigException(""No %s target found"" %(target))

                # Delete it (to be replaced... see below)
                self.delete(target)
                 
                # Iterates on ModelDevices
                i = 0
                for target_model in target_models:
                    result = target_model.match_device(candidates)
                    if len(result) == 0 and not target == 'mgt' :
                        raise ConfigDeviceNotFoundError(target_model)
                    for matching in result:
                        candidates.remove(matching)
                        #
                        # target index is now mandatory in XMF files
                        if not matching.has_index():
                            matching.add_index(i)
                            i += 1

                        # `matching' is a TargetDevice, we want to add it to the
                        # underlying Model object. The current way to do this to
                        # create a configuration line string (performed by
                        # TargetDevice.getline()) and then call Model.add(). 
                        # TODO: add methods to Model/ModelDevice to avoid the use
                        #       of temporary configuration string line.
                        self.add(target, matching.getline())
            else:
                # no backend support

                devices = copy.copy(self.get_with_dict(target))

                self.delete(target)

                target_devices = []
                i = 0
                for dict in devices:
                    t = TargetDevice(target, dict)
                    if not t.has_index():
                        t.add_index(i)
                        i += 1
                    target_devices.append(TargetDevice(target, dict))
                    self.add(target, t.getline())

                if len(target_devices) == 0:
                    raise ConfigDeviceNotFoundError(self)




        # Save XMF
        self.save(self.xmf_path, ""Shine Lustre file system config file for %s"" % \
                self.get_one('fs_name'))
            
    def _setup_nid_map(self, maps):
        """"""
        Set self.nid_map using the NidMap helper class
        """"""
        #self.nid_map = NidMap().fromlist(maps)
        self.nid_map = NidMap(maps.get_one('nodes'), maps.get_one('nids'))

    def get_nid(self, node):
        try:
            return self.nid_map[node]
        except KeyError:
            raise ConfigException(""Cannot get NID for %s, aborting. Please verify `nid_map' configuration."" % node)

    def __str__(self):
        return "">> BACKEND:\n%s\n>> MODEL:\n%s"" % (self.backend, Model.__str__(self))

    def close(self):
        if self.backend:
            self.backend.stop()
            self.backend = None
    
    def register_client(self, node):
        """"""
        This function aims to register a new client that will be able to mount the
        file system.
        Parameters:
        @type node: string
        @param node : is the new client node name
        """"""
        if self._start_backend():
            self.backend.register_client(self.fs_name, node)
        
    def unregister_client(self, node):
        """"""
        This function aims to unregister a client of this  file system
        Parameters:
        @type node: string
        @param node : is name of the client node to unregister
        """"""
        if self._start_backend():
            self.backend.unregister_client(self.fs_name, node)
    
    def set_status_client_mount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                    self.backend.MOUNT_COMPLETE, options)

    def set_status_client_mount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_FAILED, options)

    def set_status_client_mount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_WARNING, options)

    def set_status_client_umount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_COMPLETE, options)

    def set_status_client_umount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_FAILED, options)

    def set_status_client_umount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_WARNING, options)

    def get_status_clients(self):
        if self._start_backend():
            return self.backend.get_status_clients(self.fs_name)

    def set_status_target_unknown(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNKNOWN
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, node, 
                self.backend.TARGET_UNKNOWN, options)

    def set_status_target_ko(self, target, options):
        """"""
        This function is used to set the specified target status
        to KO
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                backend.TARGET_KO, options)

    def set_status_target_available(self, target, options):
        """"""
        This function is used to set the specified target status
        to AVAILABLE
        """"""
        if self._start_backend():
            # Set the fs_name to Free since these targets are availble
            # which means not used by any file system.
            self.backend.set_status_target(None, target,
                self.backend.TARGET_AVAILABLE, options)

    def set_status_target_formating(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATING, options)

    def set_status_target_format_failed(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMAT_FAILED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMAT_FAILED, options)

    def set_status_target_formated(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATED, options)

    def set_status_target_offline(self, target, options):
        """"""
        This function is used to set the specified target status
        to OFFLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_OFFLINE, options)

    def set_status_target_starting(self, target, options):
        """"""
        This function is used to set the specified target status
        to STARTING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STARTING, options)

    def set_status_target_online(self, target, options):
        """"""
        This function is used to set the specified target status
        to ONLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_ONLINE, options)

    def set_status_target_critical(self, target, options):
        """"""
        This function is used to set the specified target status
        to CRITICAL
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_CRITICAL, options)

    def set_status_target_stopping(self, target, options):
        """"""
        This function is used to set the specified target status
        to STOPPING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STOPPING, options)

    def set_status_target_unreachable(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNREACHABLE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_UNREACHABLE, options)

    def get_status_targets(self):
        """"""
        This function returns the status of each targets
        involved in the current file system.
        """"""
        if self._start_backend():
            return self.backend.get_status_targets(self.fs_name)

    def register(self):
        """"""
        This function aims to register the file system configuration
        to the backend.
        """"""
        if self._start_backend():
            return self.backend.register_fs(self)

    def unregister(self):
        """"""
        This function aims to remove a file system configuration from
        the backend.        
        """"""
        result = 0
        if self._start_backend():
            result = self.backend.unregister_fs(self)

        if not result:
            os.unlink(self.xmf_path)

        return result
/n/n/nlib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
        except KeyError:
            raise
        
        return RC_RUNTIME_ERROR


/n/n/nlib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # some common remote errors:
            # rc 127 = command not found
            # rc 126 = found but not executable
            # rc 1 = python failure...
            if rc != 0:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/nlib/Shine/Lustre/FileSystem.py/n/n# FileSystem.py -- Lustre FS
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Lustre FileSystem class.

Represents a Lustre FS.
""""""

import copy
from sets import Set
import socket

from ClusterShell.NodeSet import NodeSet, RangeSet

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

# Action exceptions
from Actions.Action import ActionErrorException
from Actions.Proxies.ProxyAction import *

from Actions.Install import Install
from Actions.Proxies.Preinstall import Preinstall
from Actions.Proxies.FSProxyAction import FSProxyAction
from Actions.Proxies.FSClientProxyAction import FSClientProxyAction

from EventHandler import *
from Client import *
from Server import *
from Target import *


class FSException(Exception):
    def __init__(self, message):
        self.message = message
    def __str__(self):
        return self.message

class FSError(FSException):
    """"""
    Base FileSystem error exception.
    """"""

class FSSyntaxError(FSError):
    def __init__(self, message):
        self.message = ""Syntax error: \""%s\"""" % (message)
    def __str__(self):
        return self.message

class FSBadTargetError(FSSyntaxError):
    def __init__(self, target_name):
        self.message = ""Syntax error: unrecognized target \""%s\"""" % (target_name)

class FSStructureError(FSError):
    """"""
    Lustre file system structure error, raised after an invalid configuration
    is encountered. For example, you will get this error if you try to assign
    two targets `MGT' to a filesystem.
    """"""

class FSRemoteError(FSError):
    """"""
    Remote host(s) not available, or a remote operation failed.
    """"""
    def __init__(self, nodes, rc, message):
        FSError.__init__(self, message)
        self.nodes = nodes
        self.rc = int(rc)

    def __str__(self):
        return ""%s: %s [rc=%d]"" % (self.nodes, self.message, self.rc)


STATUS_SERVERS      = 0x01
STATUS_HASERVERS    = 0x02
STATUS_CLIENTS      = 0x10
STATUS_ANY          = 0xff


class FileSystem:
    """"""
    The Lustre FileSystem abstract class.
    """"""

    def __init__(self, fs_name, event_handler=None):
        self.fs_name = fs_name
        self.debug = False
        self.set_eventhandler(event_handler)
        self.proxy_errors = []

        self.local_hostname = socket.gethostname()
        self.local_hostname_short = self.local_hostname.split('.', 1)[0]

        # file system MGT
        self.mgt = None

        # All FS server targets (MGT, MDT, OST...)
        self.targets = []

        # All FS clients
        self.clients = []

        # filled after successful install
        self.mgt_servers = NodeSet()
        self.mgt_count = 0

        self.mdt_servers = NodeSet()
        self.mdt_count = 0

        self.ost_servers = NodeSet()
        self.ost_count = 0

        self.target_count = 0
        self.target_servers = NodeSet()

    def set_debug(self, debug):
        self.debug = debug

    #
    # file system event handling
    #

    def _invoke_event(self, event, **kwargs):
        if 'target' in kwargs or 'client' in kwargs:
            kwargs.setdefault('node', None)
        getattr(self.event_handler, event)(**kwargs)

    def _invoke_dummy(self, event, **kwargs):
        pass

    def set_eventhandler(self, event_handler):
        self.event_handler = event_handler
        if self.event_handler is None:
            self._invoke = self._invoke_dummy
        else:
            self._invoke = self._invoke_event

    def _handle_shine_event(self, event, node, **params):
        #print ""_handle_shine_event %s %s"" % (event, params)
        target = params.get('target')
        if target:
            found = False
            for t in self.targets:
                if t.match(target):
                    # perform sanity checks here
                    old_nids = t.get_nids()
                    if old_nids != target.get_nids():
                        print ""NIDs mismatch %s -> %s"" % \
                                (','.join(old.nids), ','.join(target.get_nids))
                    # update target from remote one
                    t.update(target)
                    # substitute target parameter by local one
                    params['target'] = t
                    found = True
            if not found:
                print ""Target Update FAILED (%s)"" % target
        
        client = params.get('client')
        if client:
            found = False
            for c in self.clients:
                if c.match(client):
                    # update client from remote one
                    c.update(client)
                    # substitute client parameter
                    params['client'] = c
                    found = True
            if not found:
                print ""Client Update FAILED (%s)"" % client

        self._invoke(event, node=node, **params)

    def _handle_shine_proxy_error(self, nodes, message):
        self.proxy_errors.append((NodeSet(nodes), message))

    #
    # file system construction
    #

    def _attach_target(self, target):
        self.targets.append(target)
        if target.type == 'mgt':
            self.mgt = target
        self._update_structure()

    def _attach_client(self, client):
        self.clients.append(client)
        self._update_structure()

    def new_target(self, server, type, index, dev, jdev=None, group=None,
            tag=None, enabled=True):
        """"""
        Create a new attached target.
        """"""
        #print ""new_target on %s type %s (enabled=%s)"" % (server, type, enabled)

        if type == 'mgt' and self.mgt and len(self.mgt.get_nids()) > 0:
            raise FSStructureError(""A Lustre FS has only one MGT."")

        # Instantiate matching target class (eg. 'ost' -> OST).
        target = getattr(sys.modules[self.__class__.__module__], type.upper())(fs=self,
                server=server, index=index, dev=dev, jdev=jdev, group=group, tag=tag,
                enabled=enabled)
        
        return target

    def new_client(self, server, mount_path, enabled=True):
        """"""
        Create a new attached client.
        """"""
        client = Client(self, server, mount_path, enabled)

        return client

    def get_mgs_nids(self):
        return self.mgt.get_nids()
    
    def get_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients])

    def get_enabled_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients if c.action_enabled])

    def get_enabled_target_servers(self):
        return NodeSet.fromlist([t.server for t in self.targets if t.action_enabled])

    def get_client_statecounters(self):
        """"""
        Get (ignored, offline, error, runtime_error, mounted) client state counters tuple.
        """"""
        ignored = 0
        states = {}
        for client in self.clients:
            if client.action_enabled:
                state = states.setdefault(client.state, 0)
                states[client.state] = state + 1
            else:
                ignored += 1
        
        return (ignored,
                states.get(OFFLINE, 0),
                states.get(CLIENT_ERROR, 0),
                states.get(RUNTIME_ERROR, 0),
                states.get(MOUNTED, 0))

    def targets_by_state(self, state):
        for target in self.targets:
            #print target, target.state
            if target.action_enabled and target.state == state:
                yield target

    def target_servers_by_state(self, state):
        servers = NodeSet()
        for target in self.targets_by_state(state):
            #print ""OK %s"" % target
            servers.add(target.servers[0])
        return servers

    def _distant_action_by_server(self, action_class, servers, **kwargs):

        task = task_self()

        # filter local server
        if self.local_hostname in servers:
            distant_servers = servers.difference(self.local_hostname)
        elif self.local_hostname_short in servers:
            distant_servers = servers.difference(self.local_hostname_short)
        else:
            distant_servers = servers

        # perform action on distant servers
        if len(distant_servers) > 0:
            action = action_class(nodes=distant_servers, fs=self, **kwargs)
            action.launch()
            task.resume()

    def install(self, fs_config_file, nodes=None):
        """"""
        Install FS config files.
        """"""
        servers = NodeSet()

        for target in self.targets:
            # install on failover partners too
            for s in target.servers:
                if not nodes or s in nodes:
                    servers.add(s)

        for client in self.clients:
            # install on failover partners too
            if not nodes or client.server in nodes:
                servers.add(client.server)

        assert len(servers) > 0, ""no servers?""

        try:
            self._distant_action_by_server(Preinstall, servers)
            self._distant_action_by_server(Install, servers, config_file=fs_config_file)
        except ProxyActionError, e:
            # switch to public exception
            raise FSRemoteError(e.nodes, e.rc, e.message)
        
    def remove(self):
        """"""
        Remove FS config files.
        """"""

        result = 0

        servers = NodeSet()

        self.action_refcnt = 0
        self.proxy_errors = []

        # iterate over lustre servers
        for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
            if not e_s_targets:
                continue

            if server.is_local():
                # remove local fs configuration file
                conf_dir_path = Globals().get_conf_dir()
                fs_file = os.path.join(Globals().get_conf_dir(), ""%s.xmf"" % self.fs_name)
                rc = os.unlink(fs_file)
                result = max(result, rc)
            else:
                servers.add(server)

        if len(servers) > 0:
            # Perform the remove operations on all targets for these nodes.
            action = FSProxyAction(self, 'remove', servers, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR
        
        return result

    def _update_structure(self):
        # convenience
        for type, targets, servers in self._iter_targets_servers_by_type():
            if type == 'ost':
                self.ost_count = len(targets)
                self.ost_servers = NodeSet(servers)
            elif type == 'mdt':
                self.mdt_count = len(targets)
                self.mdt_servers = NodeSet(servers)
            elif type == 'mgt':
                self.mgt_count = len(targets)
                self.mgt_servers = NodeSet(servers)

        self.target_count = self.mgt_count + self.mdt_count + self.ost_count
        self.target_servers = self.mgt_servers | self.mdt_servers | self.ost_servers

    def _iter_targets_servers_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns a tuple (list of targets,
        list of servers) per target type.
        """"""
        last_target_type = None
        servers = NodeSet()
        targets = Set()

        #self.targets.sort()

        if reverse:
            self.targets.reverse()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(targets) > 0:
                    yield last_target_type, targets, servers
                    servers.clear()     # ClusterShell 1.1+ needed (sorry)
                    targets.clear()

            if target.action_enabled:
                targets.add(target)
                # select server: change master_server for -F node
                servers.add(target.get_selected_server())
            last_target_type = target.type

        if len(targets) > 0:
            yield last_target_type, targets, servers

    def targets_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns the following tuple:
        (type, (list of all targets of this type, list of enabled targets))
        per target type.
        """"""
        last_target_type = None
        a_targets = Set()
        e_targets = Set()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(a_targets) > 0:
                    yield last_target_type, (a_targets, e_targets)
                    a_targets.clear()
                    e_targets.clear()

            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)
            last_target_type = target.type

        if len(a_targets) > 0:
            yield last_target_type, (a_targets, e_targets)

    def _iter_targets_by_server(self):
        """"""
        Per server of target iterator : returns the following tuple:
        (server, (list of all server targets, list of enabled targets))
        per target server.
        """"""
        servers = {}
        for target in self.targets:
            a_targets, e_targets = servers.setdefault(target.get_selected_server(), (Set(), Set()))
            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)

        return servers.iteritems()


    def _iter_type_idx_for_targets(self, targets):
        last_target_type = None

        indexes = RangeSet(autostep=3)

        #self.targets.sort()

        for target in targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(indexes) > 0:
                    yield last_target_type, indexes
                    indexes.clear()     # CS 1.1+
            indexes.add(int(target.index))
            last_target_type = target.type

        if len(indexes) > 0:
            yield last_target_type, indexes

    def format(self, **kwargs):

        # Remember format launched, so we can check their status once
        # all operations are done.
        format_launched = Set()

        servers_formatall = NodeSet()

        self.proxy_errors = []
        self.action_refcnt = 0

        for server, (a_targets, e_targets) in self._iter_targets_by_server():

            if server.is_local():
                # local server
                for target in e_targets:
                    target.format(**kwargs)
                    self.action_refcnt += 1

                format_launched.update(e_targets)

            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action if ""format all targets on this server""
                    # is detected
                    servers_formatall.add(server)
                else:
                    # otherwise, format per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'format',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

                format_launched.update(e_targets)

        if len(servers_formatall) > 0:
            action = FSProxyAction(self, 'format', servers_formatall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check.
        for target in format_launched:
            if target.state != OFFLINE:
                return target.state

        return OFFLINE

    def status(self, flags=STATUS_ANY):
        """"""
        Get status of filesystem.
        """"""

        status_target_launched = Set()
        status_client_launched = Set()
        servers_statusall = NodeSet()
        self.action_refcnt = 0
        self.proxy_errors = []

        # prepare servers status checks
        if flags & STATUS_SERVERS:
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                if len(e_s_targets) == 0:
                    continue

                if server.is_local():
                    for target in e_s_targets:
                        target.status()
                        self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)
                else:
                    # distant server: check if all server targets have been selected
                    if len(a_s_targets) == len(e_s_targets):
                        # ""status on all targets for this server"" detected
                        servers_statusall.add(server)
                    else:
                        # status per selected targets on this server
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(e_s_targets):
                            action = FSProxyAction(self, 'status',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)

        # prepare clients status checks
        if flags & STATUS_CLIENTS:
            for client in self.clients:
                if client.action_enabled:
                    server = client.server
                    if server.is_local():
                        client.status()
                        self.action_refcnt += 1
                    elif server not in servers_statusall:
                        servers_statusall.add(server)
                    status_client_launched.add(client)

        # launch distant actions
        if len(servers_statusall) > 0:
            action = FSProxyAction(self, 'status', servers_statusall, self.debug)
            action.launch()
            self.action_refcnt += 1

        # run loop
        task_self().resume()
        
        # return a dict of {state : target list}
        rdict = {}

        # all launched targets+clients
        launched = (status_target_launched | status_client_launched)
        if self.proxy_errors:
            # find targets/clients affected by the runtime error(s)
            for target in launched:
                for nodes, msg in self.proxy_errors:
                    if target.server in nodes:
                        target.state = RUNTIME_ERROR

        for target in launched:
            if target.state == None:
                print target, target.server
            assert target.state != None
            targets = rdict.setdefault(target.state, [])
            targets.append(target)
        return rdict

    def status_target(self, target):
        """"""
        Launch a status request for a specific local or remote target.
        """"""

        # Don't call me if the target itself is not enabled.
        assert target.action_enabled

        server = target.get_selected_server()

        if server.is_local():
            # Target is local
            target.status()
        else:
            action = FSProxyAction(self, 'status', NodeSet(server), self.debug,
                    target.type, RangeSet(str(target.index)))
            action.launch()

        self.action_refcnt = 1
        task_self().resume()

    def start(self, **kwargs):
        """"""
        Start Lustre file system servers.
        """"""
        self.proxy_errors = []

        # What starting order to use?
        for target in self.targets:
            if isinstance(target, MDT) and target.action_enabled:
                # Found enabled MDT: perform writeconf check.
                self.status_target(target)
                if target.has_first_time_flag() or target.has_writeconf_flag():
                    # first_time or writeconf flag found, start MDT before OSTs
                    MDT.target_order = 2 # change MDT class variable order

        self.targets.sort()

        # servers_startall is used for optimization, it contains nodes
        # where we have to perform the start operation on all targets
        # found for this FS. This will limit the number of FSProxyAction
        # to spawn.
        servers_startall = NodeSet()

        # Remember targets launched, so we can check their status once
        # all operations are done (here, status are checked after all
        # targets of the same type have completed the start operation -
        # with possible failure).
        targets_launched = Set()

        # Keep number of actions in order to abort task correctly in
        # action's ev_close.
        self.action_refcnt = 0

        result = 0

        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():
            
            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():

                # To summary, we keep targets that are:
                # 1. enabled
                # 2. of according type
                # 3. on this server
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Start targets if we are on the good server.
                    for target in type_e_targets:
                        # Note that target.start() should never block here:
                        # it will perform necessary non-blocking actions and
                        # (when needed) will start local ClusterShell workers.
                        target.start(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""start all FS targets on this server"" detected
                        servers_startall.add(server)
                    else:
                        # Start per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'start',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched targets of this server for late status check.
                targets_launched.update(type_e_targets)

            if len(servers_startall) > 0:
                # Perform the start operations on all targets for these nodes.
                action = FSProxyAction(self, 'start', servers_startall, self.debug)
                action.launch()
                self.action_refcnt += 1

            # Resume current task, ie. start runloop, process workers events
            # and also act as a target-type barrier.
            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_launched:
                if target.state > result:
                    result = target.state
                    if result > RECOVERING:
                        # Avoid broken cascading starts, so we break now if
                        # a target of the previous type failed to start.
                        return result

            # Some needed cleanup before next target type.
            servers_startall.clear()
            targets_launched.clear()

        return result


    def stop(self, **kwargs):
        """"""
        Stop file system.
        """"""
        rc = MOUNTED

        # Stop: reverse order
        self.targets.sort()
        self.targets.reverse()

        # servers_stopall is used for optimization, see the comment in
        # start() for servers_startall.
        servers_stopall = NodeSet()

        # Remember targets when stop was launched.
        targets_stopping = Set()

        self.action_refcnt = 0
        self.proxy_errors = []

        # We use a similar logic than start(): see start() for comments.
        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():

            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Stop targets if we are on the good server.
                    for target in type_e_targets:
                        target.stop(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""stop all FS targets on this server"" detected
                        servers_stopall.add(server)
                    else:
                        # Stop per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'stop',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched stopping targets of this server for late status check.
                targets_stopping.update(type_e_targets)

            if len(servers_stopall) > 0:
                # Perform the stop operations on all targets for these nodes.
                action = FSProxyAction(self, 'stop', servers_stopall, self.debug)
                action.launch()
                self.action_refcnt += 1

            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_stopping:
                if target.state > rc:
                    rc = target.state

            # Some needed cleanup before next target type.
            servers_stopall.clear()
            targets_stopping.clear()

        return rc

    def mount(self, **kwargs):
        """"""
        Mount FS clients.
        """"""
        servers_mountall = NodeSet()
        clients_mounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.start(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_mountall.add(client.server)

            clients_mounting.add(client)

        if len(servers_mountall) > 0:
            action = FSClientProxyAction(self, 'mount', servers_mountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_mounting:
            if client.state != MOUNTED:
                return client.state

        return MOUNTED

    def umount(self, **kwargs):
        """"""
        Unmount FS clients.
        """"""
        servers_umountall = NodeSet()
        clients_umounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.stop(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_umountall.add(client.server)

            clients_umounting.add(client)

        if len(servers_umountall) > 0:
            action = FSClientProxyAction(self, 'umount', servers_umountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_umounting:
            if client.state != OFFLINE:
                return client.state

        return OFFLINE

    def info(self):
        pass

    def tune(self, tuning_model):
        """"""
        Tune server.
        """"""
        task = task_self()
        tune_all = NodeSet()
        type_map = { 'mgt': 'mgs', 'mdt': 'mds', 'ost' : 'oss' }
        self.action_refcnt = 0
        self.proxy_errors = []
        result = 0

        # Install tuning.conf on enabled distant servers
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if e_targets and not server.is_local():
                tune_all.add(server)
        if len(tune_all) > 0:
            self._distant_action_by_server(Install, tune_all, config_file=Globals().get_tuning_file())
            self.action_refcnt += 1
            task.resume()
            tune_all.clear()

        # Apply tunings
        self.action_refcnt = 0
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if not e_targets:
                continue
            if server.is_local():
                types = Set()
                for t in e_targets:
                    types.add(type_map[t.type])

                rc = server.tune(tuning_model, types, self.fs_name)
                result = max(result, rc)
            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action
                    tune_all.add(server)
                else:
                    # otherwise, tune per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'tune',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

        if len(tune_all) > 0:
            action = FSProxyAction(self, 'tune', tune_all, self.debug)
            action.launch()
            self.action_refcnt += 1

        task.resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        return result

/n/n/n",0
73,7ff203be36e439b535894764c37a8446351627ec,"/lib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()

/n/n/n/lib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            print ""Bad argument""
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return 0

/n/n/n/lib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Mount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError""
            raise

/n/n/n/lib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc
/n/n/n/lib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == ""fs"":
                self.status_view_fs(fs)
            elif view.startswith(""target""):
                self.status_view_targets(fs)
            elif view.startswith(""disk""):
                self.status_view_disks(fs)
            else:
                raise CommandBadParameterError(self.view_support.get_view(),
                        ""fs, targets, disks"")
        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/n/lib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Unmount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
            return RC_USER_ERROR
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
            return RC_RUNTIME_ERROR
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except KeyError:
            print ""Error - Unrecognized action""
            print
            raise
        
        return 1


/n/n/n/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/n",1
74,7ff203be36e439b535894764c37a8446351627ec,"lib/Shine/Commands/Base/Command.py/n/n# Command.py -- Base command class
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Support.Debug import Debug

from CommandRCDefs import *

import getopt


#
# Command exceptions are defined in Shine.Command.Exceptions
#

class Command(object):
    """"""
    The base class for command objects that can be added to the commands
    registry.
    """"""
    def __init__(self):
        self.options = {}
        self.getopt_string = """"
        self.params_desc = """"
        self.last_optional = 0
        self.arguments = None

        # All commands have debug support.
        self.debug_support = Debug(self)

    def is_hidden(self):
        """"""Return whether the command should not be displayed to user.""""""
        return False
    
    def get_name(self):
        raise NotImplementedError(""Derived classes must implement."")

    def get_desc(self):
        return ""Undocumented""

    def get_params_desc(self):
        pdesc = self.params_desc.strip()
        if self.has_subcommand():
            return ""%s %s"" % ('|'.join(self.get_subcommands()), pdesc)
        return pdesc

    def has_subcommand(self):
        """"""Return whether the command supports subcommand(s).""""""
        return False

    def get_subcommands(self):
        """"""Return the list of subcommand(s).""""""
        raise NotImplementedError(""Derived classes must implement."")
    
    def add_option(self, flag, arg, attr, cb=None):
        """"""
        Add an option for getopt with optional argument.
        """"""
        assert flag not in self.options

        optional = attr.get('optional', False)
        hidden = attr.get('hidden', False)

        if cb:
            self.options[flag] = cb

        object.__setattr__(self, ""opt_%s"" % flag, None)
            
        self.getopt_string += flag
        if optional:
            leftmark = '['
            rightmark = ']'
        else:
            leftmark = ''
            rightmark = ''

        if arg:
            self.getopt_string += "":""
            if not hidden:
                self.params_desc += ""%s-%s <%s>%s "" % (leftmark,
                    flag, arg, rightmark)
                self.last_optional = 0
        elif not hidden:
            if self.last_optional == 0:
                self.params_desc += ""%s-%s%s "" % (leftmark, flag, rightmark)
            else:
                self.params_desc = self.params_desc[:-2] + ""%s%s "" % (flag,
                    rightmark)
            
            if optional:
                self.last_optional = 1
            else:
                self.last_optional = 2

    def parse(self, args):
        """"""
        Parse command arguments.
        """"""
        options, arguments = getopt.gnu_getopt(args, self.getopt_string)
        self.arguments = arguments

        for opt, arg in options:
            trim_opt = opt[1:]
            callback = self.options.get(trim_opt)
            if callback:
                callback(trim_opt, arg)
            object.__setattr__(self, ""opt_%s"" % trim_opt, arg or True)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation.
        
        Return True when the user confirms the action, False otherwise.
        """"""
        i = raw_input(""%s (y)es/(N)o: "" % prompt)
        return i == 'y' or i == 'Y'


    def filter_rc(self, rc):
        """"""
        Allow derived classes to filter return codes.
        """"""
        # default is to not filter return code
        return rc

/n/n/nlib/Shine/Commands/Base/RemoteCommand.py/n/n# RemoteCommand.py -- Base command with remote capabilities
# Copyright (C) 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *
from Command import Command
from CommandRCDefs import *
from RemoteCallEventHandler import RemoteCallEventHandler
from Support.Nodes import Nodes
from Support.Yes import Yes

import socket


class RemoteCommand(Command):
    
    def __init__(self):
        Command.__init__(self)
        self.remote_call = False
        self.local_flag = False
        attr = { 'optional' : True, 'hidden' : True }
        self.add_option('L', None, attr, cb=self.parse_L)
        self.add_option('R', None, attr, cb=self.parse_R)
        self.nodes_support = Nodes(self)
        self.eventhandler = None

    def parse_L(self, opt, arg):
        self.local_flag = True

    def parse_R(self, opt, arg):
        self.remote_call = True

    def has_local_flag(self):
        return self.local_flag or self.remote_call

    def init_execute(self):
        """"""
        Initialize execution of remote command, if needed. Should be called
        first from derived classes before really executing the command.
        """"""
        # Limit the scope of the command if called with local flag (-L) or
        # called remotely (-R).
        if self.has_local_flag():
            self.opt_n = socket.gethostname().split('.', 1)[0]

    def install_eventhandler(self, local_eventhandler, global_eventhandler):
        """"""
        Select and install the appropriate event handler.
        """"""
        if self.remote_call:
            # When called remotely (-R), install a special event handler
            # that knows how to speak the Shine Proxy Protocol using pickle.
            self.eventhandler = RemoteCallEventHandler()
        elif self.local_flag:
            self.eventhandler = local_eventhandler
        else:
            self.eventhandler = global_eventhandler
        # return handler for convenience
        return self.eventhandler

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation. Overrides Command.ask_confirm to
        avoid confirmation when called remotely (-R).

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.remote_call or Command.ask_confirm(self, prompt)

    def filter_rc(self, rc):
        """"""
        When called remotely, return code are not used to handle shine action
        success or failure, nor for status info. To properly detect ssh or remote
        shine installation failures, we filter the return code here.
        """"""
        if self.remote_call:
            # Only errors of type RUNTIME ERROR are allowed to go up.
            rc &= RC_FLAG_RUNTIME_ERROR

        return Command.filter_rc(self, rc)


class RemoteCriticalCommand(RemoteCommand):

    def __init__(self):
        RemoteCommand.__init__(self)
        self.yes_support = Yes(self)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation if -y not specified.

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.yes_support.has_yes() or RemoteCommand.ask_confirm(self, prompt)

/n/n/nlib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        rc = command.execute()

        # Filter rc
        return command.filter_rc(rc)

/n/n/nlib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.CommandRCDefs import *
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes

from Exceptions import *

class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            raise CommandHelpException(""Lustre model file path (-m <model_file>) argument required."", self)
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return RC_OK

/n/n/nlib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Starting %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if not self.remote_call:
                if rc == RC_OK:
                    if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                        print ""Mount successful on %s"" % m_nodes
                elif rc == RC_RUNTIME_ERROR:
                    for nodes, msg in fs.proxy_errors:
                        print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.CommandRCDefs import *
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError %s"" % ex
            return RC_RUNTIME_ERROR

        return RC_OK
/n/n/nlib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

        return result
/n/n/nlib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)
            if not statusdict:
                continue

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if not self.remote_call and vlevel > 0:
                if view == ""fs"":
                    self.status_view_fs(fs)
                elif view.startswith(""target""):
                    self.status_view_targets(fs)
                elif view.startswith(""disk""):
                    self.status_view_disks(fs)
                else:
                    raise CommandBadParameterError(self.view_support.get_view(),
                            ""fs, targets, disks"")

        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/nlib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Stopping %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                    print ""Unmount successful on %s"" % m_nodes
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Configuration/FileSystem.py/n/n# FileSystem.py -- Lustre file system configuration
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$


from Globals import Globals
from Model import Model
from Exceptions import *
from TuningModel import TuningModel

from ClusterShell.NodeSet import NodeSet

from NidMap import NidMap
from TargetDevice import TargetDevice

import copy
import os
import sys


class FileSystem(Model):
    """"""
    Lustre File System Configuration class.
    """"""
    def __init__(self, fs_name=None, lmf=None, tuning_file=None):
        """""" Initialize File System config
        """"""
        self.backend = None

        globals = Globals()

        fs_conf_dir = os.path.expandvars(globals.get_conf_dir())
        fs_conf_dir = os.path.normpath(fs_conf_dir)

        # Load the file system from model or extended model
        if not fs_name and lmf:
            Model.__init__(self, lmf)

            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, self.get_one('fs_name'))

            self._setup_target_devices()

            # Reload
            self.set_filename(self.xmf_path)

        elif fs_name:
            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, fs_name)
            Model.__init__(self, self.xmf_path)

        self._setup_nid_map(self.get_one('nid_map'))

        self.fs_name = self.get_one('fs_name')
        
        # Initialize the tuning model to None if no special tuning configuration
        # is provided
        self.tuning_model = None
        
        if tuning_file:
            # It a tuning configuration file is provided load it
            self.tuning_model = TuningModel(tuning_file)
        else:
            self.tuning_model = TuningModel()

        #self._start_backend()

    def _start_backend(self):
        """"""
        Load and start backend subsystem once
        """"""
        if not self.backend:

            from Backend.BackendRegistry import BackendRegistry
            from Backend.Backend import Backend

            # Start the selected config backend system.
            self.backend = BackendRegistry().get_selected()
            if self.backend:
                self.backend.start()

        return self.backend

    def _setup_target_devices(self):
        """""" Generate the eXtended Model File XMF
        """"""
        self._start_backend()

        for target in [ 'mgt', 'mdt', 'ost' ]:

            if self.backend:

                # Returns a list of TargetDevices
                candidates = copy.copy(self.backend.get_target_devices(target))

                try:
                    # Save the model target selection
                    target_models = copy.copy(self.get(target))
                except KeyError, e:
                    raise ConfigException(""No %s target found"" %(target))

                # Delete it (to be replaced... see below)
                self.delete(target)
                 
                # Iterates on ModelDevices
                i = 0
                for target_model in target_models:
                    result = target_model.match_device(candidates)
                    if len(result) == 0 and not target == 'mgt' :
                        raise ConfigDeviceNotFoundError(target_model)
                    for matching in result:
                        candidates.remove(matching)
                        #
                        # target index is now mandatory in XMF files
                        if not matching.has_index():
                            matching.add_index(i)
                            i += 1

                        # `matching' is a TargetDevice, we want to add it to the
                        # underlying Model object. The current way to do this to
                        # create a configuration line string (performed by
                        # TargetDevice.getline()) and then call Model.add(). 
                        # TODO: add methods to Model/ModelDevice to avoid the use
                        #       of temporary configuration string line.
                        self.add(target, matching.getline())
            else:
                # no backend support

                devices = copy.copy(self.get_with_dict(target))

                self.delete(target)

                target_devices = []
                i = 0
                for dict in devices:
                    t = TargetDevice(target, dict)
                    if not t.has_index():
                        t.add_index(i)
                        i += 1
                    target_devices.append(TargetDevice(target, dict))
                    self.add(target, t.getline())

                if len(target_devices) == 0:
                    raise ConfigDeviceNotFoundError(self)




        # Save XMF
        self.save(self.xmf_path, ""Shine Lustre file system config file for %s"" % \
                self.get_one('fs_name'))
            
    def _setup_nid_map(self, maps):
        """"""
        Set self.nid_map using the NidMap helper class
        """"""
        #self.nid_map = NidMap().fromlist(maps)
        self.nid_map = NidMap(maps.get_one('nodes'), maps.get_one('nids'))

    def get_nid(self, node):
        try:
            return self.nid_map[node]
        except KeyError:
            raise ConfigException(""Cannot get NID for %s, aborting. Please verify `nid_map' configuration."" % node)

    def __str__(self):
        return "">> BACKEND:\n%s\n>> MODEL:\n%s"" % (self.backend, Model.__str__(self))

    def close(self):
        if self.backend:
            self.backend.stop()
            self.backend = None
    
    def register_client(self, node):
        """"""
        This function aims to register a new client that will be able to mount the
        file system.
        Parameters:
        @type node: string
        @param node : is the new client node name
        """"""
        if self._start_backend():
            self.backend.register_client(self.fs_name, node)
        
    def unregister_client(self, node):
        """"""
        This function aims to unregister a client of this  file system
        Parameters:
        @type node: string
        @param node : is name of the client node to unregister
        """"""
        if self._start_backend():
            self.backend.unregister_client(self.fs_name, node)
    
    def set_status_client_mount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                    self.backend.MOUNT_COMPLETE, options)

    def set_status_client_mount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_FAILED, options)

    def set_status_client_mount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_WARNING, options)

    def set_status_client_umount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_COMPLETE, options)

    def set_status_client_umount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_FAILED, options)

    def set_status_client_umount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_WARNING, options)

    def get_status_clients(self):
        if self._start_backend():
            return self.backend.get_status_clients(self.fs_name)

    def set_status_target_unknown(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNKNOWN
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, node, 
                self.backend.TARGET_UNKNOWN, options)

    def set_status_target_ko(self, target, options):
        """"""
        This function is used to set the specified target status
        to KO
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                backend.TARGET_KO, options)

    def set_status_target_available(self, target, options):
        """"""
        This function is used to set the specified target status
        to AVAILABLE
        """"""
        if self._start_backend():
            # Set the fs_name to Free since these targets are availble
            # which means not used by any file system.
            self.backend.set_status_target(None, target,
                self.backend.TARGET_AVAILABLE, options)

    def set_status_target_formating(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATING, options)

    def set_status_target_format_failed(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMAT_FAILED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMAT_FAILED, options)

    def set_status_target_formated(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATED, options)

    def set_status_target_offline(self, target, options):
        """"""
        This function is used to set the specified target status
        to OFFLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_OFFLINE, options)

    def set_status_target_starting(self, target, options):
        """"""
        This function is used to set the specified target status
        to STARTING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STARTING, options)

    def set_status_target_online(self, target, options):
        """"""
        This function is used to set the specified target status
        to ONLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_ONLINE, options)

    def set_status_target_critical(self, target, options):
        """"""
        This function is used to set the specified target status
        to CRITICAL
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_CRITICAL, options)

    def set_status_target_stopping(self, target, options):
        """"""
        This function is used to set the specified target status
        to STOPPING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STOPPING, options)

    def set_status_target_unreachable(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNREACHABLE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_UNREACHABLE, options)

    def get_status_targets(self):
        """"""
        This function returns the status of each targets
        involved in the current file system.
        """"""
        if self._start_backend():
            return self.backend.get_status_targets(self.fs_name)

    def register(self):
        """"""
        This function aims to register the file system configuration
        to the backend.
        """"""
        if self._start_backend():
            return self.backend.register_fs(self)

    def unregister(self):
        """"""
        This function aims to remove a file system configuration from
        the backend.        
        """"""
        result = 0
        if self._start_backend():
            result = self.backend.unregister_fs(self)

        if not result:
            os.unlink(self.xmf_path)

        return result
/n/n/nlib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
        except KeyError:
            raise
        
        return RC_RUNTIME_ERROR


/n/n/nlib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # some common remote errors:
            # rc 127 = command not found
            # rc 126 = found but not executable
            # rc 1 = python failure...
            if rc != 0:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/nlib/Shine/Lustre/FileSystem.py/n/n# FileSystem.py -- Lustre FS
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Lustre FileSystem class.

Represents a Lustre FS.
""""""

import copy
from sets import Set
import socket

from ClusterShell.NodeSet import NodeSet, RangeSet

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

# Action exceptions
from Actions.Action import ActionErrorException
from Actions.Proxies.ProxyAction import *

from Actions.Install import Install
from Actions.Proxies.Preinstall import Preinstall
from Actions.Proxies.FSProxyAction import FSProxyAction
from Actions.Proxies.FSClientProxyAction import FSClientProxyAction

from EventHandler import *
from Client import *
from Server import *
from Target import *


class FSException(Exception):
    def __init__(self, message):
        self.message = message
    def __str__(self):
        return self.message

class FSError(FSException):
    """"""
    Base FileSystem error exception.
    """"""

class FSSyntaxError(FSError):
    def __init__(self, message):
        self.message = ""Syntax error: \""%s\"""" % (message)
    def __str__(self):
        return self.message

class FSBadTargetError(FSSyntaxError):
    def __init__(self, target_name):
        self.message = ""Syntax error: unrecognized target \""%s\"""" % (target_name)

class FSStructureError(FSError):
    """"""
    Lustre file system structure error, raised after an invalid configuration
    is encountered. For example, you will get this error if you try to assign
    two targets `MGT' to a filesystem.
    """"""

class FSRemoteError(FSError):
    """"""
    Remote host(s) not available, or a remote operation failed.
    """"""
    def __init__(self, nodes, rc, message):
        FSError.__init__(self, message)
        self.nodes = nodes
        self.rc = int(rc)

    def __str__(self):
        return ""%s: %s [rc=%d]"" % (self.nodes, self.message, self.rc)


STATUS_SERVERS      = 0x01
STATUS_HASERVERS    = 0x02
STATUS_CLIENTS      = 0x10
STATUS_ANY          = 0xff


class FileSystem:
    """"""
    The Lustre FileSystem abstract class.
    """"""

    def __init__(self, fs_name, event_handler=None):
        self.fs_name = fs_name
        self.debug = False
        self.set_eventhandler(event_handler)
        self.proxy_errors = []

        self.local_hostname = socket.gethostname()
        self.local_hostname_short = self.local_hostname.split('.', 1)[0]

        # file system MGT
        self.mgt = None

        # All FS server targets (MGT, MDT, OST...)
        self.targets = []

        # All FS clients
        self.clients = []

        # filled after successful install
        self.mgt_servers = NodeSet()
        self.mgt_count = 0

        self.mdt_servers = NodeSet()
        self.mdt_count = 0

        self.ost_servers = NodeSet()
        self.ost_count = 0

        self.target_count = 0
        self.target_servers = NodeSet()

    def set_debug(self, debug):
        self.debug = debug

    #
    # file system event handling
    #

    def _invoke_event(self, event, **kwargs):
        if 'target' in kwargs or 'client' in kwargs:
            kwargs.setdefault('node', None)
        getattr(self.event_handler, event)(**kwargs)

    def _invoke_dummy(self, event, **kwargs):
        pass

    def set_eventhandler(self, event_handler):
        self.event_handler = event_handler
        if self.event_handler is None:
            self._invoke = self._invoke_dummy
        else:
            self._invoke = self._invoke_event

    def _handle_shine_event(self, event, node, **params):
        #print ""_handle_shine_event %s %s"" % (event, params)
        target = params.get('target')
        if target:
            found = False
            for t in self.targets:
                if t.match(target):
                    # perform sanity checks here
                    old_nids = t.get_nids()
                    if old_nids != target.get_nids():
                        print ""NIDs mismatch %s -> %s"" % \
                                (','.join(old.nids), ','.join(target.get_nids))
                    # update target from remote one
                    t.update(target)
                    # substitute target parameter by local one
                    params['target'] = t
                    found = True
            if not found:
                print ""Target Update FAILED (%s)"" % target
        
        client = params.get('client')
        if client:
            found = False
            for c in self.clients:
                if c.match(client):
                    # update client from remote one
                    c.update(client)
                    # substitute client parameter
                    params['client'] = c
                    found = True
            if not found:
                print ""Client Update FAILED (%s)"" % client

        self._invoke(event, node=node, **params)

    def _handle_shine_proxy_error(self, nodes, message):
        self.proxy_errors.append((NodeSet(nodes), message))

    #
    # file system construction
    #

    def _attach_target(self, target):
        self.targets.append(target)
        if target.type == 'mgt':
            self.mgt = target
        self._update_structure()

    def _attach_client(self, client):
        self.clients.append(client)
        self._update_structure()

    def new_target(self, server, type, index, dev, jdev=None, group=None,
            tag=None, enabled=True):
        """"""
        Create a new attached target.
        """"""
        #print ""new_target on %s type %s (enabled=%s)"" % (server, type, enabled)

        if type == 'mgt' and self.mgt and len(self.mgt.get_nids()) > 0:
            raise FSStructureError(""A Lustre FS has only one MGT."")

        # Instantiate matching target class (eg. 'ost' -> OST).
        target = getattr(sys.modules[self.__class__.__module__], type.upper())(fs=self,
                server=server, index=index, dev=dev, jdev=jdev, group=group, tag=tag,
                enabled=enabled)
        
        return target

    def new_client(self, server, mount_path, enabled=True):
        """"""
        Create a new attached client.
        """"""
        client = Client(self, server, mount_path, enabled)

        return client

    def get_mgs_nids(self):
        return self.mgt.get_nids()
    
    def get_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients])

    def get_enabled_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients if c.action_enabled])

    def get_enabled_target_servers(self):
        return NodeSet.fromlist([t.server for t in self.targets if t.action_enabled])

    def get_client_statecounters(self):
        """"""
        Get (ignored, offline, error, runtime_error, mounted) client state counters tuple.
        """"""
        ignored = 0
        states = {}
        for client in self.clients:
            if client.action_enabled:
                state = states.setdefault(client.state, 0)
                states[client.state] = state + 1
            else:
                ignored += 1
        
        return (ignored,
                states.get(OFFLINE, 0),
                states.get(CLIENT_ERROR, 0),
                states.get(RUNTIME_ERROR, 0),
                states.get(MOUNTED, 0))

    def targets_by_state(self, state):
        for target in self.targets:
            #print target, target.state
            if target.action_enabled and target.state == state:
                yield target

    def target_servers_by_state(self, state):
        servers = NodeSet()
        for target in self.targets_by_state(state):
            #print ""OK %s"" % target
            servers.add(target.servers[0])
        return servers

    def _distant_action_by_server(self, action_class, servers, **kwargs):

        task = task_self()

        # filter local server
        if self.local_hostname in servers:
            distant_servers = servers.difference(self.local_hostname)
        elif self.local_hostname_short in servers:
            distant_servers = servers.difference(self.local_hostname_short)
        else:
            distant_servers = servers

        # perform action on distant servers
        if len(distant_servers) > 0:
            action = action_class(nodes=distant_servers, fs=self, **kwargs)
            action.launch()
            task.resume()

    def install(self, fs_config_file, nodes=None):
        """"""
        Install FS config files.
        """"""
        servers = NodeSet()

        for target in self.targets:
            # install on failover partners too
            for s in target.servers:
                if not nodes or s in nodes:
                    servers.add(s)

        for client in self.clients:
            # install on failover partners too
            if not nodes or client.server in nodes:
                servers.add(client.server)

        assert len(servers) > 0, ""no servers?""

        try:
            self._distant_action_by_server(Preinstall, servers)
            self._distant_action_by_server(Install, servers, config_file=fs_config_file)
        except ProxyActionError, e:
            # switch to public exception
            raise FSRemoteError(e.nodes, e.rc, e.message)
        
    def remove(self):
        """"""
        Remove FS config files.
        """"""

        result = 0

        servers = NodeSet()

        self.action_refcnt = 0
        self.proxy_errors = []

        # iterate over lustre servers
        for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
            if not e_s_targets:
                continue

            if server.is_local():
                # remove local fs configuration file
                conf_dir_path = Globals().get_conf_dir()
                fs_file = os.path.join(Globals().get_conf_dir(), ""%s.xmf"" % self.fs_name)
                rc = os.unlink(fs_file)
                result = max(result, rc)
            else:
                servers.add(server)

        if len(servers) > 0:
            # Perform the remove operations on all targets for these nodes.
            action = FSProxyAction(self, 'remove', servers, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR
        
        return result

    def _update_structure(self):
        # convenience
        for type, targets, servers in self._iter_targets_servers_by_type():
            if type == 'ost':
                self.ost_count = len(targets)
                self.ost_servers = NodeSet(servers)
            elif type == 'mdt':
                self.mdt_count = len(targets)
                self.mdt_servers = NodeSet(servers)
            elif type == 'mgt':
                self.mgt_count = len(targets)
                self.mgt_servers = NodeSet(servers)

        self.target_count = self.mgt_count + self.mdt_count + self.ost_count
        self.target_servers = self.mgt_servers | self.mdt_servers | self.ost_servers

    def _iter_targets_servers_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns a tuple (list of targets,
        list of servers) per target type.
        """"""
        last_target_type = None
        servers = NodeSet()
        targets = Set()

        #self.targets.sort()

        if reverse:
            self.targets.reverse()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(targets) > 0:
                    yield last_target_type, targets, servers
                    servers.clear()     # ClusterShell 1.1+ needed (sorry)
                    targets.clear()

            if target.action_enabled:
                targets.add(target)
                # select server: change master_server for -F node
                servers.add(target.get_selected_server())
            last_target_type = target.type

        if len(targets) > 0:
            yield last_target_type, targets, servers

    def targets_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns the following tuple:
        (type, (list of all targets of this type, list of enabled targets))
        per target type.
        """"""
        last_target_type = None
        a_targets = Set()
        e_targets = Set()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(a_targets) > 0:
                    yield last_target_type, (a_targets, e_targets)
                    a_targets.clear()
                    e_targets.clear()

            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)
            last_target_type = target.type

        if len(a_targets) > 0:
            yield last_target_type, (a_targets, e_targets)

    def _iter_targets_by_server(self):
        """"""
        Per server of target iterator : returns the following tuple:
        (server, (list of all server targets, list of enabled targets))
        per target server.
        """"""
        servers = {}
        for target in self.targets:
            a_targets, e_targets = servers.setdefault(target.get_selected_server(), (Set(), Set()))
            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)

        return servers.iteritems()


    def _iter_type_idx_for_targets(self, targets):
        last_target_type = None

        indexes = RangeSet(autostep=3)

        #self.targets.sort()

        for target in targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(indexes) > 0:
                    yield last_target_type, indexes
                    indexes.clear()     # CS 1.1+
            indexes.add(int(target.index))
            last_target_type = target.type

        if len(indexes) > 0:
            yield last_target_type, indexes

    def format(self, **kwargs):

        # Remember format launched, so we can check their status once
        # all operations are done.
        format_launched = Set()

        servers_formatall = NodeSet()

        self.proxy_errors = []
        self.action_refcnt = 0

        for server, (a_targets, e_targets) in self._iter_targets_by_server():

            if server.is_local():
                # local server
                for target in e_targets:
                    target.format(**kwargs)
                    self.action_refcnt += 1

                format_launched.update(e_targets)

            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action if ""format all targets on this server""
                    # is detected
                    servers_formatall.add(server)
                else:
                    # otherwise, format per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'format',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

                format_launched.update(e_targets)

        if len(servers_formatall) > 0:
            action = FSProxyAction(self, 'format', servers_formatall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check.
        for target in format_launched:
            if target.state != OFFLINE:
                return target.state

        return OFFLINE

    def status(self, flags=STATUS_ANY):
        """"""
        Get status of filesystem.
        """"""

        status_target_launched = Set()
        status_client_launched = Set()
        servers_statusall = NodeSet()
        self.action_refcnt = 0
        self.proxy_errors = []

        # prepare servers status checks
        if flags & STATUS_SERVERS:
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                if len(e_s_targets) == 0:
                    continue

                if server.is_local():
                    for target in e_s_targets:
                        target.status()
                        self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)
                else:
                    # distant server: check if all server targets have been selected
                    if len(a_s_targets) == len(e_s_targets):
                        # ""status on all targets for this server"" detected
                        servers_statusall.add(server)
                    else:
                        # status per selected targets on this server
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(e_s_targets):
                            action = FSProxyAction(self, 'status',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)

        # prepare clients status checks
        if flags & STATUS_CLIENTS:
            for client in self.clients:
                if client.action_enabled:
                    server = client.server
                    if server.is_local():
                        client.status()
                        self.action_refcnt += 1
                    elif server not in servers_statusall:
                        servers_statusall.add(server)
                    status_client_launched.add(client)

        # launch distant actions
        if len(servers_statusall) > 0:
            action = FSProxyAction(self, 'status', servers_statusall, self.debug)
            action.launch()
            self.action_refcnt += 1

        # run loop
        task_self().resume()
        
        # return a dict of {state : target list}
        rdict = {}

        # all launched targets+clients
        launched = (status_target_launched | status_client_launched)
        if self.proxy_errors:
            # find targets/clients affected by the runtime error(s)
            for target in launched:
                for nodes, msg in self.proxy_errors:
                    if target.server in nodes:
                        target.state = RUNTIME_ERROR

        for target in launched:
            if target.state == None:
                print target, target.server
            assert target.state != None
            targets = rdict.setdefault(target.state, [])
            targets.append(target)
        return rdict

    def status_target(self, target):
        """"""
        Launch a status request for a specific local or remote target.
        """"""

        # Don't call me if the target itself is not enabled.
        assert target.action_enabled

        server = target.get_selected_server()

        if server.is_local():
            # Target is local
            target.status()
        else:
            action = FSProxyAction(self, 'status', NodeSet(server), self.debug,
                    target.type, RangeSet(str(target.index)))
            action.launch()

        self.action_refcnt = 1
        task_self().resume()

    def start(self, **kwargs):
        """"""
        Start Lustre file system servers.
        """"""
        self.proxy_errors = []

        # What starting order to use?
        for target in self.targets:
            if isinstance(target, MDT) and target.action_enabled:
                # Found enabled MDT: perform writeconf check.
                self.status_target(target)
                if target.has_first_time_flag() or target.has_writeconf_flag():
                    # first_time or writeconf flag found, start MDT before OSTs
                    MDT.target_order = 2 # change MDT class variable order

        self.targets.sort()

        # servers_startall is used for optimization, it contains nodes
        # where we have to perform the start operation on all targets
        # found for this FS. This will limit the number of FSProxyAction
        # to spawn.
        servers_startall = NodeSet()

        # Remember targets launched, so we can check their status once
        # all operations are done (here, status are checked after all
        # targets of the same type have completed the start operation -
        # with possible failure).
        targets_launched = Set()

        # Keep number of actions in order to abort task correctly in
        # action's ev_close.
        self.action_refcnt = 0

        result = 0

        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():
            
            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():

                # To summary, we keep targets that are:
                # 1. enabled
                # 2. of according type
                # 3. on this server
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Start targets if we are on the good server.
                    for target in type_e_targets:
                        # Note that target.start() should never block here:
                        # it will perform necessary non-blocking actions and
                        # (when needed) will start local ClusterShell workers.
                        target.start(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""start all FS targets on this server"" detected
                        servers_startall.add(server)
                    else:
                        # Start per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'start',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched targets of this server for late status check.
                targets_launched.update(type_e_targets)

            if len(servers_startall) > 0:
                # Perform the start operations on all targets for these nodes.
                action = FSProxyAction(self, 'start', servers_startall, self.debug)
                action.launch()
                self.action_refcnt += 1

            # Resume current task, ie. start runloop, process workers events
            # and also act as a target-type barrier.
            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_launched:
                if target.state > result:
                    result = target.state
                    if result > RECOVERING:
                        # Avoid broken cascading starts, so we break now if
                        # a target of the previous type failed to start.
                        return result

            # Some needed cleanup before next target type.
            servers_startall.clear()
            targets_launched.clear()

        return result


    def stop(self, **kwargs):
        """"""
        Stop file system.
        """"""
        rc = MOUNTED

        # Stop: reverse order
        self.targets.sort()
        self.targets.reverse()

        # servers_stopall is used for optimization, see the comment in
        # start() for servers_startall.
        servers_stopall = NodeSet()

        # Remember targets when stop was launched.
        targets_stopping = Set()

        self.action_refcnt = 0
        self.proxy_errors = []

        # We use a similar logic than start(): see start() for comments.
        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():

            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Stop targets if we are on the good server.
                    for target in type_e_targets:
                        target.stop(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""stop all FS targets on this server"" detected
                        servers_stopall.add(server)
                    else:
                        # Stop per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'stop',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched stopping targets of this server for late status check.
                targets_stopping.update(type_e_targets)

            if len(servers_stopall) > 0:
                # Perform the stop operations on all targets for these nodes.
                action = FSProxyAction(self, 'stop', servers_stopall, self.debug)
                action.launch()
                self.action_refcnt += 1

            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_stopping:
                if target.state > rc:
                    rc = target.state

            # Some needed cleanup before next target type.
            servers_stopall.clear()
            targets_stopping.clear()

        return rc

    def mount(self, **kwargs):
        """"""
        Mount FS clients.
        """"""
        servers_mountall = NodeSet()
        clients_mounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.start(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_mountall.add(client.server)

            clients_mounting.add(client)

        if len(servers_mountall) > 0:
            action = FSClientProxyAction(self, 'mount', servers_mountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_mounting:
            if client.state != MOUNTED:
                return client.state

        return MOUNTED

    def umount(self, **kwargs):
        """"""
        Unmount FS clients.
        """"""
        servers_umountall = NodeSet()
        clients_umounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.stop(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_umountall.add(client.server)

            clients_umounting.add(client)

        if len(servers_umountall) > 0:
            action = FSClientProxyAction(self, 'umount', servers_umountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_umounting:
            if client.state != OFFLINE:
                return client.state

        return OFFLINE

    def info(self):
        pass

    def tune(self, tuning_model):
        """"""
        Tune server.
        """"""
        task = task_self()
        tune_all = NodeSet()
        type_map = { 'mgt': 'mgs', 'mdt': 'mds', 'ost' : 'oss' }
        self.action_refcnt = 0
        self.proxy_errors = []
        result = 0

        # Install tuning.conf on enabled distant servers
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if e_targets and not server.is_local():
                tune_all.add(server)
        if len(tune_all) > 0:
            self._distant_action_by_server(Install, tune_all, config_file=Globals().get_tuning_file())
            self.action_refcnt += 1
            task.resume()
            tune_all.clear()

        # Apply tunings
        self.action_refcnt = 0
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if not e_targets:
                continue
            if server.is_local():
                types = Set()
                for t in e_targets:
                    types.add(type_map[t.type])

                rc = server.tune(tuning_model, types, self.fs_name)
                result = max(result, rc)
            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action
                    tune_all.add(server)
                else:
                    # otherwise, tune per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'tune',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

        if len(tune_all) > 0:
            action = FSProxyAction(self, 'tune', tune_all, self.debug)
            action.launch()
            self.action_refcnt += 1

        task.resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        return result

/n/n/n",0
75,7ff203be36e439b535894764c37a8446351627ec,"/lib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()

/n/n/n/lib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            print ""Bad argument""
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return 0

/n/n/n/lib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Mount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError""
            raise

/n/n/n/lib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc
/n/n/n/lib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == ""fs"":
                self.status_view_fs(fs)
            elif view.startswith(""target""):
                self.status_view_targets(fs)
            elif view.startswith(""disk""):
                self.status_view_disks(fs)
            else:
                raise CommandBadParameterError(self.view_support.get_view(),
                        ""fs, targets, disks"")
        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/n/lib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Unmount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
            return RC_USER_ERROR
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
            return RC_RUNTIME_ERROR
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except KeyError:
            print ""Error - Unrecognized action""
            print
            raise
        
        return 1


/n/n/n/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/n",1
76,9d9b01839cf3639e59d29c27e70688bdbf44db96,"classes.py/n/n""""""
classes.py - Base classes for PyLink IRC Services.

This module contains the base classes used by PyLink, including threaded IRC
connections and objects used to represent IRC servers, users, and channels.

Here be dragons.
""""""

import threading
import time
import socket
import ssl
import hashlib
from copy import deepcopy
import inspect
import re
from collections import defaultdict, deque
import ipaddress

try:
    import ircmatch
except ImportError:
    raise ImportError(""PyLink requires ircmatch to function; please install it and try again."")

from . import world, utils, structures, conf, __version__
from .log import *

### Exceptions

class ProtocolError(RuntimeError):
    pass

### Internal classes (users, servers, channels)

class Irc(utils.DeprecatedAttributesObject):
    """"""Base IRC object for PyLink.""""""

    def __init__(self, netname, proto, conf):
        """"""
        Initializes an IRC object. This takes 3 variables: the network name
        (a string), the name of the protocol module to use for this connection,
        and a configuration object.
        """"""
        self.deprecated_attributes = {
            'conf': 'Deprecated since 1.2; consider switching to conf.conf',
            'botdata': ""Deprecated since 1.2; consider switching to conf.conf['bot']"",
        }

        self.loghandlers = []
        self.name = netname
        self.conf = conf
        self.sid = None
        self.serverdata = conf['servers'][netname]
        self.botdata = conf['bot']
        self.protoname = proto.__name__.split('.')[-1]  # Remove leading pylinkirc.protocols.
        self.proto = proto.Class(self)
        self.pingfreq = self.serverdata.get('pingfreq') or 90
        self.pingtimeout = self.pingfreq * 2

        self.queue = deque()

        self.connected = threading.Event()
        self.aborted = threading.Event()
        self.reply_lock = threading.RLock()

        self.pingTimer = None

        # Sets the multiplier for autoconnect delay (grows with time).
        self.autoconnect_active_multiplier = 1

        self.initVars()

        if world.testing:
            # HACK: Don't thread if we're running tests.
            self.connect()
        else:
            self.connection_thread = threading.Thread(target=self.connect,
                                                      name=""Listener for %s"" %
                                                      self.name)
            self.connection_thread.start()

    def logSetup(self):
        """"""
        Initializes any channel loggers defined for the current network.
        """"""
        try:
            channels = conf.conf['logging']['channels'][self.name]
        except KeyError:  # Not set up; just ignore.
            return

        log.debug('(%s) Setting up channel logging to channels %r', self.name,
                  channels)

        if not self.loghandlers:
            # Only create handlers if they haven't already been set up.

            for channel, chandata in channels.items():
                # Fetch the log level for this channel block.
                level = None
                if chandata is not None:
                    level = chandata.get('loglevel')

                handler = PyLinkChannelLogger(self, channel, level=level)
                self.loghandlers.append(handler)
                log.addHandler(handler)

    def initVars(self):
        """"""
        (Re)sets an IRC object to its default state. This should be called when
        an IRC object is first created, and on every reconnection to a network.
        """"""
        self.pingfreq = self.serverdata.get('pingfreq') or 90
        self.pingtimeout = self.pingfreq * 3

        self.pseudoclient = None
        self.lastping = time.time()

        self.queue.clear()

        # Internal variable to set the place and caller of the last command (in PM
        # or in a channel), used by fantasy command support.
        self.called_by = None
        self.called_in = None

        # Intialize the server, channel, and user indexes to be populated by
        # our protocol module. For the server index, we can add ourselves right
        # now.
        self.servers = {}
        self.users = {}
        self.channels = structures.KeyedDefaultdict(IrcChannel)

        # This sets the list of supported channel and user modes: the default
        # RFC1459 modes are implied. Named modes are used here to make
        # protocol-independent code easier to write, as mode chars vary by
        # IRCd.
        # Protocol modules should add to and/or replace this with what their
        # protocol supports. This can be a hardcoded list or something
        # negotiated on connect, depending on the nature of their protocol.
        self.cmodes = {'op': 'o', 'secret': 's', 'private': 'p',
                       'noextmsg': 'n', 'moderated': 'm', 'inviteonly': 'i',
                       'topiclock': 't', 'limit': 'l', 'ban': 'b',
                       'voice': 'v', 'key': 'k',
                       # This fills in the type of mode each mode character is.
                       # A-type modes are list modes (i.e. bans, ban exceptions, etc.),
                       # B-type modes require an argument to both set and unset,
                       #   but there can only be one value at a time
                       #   (i.e. cmode +k).
                       # C-type modes require an argument to set but not to unset
                       #   (one sets ""+l limit"" and # ""-l""),
                       # and D-type modes take no arguments at all.
                       '*A': 'b',
                       '*B': 'k',
                       '*C': 'l',
                       '*D': 'imnpstr'}
        self.umodes = {'invisible': 'i', 'snomask': 's', 'wallops': 'w',
                       'oper': 'o',
                       '*A': '', '*B': '', '*C': '', '*D': 'iosw'}

        # This max nick length starts off as the config value, but may be
        # overwritten later by the protocol module if such information is
        # received. It defaults to 30.
        self.maxnicklen = self.serverdata.get('maxnicklen', 30)

        # Defines a list of supported prefix modes.
        self.prefixmodes = {'o': '@', 'v': '+'}

        # Defines the uplink SID (to be filled in by protocol module).
        self.uplink = None
        self.start_ts = int(time.time())

        # Set up channel logging for the network
        self.logSetup()

    def processQueue(self):
        """"""Loop to process outgoing queue data.""""""
        while not self.aborted.is_set():
            if self.queue:  # Only process if there's data.
                data = self.queue.popleft()
                self._send(data)
            throttle_time = self.serverdata.get('throttle_time', 0.005)
            self.aborted.wait(throttle_time)
        log.debug('(%s) Stopping queue thread as aborted is set', self.name)

    def connect(self):
        """"""
        Runs the connect loop for the IRC object. This is usually called by
        __init__ in a separate thread to allow multiple concurrent connections.
        """"""
        while True:

            self.aborted.clear()
            self.initVars()

            try:
                self.proto.validateServerConf()
            except AssertionError as e:
                log.exception(""(%s) Configuration error: %s"", self.name, e)
                return

            ip = self.serverdata[""ip""]
            port = self.serverdata[""port""]
            checks_ok = True
            try:
                # Set the socket type (IPv6 or IPv4).
                stype = socket.AF_INET6 if self.serverdata.get(""ipv6"") else socket.AF_INET

                # Creat the socket.
                self.socket = socket.socket(stype)
                self.socket.setblocking(0)

                # Set the socket bind if applicable.
                if 'bindhost' in self.serverdata:
                    self.socket.bind((self.serverdata['bindhost'], 0))

                # Set the connection timeouts. Initial connection timeout is a
                # lot smaller than the timeout after we've connected; this is
                # intentional.
                self.socket.settimeout(self.pingfreq)

                # Resolve hostnames if it's not an IP address already.
                old_ip = ip
                ip = socket.getaddrinfo(ip, port, stype)[0][-1][0]
                log.debug('(%s) Resolving address %s to %s', self.name, old_ip, ip)

                # Enable SSL if set to do so. This requires a valid keyfile and
                # certfile to be present.
                self.ssl = self.serverdata.get('ssl')
                if self.ssl:
                    log.info('(%s) Attempting SSL for this connection...', self.name)
                    certfile = self.serverdata.get('ssl_certfile')
                    keyfile = self.serverdata.get('ssl_keyfile')

                    context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
                    # Disable SSLv2 and SSLv3 - these are insecure
                    context.options |= ssl.OP_NO_SSLv2
                    context.options |= ssl.OP_NO_SSLv3

                    if certfile and keyfile:
                        try:
                            context.load_cert_chain(certfile, keyfile)
                        except OSError:
                             log.exception('(%s) Caught OSError trying to '
                                           'initialize the SSL connection; '
                                           'are ""ssl_certfile"" and '
                                           '""ssl_keyfile"" set correctly?',
                                           self.name)
                             checks_ok = False

                    self.socket = context.wrap_socket(self.socket)

                log.info(""Connecting to network %r on %s:%s"", self.name, ip, port)
                self.socket.connect((ip, port))
                self.socket.settimeout(self.pingtimeout)

                # If SSL was enabled, optionally verify the certificate
                # fingerprint for some added security. I don't bother to check
                # the entire certificate for validity, since most IRC networks
                # self-sign their certificates anyways.
                if self.ssl and checks_ok:
                    peercert = self.socket.getpeercert(binary_form=True)

                    # Hash type is configurable using the ssl_fingerprint_type
                    # value, and defaults to sha256.
                    hashtype = self.serverdata.get('ssl_fingerprint_type', 'sha256').lower()

                    try:
                        hashfunc = getattr(hashlib, hashtype)
                    except AttributeError:
                        log.error('(%s) Unsupported SSL certificate fingerprint type %r given, disconnecting...',
                                  self.name, hashtype)
                        checks_ok = False
                    else:
                        fp = hashfunc(peercert).hexdigest()
                        expected_fp = self.serverdata.get('ssl_fingerprint')

                        if expected_fp and checks_ok:
                            if fp != expected_fp:
                                # SSL Fingerprint doesn't match; break.
                                log.error('(%s) Uplink\'s SSL certificate '
                                          'fingerprint (%s) does not match the '
                                          'one configured: expected %r, got %r; '
                                          'disconnecting...', self.name, hashtype,
                                          expected_fp, fp)
                                checks_ok = False
                            else:
                                log.info('(%s) Uplink SSL certificate fingerprint '
                                         '(%s) verified: %r', self.name, hashtype,
                                         fp)
                        else:
                            log.info('(%s) Uplink\'s SSL certificate fingerprint (%s) '
                                     'is %r. You can enhance the security of your '
                                     'link by specifying this in a ""ssl_fingerprint""'
                                     ' option in your server block.', self.name,
                                     hashtype, fp)

                if checks_ok:

                    self.queue_thread = threading.Thread(name=""Queue thread for %s"" % self.name,
                                                         target=self.processQueue, daemon=True)
                    self.queue_thread.start()

                    self.sid = self.serverdata.get(""sid"")
                    # All our checks passed, get the protocol module to connect and run the listen
                    # loop. This also updates any SID values should the protocol module do so.
                    self.proto.connect()

                    log.info('(%s) Enumerating our own SID %s', self.name, self.sid)
                    host = self.hostname()

                    self.servers[self.sid] = IrcServer(None, host, internal=True,
                            desc=self.serverdata.get('serverdesc')
                            or conf.conf['bot']['serverdesc'])

                    log.info('(%s) Starting ping schedulers....', self.name)
                    self.schedulePing()
                    log.info('(%s) Server ready; listening for data.', self.name)
                    self.autoconnect_active_multiplier = 1  # Reset any extra autoconnect delays
                    self.run()
                else:  # Configuration error :(
                    log.error('(%s) A configuration error was encountered '
                              'trying to set up this connection. Please check'
                              ' your configuration file and try again.',
                              self.name)
            # self.run() or the protocol module it called raised an exception, meaning we've disconnected!
            # Note: socket.error, ConnectionError, IOError, etc. are included in OSError since Python 3.3,
            # so we don't need to explicitly catch them here.
            # We also catch SystemExit here as a way to abort out connection threads properly, and stop the
            # IRC connection from freezing instead.
            except (OSError, RuntimeError, SystemExit) as e:
                log.error('(%s) Disconnected from IRC: %s: %s',
                          self.name, type(e).__name__, str(e))

            self.disconnect()

            # If autoconnect is enabled, loop back to the start. Otherwise,
            # return and stop.
            autoconnect = self.serverdata.get('autoconnect')

            # Sets the autoconnect growth multiplier (e.g. a value of 2 multiplies the autoconnect
            # time by 2 on every failure, etc.)
            autoconnect_multiplier = self.serverdata.get('autoconnect_multiplier', 2)
            autoconnect_max = self.serverdata.get('autoconnect_max', 1800)
            # These values must at least be 1.
            autoconnect_multiplier = max(autoconnect_multiplier, 1)
            autoconnect_max = max(autoconnect_max, 1)

            log.debug('(%s) Autoconnect delay set to %s seconds.', self.name, autoconnect)
            if autoconnect is not None and autoconnect >= 1:
                log.debug('(%s) Multiplying autoconnect delay %s by %s.', self.name, autoconnect, self.autoconnect_active_multiplier)
                autoconnect *= self.autoconnect_active_multiplier
                # Add a cap on the max. autoconnect delay, so that we don't go on forever...
                autoconnect = min(autoconnect, autoconnect_max)

                log.info('(%s) Going to auto-reconnect in %s seconds.', self.name, autoconnect)
                # Continue when either self.aborted is set or the autoconnect time passes.
                # Compared to time.sleep(), this allows us to stop connections quicker if we
                # break while while for autoconnect.
                self.aborted.clear()
                self.aborted.wait(autoconnect)

                # Store in the local state what the autoconnect multiplier currently is.
                self.autoconnect_active_multiplier *= autoconnect_multiplier

                if self not in world.networkobjects.values():
                    log.debug('Stopping stale connect loop for old connection %r', self.name)
                    return

            else:
                log.info('(%s) Stopping connect loop (autoconnect value %r is < 1).', self.name, autoconnect)
                return

    def disconnect(self):
        """"""Handle disconnects from the remote server.""""""
        was_successful = self.connected.is_set()
        log.debug('(%s) disconnect: got %s for was_successful state', self.name, was_successful)

        log.debug('(%s) disconnect: Clearing self.connected state.', self.name)
        self.connected.clear()

        log.debug('(%s) Removing channel logging handlers due to disconnect.', self.name)
        while self.loghandlers:
            log.removeHandler(self.loghandlers.pop())

        try:
            log.debug('(%s) disconnect: Shutting down socket.', self.name)
            self.socket.shutdown(socket.SHUT_RDWR)
        except:  # Socket timed out during creation; ignore
            pass

        self.socket.close()

        if self.pingTimer:
            log.debug('(%s) Canceling pingTimer at %s due to disconnect() call', self.name, time.time())
            self.pingTimer.cancel()

        log.debug('(%s) disconnect: Setting self.aborted to True.', self.name)
        self.aborted.set()

        # Internal hook signifying that a network has disconnected.
        self.callHooks([None, 'PYLINK_DISCONNECT', {'was_successful': was_successful}])

        log.debug('(%s) disconnect: Clearing state via initVars().', self.name)
        self.initVars()

    def run(self):
        """"""Main IRC loop which listens for messages.""""""
        # Some magic below cause this to work, though anything that's
        # not encoded in UTF-8 doesn't work very well.
        buf = b""""
        data = b""""
        while not self.aborted.is_set():

            try:
                data = self.socket.recv(2048)
            except OSError:
                # Suppress socket read warnings from lingering recv() calls if
                # we've been told to shutdown.
                if self.aborted.is_set():
                    return
                raise

            buf += data
            if not data:
                log.error('(%s) No data received, disconnecting!', self.name)
                return
            elif (time.time() - self.lastping) > self.pingtimeout:
                log.error('(%s) Connection timed out.', self.name)
                return
            while b'\n' in buf:
                line, buf = buf.split(b'\n', 1)
                line = line.strip(b'\r')
                # FIXME: respect other encodings?
                line = line.decode(""utf-8"", ""replace"")
                self.runline(line)

    def runline(self, line):
        """"""Sends a command to the protocol module.""""""
        log.debug(""(%s) <- %s"", self.name, line)
        try:
            hook_args = self.proto.handle_events(line)
        except Exception:
            log.exception('(%s) Caught error in handle_events, disconnecting!', self.name)
            log.error('(%s) The offending line was: <- %s', self.name, line)
            self.aborted.set()
            return
        # Only call our hooks if there's data to process. Handlers that support
        # hooks will return a dict of parsed arguments, which can be passed on
        # to plugins and the like. For example, the JOIN handler will return
        # something like: {'channel': '#whatever', 'users': ['UID1', 'UID2',
        # 'UID3']}, etc.
        if hook_args is not None:
            self.callHooks(hook_args)

        return hook_args

    def callHooks(self, hook_args):
        """"""Calls a hook function with the given hook args.""""""
        numeric, command, parsed_args = hook_args
        # Always make sure TS is sent.
        if 'ts' not in parsed_args:
            parsed_args['ts'] = int(time.time())
        hook_cmd = command
        hook_map = self.proto.hook_map

        # If the hook name is present in the protocol module's hook_map, then we
        # should set the hook name to the name that points to instead.
        # For example, plugins will read SETHOST as CHGHOST, EOS (end of sync)
        # as ENDBURST, etc.
        if command in hook_map:
            hook_cmd = hook_map[command]

        # However, individual handlers can also return a 'parse_as' key to send
        # their payload to a different hook. An example of this is ""/join 0""
        # being interpreted as leaving all channels (PART).
        hook_cmd = parsed_args.get('parse_as') or hook_cmd

        log.debug('(%s) Raw hook data: [%r, %r, %r] received from %s handler '
                  '(calling hook %s)', self.name, numeric, hook_cmd, parsed_args,
                  command, hook_cmd)

        # Iterate over registered hook functions, catching errors accordingly.
        for hook_func in world.hooks[hook_cmd]:
            try:
                log.debug('(%s) Calling hook function %s from plugin ""%s""', self.name,
                          hook_func, hook_func.__module__)
                hook_func(self, numeric, command, parsed_args)
            except Exception:
                # We don't want plugins to crash our servers...
                log.exception('(%s) Unhandled exception caught in hook %r from plugin ""%s""',
                              self.name, hook_func, hook_func.__module__)
                log.error('(%s) The offending hook data was: %s', self.name,
                          hook_args)
                continue

    def _send(self, data):
        """"""Sends raw text to the uplink server.""""""
        # Safeguard against newlines in input!! Otherwise, each line gets
        # treated as a separate command, which is particularly nasty.
        data = data.replace('\n', ' ')
        data = data.encode(""utf-8"") + b""\n""
        stripped_data = data.decode(""utf-8"").strip(""\n"")
        log.debug(""(%s) -> %s"", self.name, stripped_data)

        try:
            self.socket.send(data)
        except (OSError, AttributeError):
            log.debug(""(%s) Dropping message %r; network isn't connected!"", self.name, stripped_data)

    def send(self, data, queue=True):
        """"""send() wrapper with optional queueing support.""""""
        if queue:
            self.queue.append(data)
        else:
            self._send(data)

    def schedulePing(self):
        """"""Schedules periodic pings in a loop.""""""
        self.proto.ping()

        self.pingTimer = threading.Timer(self.pingfreq, self.schedulePing)
        self.pingTimer.daemon = True
        self.pingTimer.name = 'Ping timer loop for %s' % self.name
        self.pingTimer.start()

        log.debug('(%s) Ping scheduled at %s', self.name, time.time())

    def __repr__(self):
        return ""<classes.Irc object for %r>"" % self.name

    ### General utility functions
    def callCommand(self, source, text):
        """"""
        Calls a PyLink bot command. source is the caller's UID, and text is the
        full, unparsed text of the message.
        """"""
        world.services['pylink'].call_cmd(self, source, text)

    def msg(self, target, text, notice=None, source=None, loopback=True):
        """"""Handy function to send messages/notices to clients. Source
        is optional, and defaults to the main PyLink client if not specified.""""""
        if not text:
            return

        if not (source or self.pseudoclient):
            # No explicit source set and our main client wasn't available; abort.
            return
        source = source or self.pseudoclient.uid

        if notice:
            self.proto.notice(source, target, text)
            cmd = 'PYLINK_SELF_NOTICE'
        else:
            self.proto.message(source, target, text)
            cmd = 'PYLINK_SELF_PRIVMSG'

        if loopback:
            # Determines whether we should send a hook for this msg(), to relay things like services
            # replies across relay.
            self.callHooks([source, cmd, {'target': target, 'text': text}])

    def _reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,
            loopback=True):
        """"""
        Core of the reply() function - replies to the last caller in the right context
        (channel or PM).
        """"""
        if private is None:
            # Allow using private replies as the default, if no explicit setting was given.
            private = conf.conf['bot'].get(""prefer_private_replies"")

        # Private reply is enabled, or the caller was originally a PM
        if private or (self.called_in in self.users):
            if not force_privmsg_in_private:
                # For private replies, the default is to override the notice=True/False argument,
                # and send replies as notices regardless. This is standard behaviour for most
                # IRC services, but can be disabled if force_privmsg_in_private is given.
                notice = True
            target = self.called_by
        else:
            target = self.called_in

        self.msg(target, text, notice=notice, source=source, loopback=loopback)

    def reply(self, *args, **kwargs):
        """"""
        Replies to the last caller in the right context (channel or PM).

        This function wraps around _reply() and can be monkey-patched in a thread-safe manner
        to temporarily redirect plugin output to another target.
        """"""
        with self.reply_lock:
            self._reply(*args, **kwargs)

    def error(self, text, **kwargs):
        """"""Replies with an error to the last caller in the right context (channel or PM).""""""
        # This is a stub to alias error to reply
        self.reply(""Error: %s"" % text, **kwargs)

    def toLower(self, text):
        """"""Returns a lowercase representation of text based on the IRC object's
        casemapping (rfc1459 or ascii).""""""
        if self.proto.casemapping == 'rfc1459':
            text = text.replace('{', '[')
            text = text.replace('}', ']')
            text = text.replace('|', '\\')
            text = text.replace('~', '^')
        # Encode the text as bytes first, and then lowercase it so that only ASCII characters are
        # changed. Unicode in channel names, etc. is case sensitive because IRC is just that old of
        # a protocol!!!
        return text.encode().lower().decode()

    def parseModes(self, target, args):
        """"""Parses a modestring list into a list of (mode, argument) tuples.
        ['+mitl-o', '3', 'person'] => [('+m', None), ('+i', None), ('+t', None), ('+l', '3'), ('-o', 'person')]
        """"""
        # http://www.irc.org/tech_docs/005.html
        # A = Mode that adds or removes a nick or address to a list. Always has a parameter.
        # B = Mode that changes a setting and always has a parameter.
        # C = Mode that changes a setting and only has a parameter when set.
        # D = Mode that changes a setting and never has a parameter.

        if type(args) == str:
            # If the modestring was given as a string, split it into a list.
            args = args.split()

        assert args, 'No valid modes were supplied!'
        usermodes = not utils.isChannel(target)
        prefix = ''
        modestring = args[0]
        args = args[1:]
        if usermodes:
            log.debug('(%s) Using self.umodes for this query: %s', self.name, self.umodes)

            if target not in self.users:
                log.debug('(%s) Possible desync! Mode target %s is not in the users index.', self.name, target)
                return []  # Return an empty mode list

            supported_modes = self.umodes
            oldmodes = self.users[target].modes
        else:
            log.debug('(%s) Using self.cmodes for this query: %s', self.name, self.cmodes)

            supported_modes = self.cmodes
            oldmodes = self.channels[target].modes
        res = []
        for mode in modestring:
            if mode in '+-':
                prefix = mode
            else:
                if not prefix:
                    prefix = '+'
                arg = None
                log.debug('Current mode: %s%s; args left: %s', prefix, mode, args)
                try:
                    if mode in self.prefixmodes and not usermodes:
                        # We're setting a prefix mode on someone (e.g. +o user1)
                        log.debug('Mode %s: This mode is a prefix mode.', mode)
                        arg = args.pop(0)
                        # Convert nicks to UIDs implicitly; most IRCds will want
                        # this already.
                        arg = self.nickToUid(arg) or arg
                        if arg not in self.users:  # Target doesn't exist, skip it.
                            log.debug('(%s) Skipping setting mode ""%s %s""; the '
                                      'target doesn\'t seem to exist!', self.name,
                                      mode, arg)
                            continue
                    elif mode in (supported_modes['*A'] + supported_modes['*B']):
                        # Must have parameter.
                        log.debug('Mode %s: This mode must have parameter.', mode)
                        arg = args.pop(0)
                        if prefix == '-':
                            if mode in supported_modes['*B'] and arg == '*':
                                # Charybdis allows unsetting +k without actually
                                # knowing the key by faking the argument when unsetting
                                # as a single ""*"".
                                # We'd need to know the real argument of +k for us to
                                # be able to unset the mode.
                                oldarg = dict(oldmodes).get(mode)
                                if oldarg:
                                    # Set the arg to the old one on the channel.
                                    arg = oldarg
                                    log.debug(""Mode %s: coersing argument of '*' to %r."", mode, arg)

                            log.debug('(%s) parseModes: checking if +%s %s is in old modes list: %s', self.name, mode, arg, oldmodes)

                            if (mode, arg) not in oldmodes:
                                # Ignore attempts to unset bans that don't exist.
                                log.debug(""(%s) parseModes(): ignoring removal of non-existent list mode +%s %s"", self.name, mode, arg)
                                continue

                    elif prefix == '+' and mode in supported_modes['*C']:
                        # Only has parameter when setting.
                        log.debug('Mode %s: Only has parameter when setting.', mode)
                        arg = args.pop(0)
                except IndexError:
                    log.warning('(%s/%s) Error while parsing mode %r: mode requires an '
                                'argument but none was found. (modestring: %r)',
                                self.name, target, mode, modestring)
                    continue  # Skip this mode; don't error out completely.
                res.append((prefix + mode, arg))
        return res

    def applyModes(self, target, changedmodes):
        """"""Takes a list of parsed IRC modes, and applies them on the given target.

        The target can be either a channel or a user; this is handled automatically.""""""
        usermodes = not utils.isChannel(target)
        log.debug('(%s) Using usermodes for this query? %s', self.name, usermodes)

        try:
            if usermodes:
                old_modelist = self.users[target].modes
                supported_modes = self.umodes
            else:
                old_modelist = self.channels[target].modes
                supported_modes = self.cmodes
        except KeyError:
            log.warning('(%s) Possible desync? Mode target %s is unknown.', self.name, target)
            return

        modelist = set(old_modelist)
        log.debug('(%s) Applying modes %r on %s (initial modelist: %s)', self.name, changedmodes, target, modelist)
        for mode in changedmodes:
            # Chop off the +/- part that parseModes gives; it's meaningless for a mode list.
            try:
                real_mode = (mode[0][1], mode[1])
            except IndexError:
                real_mode = mode

            if not usermodes:
                # We only handle +qaohv for now. Iterate over every supported mode:
                # if the IRCd supports this mode and it is the one being set, add/remove
                # the person from the corresponding prefix mode list (e.g. c.prefixmodes['op']
                # for ops).
                for pmode, pmodelist in self.channels[target].prefixmodes.items():
                    if pmode in self.cmodes and real_mode[0] == self.cmodes[pmode]:
                        log.debug('(%s) Initial prefixmodes list: %s', self.name, pmodelist)
                        if mode[0][0] == '+':
                            pmodelist.add(mode[1])
                        else:
                            pmodelist.discard(mode[1])

                        log.debug('(%s) Final prefixmodes list: %s', self.name, pmodelist)

                if real_mode[0] in self.prefixmodes:
                    # Don't add prefix modes to IrcChannel.modes; they belong in the
                    # prefixmodes mapping handled above.
                    log.debug('(%s) Not adding mode %s to IrcChannel.modes because '
                              'it\'s a prefix mode.', self.name, str(mode))
                    continue

            if mode[0][0] != '-':
                # We're adding a mode
                existing = [m for m in modelist if m[0] == real_mode[0] and m[1] != real_mode[1]]
                if existing and real_mode[1] and real_mode[0] not in self.cmodes['*A']:
                    # The mode we're setting takes a parameter, but is not a list mode (like +beI).
                    # Therefore, only one version of it can exist at a time, and we must remove
                    # any old modepairs using the same letter. Otherwise, we'll get duplicates when,
                    # for example, someone sets mode ""+l 30"" on a channel already set ""+l 25"".
                    log.debug('(%s) Old modes for mode %r exist on %s, removing them: %s',
                              self.name, real_mode, target, str(existing))
                    [modelist.discard(oldmode) for oldmode in existing]
                modelist.add(real_mode)
                log.debug('(%s) Adding mode %r on %s', self.name, real_mode, target)
            else:
                log.debug('(%s) Removing mode %r on %s', self.name, real_mode, target)
                # We're removing a mode
                if real_mode[1] is None:
                    # We're removing a mode that only takes arguments when setting.
                    # Remove all mode entries that use the same letter as the one
                    # we're unsetting.
                    for oldmode in modelist.copy():
                        if oldmode[0] == real_mode[0]:
                            modelist.discard(oldmode)
                else:
                    modelist.discard(real_mode)
        log.debug('(%s) Final modelist: %s', self.name, modelist)
        try:
            if usermodes:
                self.users[target].modes = modelist
            else:
                self.channels[target].modes = modelist
        except KeyError:
            log.warning(""(%s) Invalid MODE target %s (usermodes=%s)"", self.name, target, usermodes)

    @staticmethod
    def _flip(mode):
        """"""Flips a mode character.""""""
        # Make it a list first, strings don't support item assignment
        mode = list(mode)
        if mode[0] == '-':  # Query is something like ""-n""
            mode[0] = '+'  # Change it to ""+n""
        elif mode[0] == '+':
            mode[0] = '-'
        else:  # No prefix given, assume +
            mode.insert(0, '-')
        return ''.join(mode)

    def reverseModes(self, target, modes, oldobj=None):
        """"""Reverses/Inverts the mode string or mode list given.

        Optionally, an oldobj argument can be given to look at an earlier state of
        a channel/user object, e.g. for checking the op status of a mode setter
        before their modes are processed and added to the channel state.

        This function allows both mode strings or mode lists. Example uses:
            ""+mi-lk test => ""-mi+lk test""
            ""mi-k test => ""-mi+k test""
            [('+m', None), ('+r', None), ('+l', '3'), ('-o', 'person')
             => {('-m', None), ('-r', None), ('-l', None), ('+o', 'person')})
            {('s', None), ('+o', 'whoever') => {('-s', None), ('-o', 'whoever')})
        """"""
        origtype = type(modes)
        # If the query is a string, we have to parse it first.
        if origtype == str:
            modes = self.parseModes(target, modes.split("" ""))
        # Get the current mode list first.
        if utils.isChannel(target):
            c = oldobj or self.channels[target]
            oldmodes = c.modes.copy()
            possible_modes = self.cmodes.copy()
            # For channels, this also includes the list of prefix modes.
            possible_modes['*A'] += ''.join(self.prefixmodes)
            for name, userlist in c.prefixmodes.items():
                try:
                    oldmodes.update([(self.cmodes[name], u) for u in userlist])
                except KeyError:
                    continue
        else:
            oldmodes = self.users[target].modes
            possible_modes = self.umodes
        newmodes = []
        log.debug('(%s) reverseModes: old/current mode list for %s is: %s', self.name,
                   target, oldmodes)
        for char, arg in modes:
            # Mode types:
            # A = Mode that adds or removes a nick or address to a list. Always has a parameter.
            # B = Mode that changes a setting and always has a parameter.
            # C = Mode that changes a setting and only has a parameter when set.
            # D = Mode that changes a setting and never has a parameter.
            mchar = char[-1]
            if mchar in possible_modes['*B'] + possible_modes['*C']:
                # We need to find the current mode list, so we can reset arguments
                # for modes that have arguments. For example, setting +l 30 on a channel
                # that had +l 50 set should give ""+l 30"", not ""-l"".
                oldarg = [m for m in oldmodes if m[0] == mchar]
                if oldarg:  # Old mode argument for this mode existed, use that.
                    oldarg = oldarg[0]
                    mpair = ('+%s' % oldarg[0], oldarg[1])
                else:  # Not found, flip the mode then.
                    # Mode takes no arguments when unsetting.
                    if mchar in possible_modes['*C'] and char[0] != '-':
                        arg = None
                    mpair = (self._flip(char), arg)
            else:
                mpair = (self._flip(char), arg)
            if char[0] != '-' and (mchar, arg) in oldmodes:
                # Mode is already set.
                log.debug(""(%s) reverseModes: skipping reversing '%s %s' with %s since we're ""
                          ""setting a mode that's already set."", self.name, char, arg, mpair)
                continue
            elif char[0] == '-' and (mchar, arg) not in oldmodes and mchar in possible_modes['*A']:
                # We're unsetting a prefixmode that was never set - don't set it in response!
                # Charybdis lacks verification for this server-side.
                log.debug(""(%s) reverseModes: skipping reversing '%s %s' with %s since it ""
                          ""wasn't previously set."", self.name, char, arg, mpair)
                continue
            newmodes.append(mpair)

        log.debug('(%s) reverseModes: new modes: %s', self.name, newmodes)
        if origtype == str:
            # If the original query is a string, send it back as a string.
            return self.joinModes(newmodes)
        else:
            return set(newmodes)

    @staticmethod
    def joinModes(modes, sort=False):
        """"""Takes a list of (mode, arg) tuples in parseModes() format, and
        joins them into a string.

        See testJoinModes in tests/test_utils.py for some examples.""""""
        prefix = '+'  # Assume we're adding modes unless told otherwise
        modelist = ''
        args = []

        # Sort modes alphabetically like a conventional IRCd.
        if sort:
            modes = sorted(modes)

        for modepair in modes:
            mode, arg = modepair
            assert len(mode) in (1, 2), ""Incorrect length of a mode (received %r)"" % mode
            try:
                # If the mode has a prefix, use that.
                curr_prefix, mode = mode
            except ValueError:
                # If not, the current prefix stays the same; move on to the next
                # modepair.
                pass
            else:
                # If the prefix of this mode isn't the same as the last one, add
                # the prefix to the modestring. This prevents '+nt-lk' from turning
                # into '+n+t-l-k' or '+ntlk'.
                if prefix != curr_prefix:
                    modelist += curr_prefix
                    prefix = curr_prefix
            modelist += mode
            if arg is not None:
                args.append(arg)
        if not modelist.startswith(('+', '-')):
            # Our starting mode didn't have a prefix with it. Assume '+'.
            modelist = '+' + modelist
        if args:
            # Add the args if there are any.
            modelist += ' %s' % ' '.join(args)
        return modelist

    @classmethod
    def wrapModes(cls, modes, limit, max_modes_per_msg=0):
        """"""
        Takes a list of modes and wraps it across multiple lines.
        """"""
        strings = []

        # This process is slightly trickier than just wrapping arguments, because modes create
        # positional arguments that can't be separated from its character.
        queued_modes = []
        total_length = 0

        last_prefix = '+'
        orig_modes = modes.copy()
        modes = list(modes)
        while modes:
            # PyLink mode lists come in the form [('+t', None), ('-b', '*!*@someone'), ('+l', 3)]
            # The +/- part is optional depending on context, and should either:
            # 1) The prefix of the last mode.
            # 2) + (adding modes), if no prefix was ever given
            next_mode = modes.pop(0)

            modechar, arg = next_mode
            prefix = modechar[0]
            if prefix not in '+-':
                prefix = last_prefix
                # Explicitly add the prefix to the mode character to prevent
                # ambiguity when passing it to joinModes().
                modechar = prefix + modechar
                # XXX: because tuples are immutable, we have to replace the entire modepair..
                next_mode = (modechar, arg)

            # Figure out the length that the next mode will add to the buffer. If we're changing
            # from + to - (setting to removing modes) or vice versa, we'll need two characters
            # (""+"" or ""-"") plus the mode char itself.
            next_length = 1
            if prefix != last_prefix:
                next_length += 1

            # Replace the last_prefix with the current one for the next iteration.
            last_prefix = prefix

            if arg:
                # This mode has an argument, so add the length of that and a space.
                next_length += 1
                next_length += len(arg)

            assert next_length <= limit, \
                ""wrapModes: Mode %s is too long for the given length %s"" % (next_mode, limit)

            # Check both message length and max. modes per msg if enabled.
            if (next_length + total_length) <= limit and ((not max_modes_per_msg) or len(queued_modes) < max_modes_per_msg):
                # We can fit this mode in the next message; add it.
                total_length += next_length
                log.debug('wrapModes: Adding mode %s to queued modes', str(next_mode))
                queued_modes.append(next_mode)
                log.debug('wrapModes: queued modes: %s', queued_modes)
            else:
                # Otherwise, create a new message by joining the previous queue.
                # Then, add our current mode.
                strings.append(cls.joinModes(queued_modes))
                queued_modes.clear()

                log.debug('wrapModes: cleared queue (length %s) and now adding %s', limit, str(next_mode))
                queued_modes.append(next_mode)
                total_length = next_length
        else:
            # Everything fit in one line, so just use that.
            strings.append(cls.joinModes(queued_modes))

        log.debug('wrapModes: returning %s for %s', strings, orig_modes)
        return strings

    def version(self):
        """"""
        Returns a detailed version string including the PyLink daemon version,
        the protocol module in use, and the server hostname.
        """"""
        fullversion = 'PyLink-%s. %s :[protocol:%s]' % (__version__, self.hostname(), self.protoname)
        return fullversion

    def hostname(self):
        """"""
        Returns the server hostname used by PyLink on the given server.
        """"""
        return self.serverdata.get('hostname', world.fallback_hostname)

    ### State checking functions
    def nickToUid(self, nick):
        """"""Looks up the UID of a user with the given nick, if one is present.""""""
        nick = self.toLower(nick)
        for k, v in self.users.copy().items():
            if self.toLower(v.nick) == nick:
                return k

    def isInternalClient(self, numeric):
        """"""
        Returns whether the given client numeric (UID) is a PyLink client.
        """"""
        sid = self.getServer(numeric)
        if sid and self.servers[sid].internal:
            return True
        return False

    def isInternalServer(self, sid):
        """"""Returns whether the given SID is an internal PyLink server.""""""
        return (sid in self.servers and self.servers[sid].internal)

    def getServer(self, numeric):
        """"""Finds the SID of the server a user is on.""""""
        userobj = self.users.get(numeric)
        if userobj:
            return userobj.server

    def isManipulatableClient(self, uid):
        """"""
        Returns whether the given user is marked as an internal, manipulatable
        client. Usually, automatically spawned services clients should have this
        set True to prevent interactions with opers (like mode changes) from
        causing desyncs.
        """"""
        return self.isInternalClient(uid) and self.users[uid].manipulatable

    def getServiceBot(self, uid):
        """"""
        Checks whether the given UID is a registered service bot. If True,
        returns the cooresponding ServiceBot object.
        """"""
        userobj = self.users.get(uid)
        if not userobj:
            return False

        # Look for the ""service"" attribute in the IrcUser object, if one exists.
        try:
            sname = userobj.service
            # Warn if the service name we fetched isn't a registered service.
            if sname not in world.services.keys():
                log.warning(""(%s) User %s / %s had a service bot record to a service that doesn't ""
                            ""exist (%s)!"", self.name, uid, userobj.nick, sname)
            return world.services.get(sname)
        except AttributeError:
            return False

    def getHostmask(self, user, realhost=False, ip=False):
        """"""
        Returns the hostmask of the given user, if present. If the realhost option
        is given, return the real host of the user instead of the displayed host.
        If the ip option is given, return the IP address of the user (this overrides
        realhost).""""""
        userobj = self.users.get(user)

        try:
            nick = userobj.nick
        except AttributeError:
            nick = '<unknown-nick>'

        try:
            ident = userobj.ident
        except AttributeError:
            ident = '<unknown-ident>'

        try:
            if ip:
                host = userobj.ip
            elif realhost:
                host = userobj.realhost
            else:
                host = userobj.host
        except AttributeError:
            host = '<unknown-host>'

        return '%s!%s@%s' % (nick, ident, host)

    def getFriendlyName(self, entityid):
        """"""
        Returns the friendly name of a SID or UID (server name for SIDs, nick for UID).
        """"""
        if entityid in self.servers:
            return self.servers[entityid].name
        elif entityid in self.users:
            return self.users[entityid].nick
        else:
            raise KeyError(""Unknown UID/SID %s"" % entityid)

    def getFullNetworkName(self):
        """"""
        Returns the full network name (as defined by the ""netname"" option), or the
        short network name if that isn't defined.
        """"""
        return self.serverdata.get('netname', self.name)

    def isOper(self, uid, allowAuthed=True, allowOper=True):
        """"""
        Returns whether the given user has operator status on PyLink. This can be achieved
        by either identifying to PyLink as admin (if allowAuthed is True),
        or having user mode +o set (if allowOper is True). At least one of
        allowAuthed or allowOper must be True for this to give any meaningful
        results.
        """"""
        if uid in self.users:
            if allowOper and (""o"", None) in self.users[uid].modes:
                return True
            elif allowAuthed and self.users[uid].account:
                return True
        return False

    def checkAuthenticated(self, uid, allowAuthed=True, allowOper=True):
        """"""
        Checks whether the given user has operator status on PyLink, raising
        NotAuthorizedError and logging the access denial if not.
        """"""
        log.warning(""(%s) Irc.checkAuthenticated() is deprecated as of PyLink 1.2 and may be ""
                    ""removed in a future relase. Consider migrating to the PyLink Permissions API."",
                    self.name)
        lastfunc = inspect.stack()[1][3]
        if not self.isOper(uid, allowAuthed=allowAuthed, allowOper=allowOper):
            log.warning('(%s) Access denied for %s calling %r', self.name,
                        self.getHostmask(uid), lastfunc)
            raise utils.NotAuthorizedError(""You are not authenticated!"")
        return True

    def matchHost(self, glob, target, ip=True, realhost=True):
        """"""
        Checks whether the given host, or given UID's hostmask matches the given nick!user@host
        glob.

        If the target given is a UID, and the 'ip' or 'realhost' options are True, this will also
        match against the target's IP address and real host, respectively.

        This function respects IRC casemappings (rfc1459 and ascii). If the given target is a UID,
        and the 'ip' option is enabled, the host portion of the glob is also matched as a CIDR
        range.
        """"""
        # Get the corresponding casemapping value used by ircmatch.
        if self.proto.casemapping == 'rfc1459':
            casemapping = 0
        else:
            casemapping = 1

        # Try to convert target into a UID. If this fails, it's probably a hostname.
        target = self.nickToUid(target) or target

        # Prepare a list of hosts to check against.
        if target in self.users:
            if glob.startswith(('$', '!$')):
                # !$exttarget inverts the given match.
                invert = glob.startswith('!$')

                # Exttargets start with $. Skip regular ban matching and find the matching ban handler.
                glob = glob.lstrip('$!')
                exttargetname = glob.split(':', 1)[0]
                handler = world.exttarget_handlers.get(exttargetname)

                if handler:
                    # Handler exists. Return what it finds.
                    result = handler(self, glob, target)
                    log.debug('(%s) Got %s from exttarget %s in matchHost() glob $%s for target %s',
                              self.name, result, exttargetname, glob, target)
                    if invert:  # Anti-exttarget was specified.
                        result = not result
                    return result
                else:
                    log.debug('(%s) Unknown exttarget %s in matchHost() glob $%s', self.name,
                              exttargetname, glob)
                    return False

            hosts = {self.getHostmask(target)}

            if ip:
                hosts.add(self.getHostmask(target, ip=True))

                # HACK: support CIDR hosts in the hosts portion
                try:
                    header, cidrtarget = glob.split('@', 1)
                    log.debug('(%s) Processing CIDRs for %s (full host: %s)', self.name,
                              cidrtarget, glob)
                    # Try to parse the host portion as a CIDR range
                    network = ipaddress.ip_network(cidrtarget)

                    log.debug('(%s) Found CIDR for %s, replacing target host with IP %s', self.name,
                              realhost, target)
                    real_ip = self.users[target].ip
                    if ipaddress.ip_address(real_ip) in network:
                        # If the CIDR matches, hack around the host matcher by pretending that
                        # the lookup target was the IP and not the CIDR range!
                        glob = '@'.join((header, real_ip))
                except ValueError:
                    pass

            if realhost:
                hosts.add(self.getHostmask(target, realhost=True))

        else:  # We were given a host, use that.
            hosts = [target]

        # Iterate over the hosts to match using ircmatch.
        for host in hosts:
            if ircmatch.match(casemapping, glob, host):
                return True

        return False

class IrcUser():
    """"""PyLink IRC user class.""""""
    def __init__(self, nick, ts, uid, server, ident='null', host='null',
                 realname='PyLink dummy client', realhost='null',
                 ip='0.0.0.0', manipulatable=False, opertype='IRC Operator'):
        self.nick = nick
        self.ts = ts
        self.uid = uid
        self.ident = ident
        self.host = host
        self.realhost = realhost
        self.ip = ip
        self.realname = realname
        self.modes = set()  # Tracks user modes
        self.server = server

        # Tracks PyLink identification status
        self.account = ''

        # Tracks oper type (for display only)
        self.opertype = opertype

        # Tracks external services identification status
        self.services_account = ''

        # Tracks channels the user is in
        self.channels = set()

        # Tracks away message status
        self.away = ''

        # This sets whether the client should be marked as manipulatable.
        # Plugins like bots.py's commands should take caution against
        # manipulating these ""protected"" clients, to prevent desyncs and such.
        # For ""serious"" service clients, this should always be False.
        self.manipulatable = manipulatable

    def __repr__(self):
        return 'IrcUser(%s/%s)' % (self.uid, self.nick)

class IrcServer():
    """"""PyLink IRC server class.

    uplink: The SID of this IrcServer instance's uplink. This is set to None
            for the main PyLink PseudoServer!
    name: The name of the server.
    internal: Whether the server is an internal PyLink PseudoServer.
    """"""

    def __init__(self, uplink, name, internal=False, desc=""(None given)""):
        self.uplink = uplink
        self.users = set()
        self.internal = internal
        self.name = name.lower()
        self.desc = desc

    def __repr__(self):
        return 'IrcServer(%s)' % self.name

class IrcChannel():
    """"""PyLink IRC channel class.""""""
    def __init__(self, name=None):
        # Initialize variables, such as the topic, user list, TS, who's opped, etc.
        self.users = set()
        self.modes = set()
        self.topic = ''
        self.ts = int(time.time())
        self.prefixmodes = {'op': set(), 'halfop': set(), 'voice': set(),
                            'owner': set(), 'admin': set()}

        # Determines whether a topic has been set here or not. Protocol modules
        # should set this.
        self.topicset = False

        # Saves the channel name (may be useful to plugins, etc.)
        self.name = name

    def __repr__(self):
        return 'IrcChannel(%s)' % self.name

    def removeuser(self, target):
        """"""Removes a user from a channel.""""""
        for s in self.prefixmodes.values():
            s.discard(target)
        self.users.discard(target)

    def deepcopy(self):
        """"""Returns a deep copy of the channel object.""""""
        return deepcopy(self)

    def isVoice(self, uid):
        """"""Returns whether the given user is voice in the channel.""""""
        return uid in self.prefixmodes['voice']

    def isHalfop(self, uid):
        """"""Returns whether the given user is halfop in the channel.""""""
        return uid in self.prefixmodes['halfop']

    def isOp(self, uid):
        """"""Returns whether the given user is op in the channel.""""""
        return uid in self.prefixmodes['op']

    def isAdmin(self, uid):
        """"""Returns whether the given user is admin (&) in the channel.""""""
        return uid in self.prefixmodes['admin']

    def isOwner(self, uid):
        """"""Returns whether the given user is owner (~) in the channel.""""""
        return uid in self.prefixmodes['owner']

    def isVoicePlus(self, uid):
        """"""Returns whether the given user is voice or above in the channel.""""""
        # If the user has any prefix mode, it has to be voice or greater.
        return bool(self.getPrefixModes(uid))

    def isHalfopPlus(self, uid):
        """"""Returns whether the given user is halfop or above in the channel.""""""
        for mode in ('halfop', 'op', 'admin', 'owner'):
            if uid in self.prefixmodes[mode]:
                return True
        return False

    def isOpPlus(self, uid):
        """"""Returns whether the given user is op or above in the channel.""""""
        for mode in ('op', 'admin', 'owner'):
            if uid in self.prefixmodes[mode]:
                return True
        return False

    @staticmethod
    def sortPrefixes(key):
        """"""
        Implements a sorted()-compatible sorter for prefix modes, giving each one a
        numeric value.
        """"""
        values = {'owner': 100, 'admin': 10, 'op': 5, 'halfop': 4, 'voice': 3}

        # Default to highest value (1000) for unknown modes, should we choose to
        # support them.
        return values.get(key, 1000)

    def getPrefixModes(self, uid, prefixmodes=None):
        """"""Returns a list of all named prefix modes the given user has in the channel.

        Optionally, a prefixmodes argument can be given to look at an earlier state of
        the channel's prefix modes mapping, e.g. for checking the op status of a mode
        setter before their modes are processed and added to the channel state.
        """"""

        if uid not in self.users:
            raise KeyError(""User %s does not exist or is not in the channel"" % uid)

        result = []
        prefixmodes = prefixmodes or self.prefixmodes

        for mode, modelist in prefixmodes.items():
            if uid in modelist:
                result.append(mode)

        return sorted(result, key=self.sortPrefixes)

class Protocol():
    """"""Base Protocol module class for PyLink.""""""
    def __init__(self, irc):
        self.irc = irc
        self.casemapping = 'rfc1459'
        self.hook_map = {}

        # Lock for updateTS to make sure only one thread can change the channel TS at one time.
        self.ts_lock = threading.Lock()

        # Lists required conf keys for the server block.
        self.conf_keys = {'ip', 'port', 'hostname', 'sid', 'sidrange', 'protocol', 'sendpass',
                          'recvpass'}

        # Defines a set of PyLink protocol capabilities
        self.protocol_caps = set()

    def validateServerConf(self):
        """"""Validates that the server block given contains the required keys.""""""
        for k in self.conf_keys:
            assert k in self.irc.serverdata, ""Missing option %r in server block for network %s."" % (k, self.irc.name)

        port = self.irc.serverdata['port']
        assert type(port) == int and 0 < port < 65535, ""Invalid port %r for network %s"" % (port, self.irc.name)

    @staticmethod
    def parseArgs(args):
        """"""
        Parses a string or list of of RFC1459-style arguments, where "":"" may
        be used for multi-word arguments that last until the end of a line.
        """"""
        if isinstance(args, str):
            args = args.split(' ')

        real_args = []
        for idx, arg in enumerate(args):
            if arg.startswith(':') and idx != 0:
                # "":"" is used to begin multi-word arguments that last until the end of the message.
                # Use list splicing here to join them into one argument, and then add it to our list of args.
                joined_arg = ' '.join(args[idx:])[1:]  # Cut off the leading : as well
                real_args.append(joined_arg)
                break
            real_args.append(arg)

        return real_args

    def hasCap(self, capab):
        """"""
        Returns whether this protocol module instance has the requested capability.
        """"""
        return capab.lower() in self.protocol_caps

    def removeClient(self, numeric):
        """"""Internal function to remove a client from our internal state.""""""
        for c, v in self.irc.channels.copy().items():
            v.removeuser(numeric)
            # Clear empty non-permanent channels.
            if not (self.irc.channels[c].users or ((self.irc.cmodes.get('permanent'), None) in self.irc.channels[c].modes)):
                del self.irc.channels[c]
            assert numeric not in v.users, ""IrcChannel's removeuser() is broken!""

        sid = self.irc.getServer(numeric)
        log.debug('Removing client %s from self.irc.users', numeric)
        del self.irc.users[numeric]
        log.debug('Removing client %s from self.irc.servers[%s].users', numeric, sid)
        self.irc.servers[sid].users.discard(numeric)

    def updateTS(self, sender, channel, their_ts, modes=[]):
        """"""
        Merges modes of a channel given the remote TS and a list of modes.
        """"""

        # Okay, so the situation is that we have 6 possible TS/sender combinations:

        #                       | our TS lower | TS equal | their TS lower
        # mode origin is us     |   OVERWRITE  |   MERGE  |    IGNORE
        # mode origin is uplink |    IGNORE    |   MERGE  |   OVERWRITE

        def _clear():
            log.debug(""(%s) Clearing local modes from channel %s due to TS change"", self.irc.name,
                      channel)
            self.irc.channels[channel].modes.clear()
            for p in self.irc.channels[channel].prefixmodes.values():
                for user in p.copy():
                    if not self.irc.isInternalClient(user):
                        p.discard(user)

        def _apply():
            if modes:
                log.debug(""(%s) Applying modes on channel %s (TS ok)"", self.irc.name,
                          channel)
                self.irc.applyModes(channel, modes)

        # Use a lock so only one thread can change a channel's TS at once: this prevents race
        # conditions from desyncing the channel list.
        with self.ts_lock:
            our_ts = self.irc.channels[channel].ts
            assert type(our_ts) == int, ""Wrong type for our_ts (expected int, got %s)"" % type(our_ts)
            assert type(their_ts) == int, ""Wrong type for their_ts (expected int, got %s)"" % type(their_ts)

            # Check if we're the mode sender based on the UID / SID given.
            our_mode = self.irc.isInternalClient(sender) or self.irc.isInternalServer(sender)

            log.debug(""(%s/%s) our_ts: %s; their_ts: %s; is the mode origin us? %s"", self.irc.name,
                      channel, our_ts, their_ts, our_mode)

            if their_ts == our_ts:
                log.debug(""(%s/%s) remote TS of %s is equal to our %s; mode query %s"",
                          self.irc.name, channel, their_ts, our_ts, modes)
                # Their TS is equal to ours. Merge modes.
                _apply()

            elif (their_ts < our_ts):
                if their_ts < 750000:
                    log.warning('(%s) Possible desync? Not setting bogus TS %s on channel %s', self.irc.name, their_ts, channel)
                else:
                    log.debug('(%s) Resetting channel TS of %s from %s to %s (remote has lower TS)',
                              self.irc.name, channel, our_ts, their_ts)
                    self.irc.channels[channel].ts = their_ts

                # Remote TS was lower and we're receiving modes. Clear the modelist and apply theirs.

                _clear()
                _apply()

    def _getSid(self, sname):
        """"""Returns the SID of a server with the given name, if present.""""""
        name = sname.lower()
        for k, v in self.irc.servers.items():
            if v.name.lower() == name:
                return k
        else:
            return sname  # Fall back to given text instead of None

    def _getUid(self, target):
        """"""Converts a nick argument to its matching UID. This differs from irc.nickToUid()
        in that it returns the original text instead of None, if no matching nick is found.""""""
        target = self.irc.nickToUid(target) or target
        return target

    @classmethod
    def parsePrefixedArgs(cls, args):
        """"""Similar to parseArgs(), but stripping leading colons from the first argument
        of a line (usually the sender field).""""""
        args = cls.parseArgs(args)
        args[0] = args[0].split(':', 1)[1]
        return args

    def _squit(self, numeric, command, args):
        """"""Handles incoming SQUITs.""""""

        split_server = self._getSid(args[0])

        # Normally we'd only need to check for our SID as the SQUIT target, but Nefarious
        # actually uses the uplink server as the SQUIT target.
        # <- ABAAE SQ nefarious.midnight.vpn 0 :test
        if split_server in (self.irc.sid, self.irc.uplink):
            raise ProtocolError('SQUIT received: (reason: %s)' % args[-1])

        affected_users = []
        affected_nicks = defaultdict(list)
        log.debug('(%s) Splitting server %s (reason: %s)', self.irc.name, split_server, args[-1])

        if split_server not in self.irc.servers:
            log.warning(""(%s) Tried to split a server (%s) that didn't exist!"", self.irc.name, split_server)
            return

        # Prevent RuntimeError: dictionary changed size during iteration
        old_servers = self.irc.servers.copy()
        old_channels = self.irc.channels.copy()

        # Cycle through our list of servers. If any server's uplink is the one that is being SQUIT,
        # remove them and all their users too.
        for sid, data in old_servers.items():
            if data.uplink == split_server:
                log.debug('Server %s also hosts server %s, removing those users too...', split_server, sid)
                # Recursively run SQUIT on any other hubs this server may have been connected to.
                args = self._squit(sid, 'SQUIT', [sid, ""0"",
                                   ""PyLink: Automatically splitting leaf servers of %s"" % sid])
                affected_users += args['users']

        for user in self.irc.servers[split_server].users.copy():
            affected_users.append(user)
            nick = self.irc.users[user].nick

            # Nicks affected is channel specific for SQUIT:. This makes Clientbot's SQUIT relaying
            # much easier to implement.
            for name, cdata in old_channels.items():
                if user in cdata.users:
                    affected_nicks[name].append(nick)

            log.debug('Removing client %s (%s)', user, nick)
            self.removeClient(user)

        serverdata = self.irc.servers[split_server]
        sname = serverdata.name
        uplink = serverdata.uplink

        del self.irc.servers[split_server]
        log.debug('(%s) Netsplit affected users: %s', self.irc.name, affected_users)

        return {'target': split_server, 'users': affected_users, 'name': sname,
                'uplink': uplink, 'nicks': affected_nicks, 'serverdata': serverdata,
                'channeldata': old_channels}

    @staticmethod
    def parseCapabilities(args, fallback=''):
        """"""
        Parses a string of capabilities in the 005 / RPL_ISUPPORT format.
        """"""

        if type(args) == str:
            args = args.split(' ')

        caps = {}
        for cap in args:
            try:
                # Try to split it as a KEY=VALUE pair.
                key, value = cap.split('=', 1)
            except ValueError:
                key = cap
                value = fallback
            caps[key] = value

        return caps

    @staticmethod
    def parsePrefixes(args):
        """"""
        Separates prefixes field like ""(qaohv)~&@%+"" into a dict mapping mode characters to mode
        prefixes.
        """"""
        prefixsearch = re.search(r'\(([A-Za-z]+)\)(.*)', args)
        return dict(zip(prefixsearch.group(1), prefixsearch.group(2)))

    def handle_error(self, numeric, command, args):
        """"""Handles ERROR messages - these mean that our uplink has disconnected us!""""""
        raise ProtocolError('Received an ERROR, disconnecting!')
/n/n/nplugins/networks.py/n/n""""""Networks plugin - allows you to manipulate connections to various configured networks.""""""
import importlib
import types

from pylinkirc import utils, world, conf, classes
from pylinkirc.log import log
from pylinkirc.coremods import control, permissions

@utils.add_cmd
def disconnect(irc, source, args):
    """"""<network>

    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.

    To reconnect a network disconnected using this command, use REHASH to reload the networks list.""""""
    permissions.checkPermissions(irc, source, ['networks.disconnect'])
    try:
        netname = args[0]
        network = world.networkobjects[netname]
    except IndexError:  # No argument given.
        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    irc.reply(""Done. If you want to reconnect this network, use the 'rehash' command."")

    control.remove_network(network)

@utils.add_cmd
def autoconnect(irc, source, args):
    """"""<network> <seconds>

    Sets the autoconnect time for <network> to <seconds>.
    You can disable autoconnect for a network by setting <seconds> to a negative value.""""""
    permissions.checkPermissions(irc, source, ['networks.autoconnect'])
    try:
        netname = args[0]
        seconds = float(args[1])
        network = world.networkobjects[netname]
    except IndexError:  # Arguments not given.
        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    except ValueError:
        irc.error('Invalid argument ""%s"" for <seconds>.' % seconds)
        return
    network.serverdata['autoconnect'] = seconds
    irc.reply(""Done."")

remote_parser = utils.IRCParser()
remote_parser.add_argument('network')
remote_parser.add_argument('--service', type=str, default='pylink')
remote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)
@utils.add_cmd
def remote(irc, source, args):
    """"""<network> [--service <service name>] <command>

    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are
    supported and returned here, but others are dropped due to protocol limitations.""""""
    permissions.checkPermissions(irc, source, ['networks.remote'])

    args = remote_parser.parse_args(args)
    netname = args.network

    if netname == irc.name:
        # This would actually throw _remote_reply() into a loop, so check for it here...
        # XXX: properly fix this.
        irc.error(""Cannot remote-send a command to the local network; use a normal command!"")
        return

    try:
        remoteirc = world.networkobjects[netname]
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return

    if args.service not in world.services:
        irc.error('Unknown service %r.' % args.service)
        return

    # Force remoteirc.called_in to something private in order to prevent
    # accidental information leakage from replies.
    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid

    # Set the identification override to the caller's account.
    remoteirc.pseudoclient.account = irc.users[source].account

    def _remote_reply(placeholder_self, text, **kwargs):
        """"""
        reply() rerouter for the 'remote' command.
        """"""
        assert irc.name != placeholder_self.name, \
            ""Refusing to route reply back to the same "" \
            ""network, as this would cause a recursive loop""
        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,
                  text, placeholder_self.name)

        # Override the source option to make sure the source is valid on the local network.
        if 'source' in kwargs:
            del kwargs['source']
        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)

    old_reply = remoteirc._reply

    with remoteirc.reply_lock:
        try:  # Remotely call the command (use the PyLink client as a dummy user).
            # Override the remote irc.reply() to send replies HERE.
            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)
            remoteirc._reply = types.MethodType(_remote_reply, remoteirc)
            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,
                                                  ' '.join(args.command))
        finally:
            # Restore the original remoteirc.reply()
            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)
            remoteirc._reply = old_reply
            # Remove the identification override after we finish.
            remoteirc.pseudoclient.account = ''

@utils.add_cmd
def reloadproto(irc, source, args):
    """"""<protocol module name>

    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.""""""
    permissions.checkPermissions(irc, source, ['networks.reloadproto'])
    try:
        name = args[0]
    except IndexError:
        irc.error('Not enough arguments (needs 1: protocol module name)')
        return

    proto = utils.getProtocolModule(name)
    importlib.reload(proto)

    irc.reply(""Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply."" % name)
/n/n/n",0
77,9d9b01839cf3639e59d29c27e70688bdbf44db96,"/plugins/networks.py/n/n""""""Networks plugin - allows you to manipulate connections to various configured networks.""""""
import importlib
import types

from pylinkirc import utils, world, conf, classes
from pylinkirc.log import log
from pylinkirc.coremods import control, permissions

@utils.add_cmd
def disconnect(irc, source, args):
    """"""<network>

    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.

    To reconnect a network disconnected using this command, use REHASH to reload the networks list.""""""
    permissions.checkPermissions(irc, source, ['networks.disconnect'])
    try:
        netname = args[0]
        network = world.networkobjects[netname]
    except IndexError:  # No argument given.
        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    irc.reply(""Done. If you want to reconnect this network, use the 'rehash' command."")

    control.remove_network(network)

@utils.add_cmd
def autoconnect(irc, source, args):
    """"""<network> <seconds>

    Sets the autoconnect time for <network> to <seconds>.
    You can disable autoconnect for a network by setting <seconds> to a negative value.""""""
    permissions.checkPermissions(irc, source, ['networks.autoconnect'])
    try:
        netname = args[0]
        seconds = float(args[1])
        network = world.networkobjects[netname]
    except IndexError:  # Arguments not given.
        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    except ValueError:
        irc.error('Invalid argument ""%s"" for <seconds>.' % seconds)
        return
    network.serverdata['autoconnect'] = seconds
    irc.reply(""Done."")

remote_parser = utils.IRCParser()
remote_parser.add_argument('network')
remote_parser.add_argument('--service', type=str, default='pylink')
remote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)
@utils.add_cmd
def remote(irc, source, args):
    """"""<network> [--service <service name>] <command>

    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are
    supported and returned here, but others are dropped due to protocol limitations.""""""
    permissions.checkPermissions(irc, source, ['networks.remote'])

    args = remote_parser.parse_args(args)
    netname = args.network

    if netname == irc.name:
        # This would actually throw _remote_reply() into a loop, so check for it here...
        # XXX: properly fix this.
        irc.error(""Cannot remote-send a command to the local network; use a normal command!"")
        return

    try:
        remoteirc = world.networkobjects[netname]
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return

    if args.service not in world.services:
        irc.error('Unknown service %r.' % args.service)
        return

    # Force remoteirc.called_in to something private in order to prevent
    # accidental information leakage from replies.
    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid

    # Set the identification override to the caller's account.
    remoteirc.pseudoclient.account = irc.users[source].account

    def _remote_reply(placeholder_self, text, **kwargs):
        """"""
        reply() rerouter for the 'remote' command.
        """"""
        assert irc.name != placeholder_self.name, \
            ""Refusing to route reply back to the same "" \
            ""network, as this would cause a recursive loop""
        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,
                  text, placeholder_self.name)

        # Override the source option to make sure the source is valid on the local network.
        if 'source' in kwargs:
            del kwargs['source']
        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)

    old_reply = remoteirc.reply

    with remoteirc.reply_lock:
        try:  # Remotely call the command (use the PyLink client as a dummy user).
            # Override the remote irc.reply() to send replies HERE.
            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)
            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)
            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,
                                                  ' '.join(args.command))
        finally:
            # Restore the original remoteirc.reply()
            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)
            remoteirc.reply = old_reply
            # Remove the identification override after we finish.
            remoteirc.pseudoclient.account = ''

@utils.add_cmd
def reloadproto(irc, source, args):
    """"""<protocol module name>

    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.""""""
    permissions.checkPermissions(irc, source, ['networks.reloadproto'])
    try:
        name = args[0]
    except IndexError:
        irc.error('Not enough arguments (needs 1: protocol module name)')
        return

    proto = utils.getProtocolModule(name)
    importlib.reload(proto)

    irc.reply(""Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply."" % name)
/n/n/n",1
78,5577269b7283a48d0b421dabd44687460955a0c8,"benchmark/cli.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import argparse  # for command line parsing
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import sys
import shutil
from benchmark import core, config, data_service


def get_cli_arguments():
    """""" Returns command line arguments. 

    Returns:
    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), 
    and config argument for where is the config file. """"""

    logging.debug('Getting cli arguments')

    parser = argparse.ArgumentParser(description=""A benchmark for genomics routines in Python."")

    # Enable three exclusive groups of options (using subparsers)
    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525

    subparser = parser.add_subparsers(title=""commands"", dest=""command"")
    subparser.required = True

    config_parser = subparser.add_parser(""config"",
                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')
    config_parser.add_argument(""--output_config"", type=str, required=True,
                               help=""Specify the output path to a configuration file."", metavar=""FILEPATH"")
    config_parser.add_argument(""-f"", action=""store_true"", help=""Overwrite the destination file if it already exists."")

    data_setup_parser = subparser.add_parser(""setup"",
                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')
    data_setup_parser.add_argument(""--config_file"", required=True, help=""Location of the configuration file"",
                                   metavar=""FILEPATH"")

    benchmark_exec_parser = subparser.add_parser(""exec"",
                                                 help='Execution of the benchmark modes. It requires a configuration file.')

    timestamp_current = datetime.datetime.fromtimestamp(time.time())
    benchmark_label_default = ""run_{timestamp}"".format(timestamp=timestamp_current.strftime(""%Y-%m-%d_%H-%M-%S""))
    benchmark_exec_parser.add_argument(""--label"", type=str, default=benchmark_label_default, metavar=""RUN_LABEL"",
                                       help=""Label for the benchmark run."")
    benchmark_exec_parser.add_argument(""--config_file"", type=str, required=True,
                                       help=""Specify the path to a configuration file."", metavar=""FILEPATH"")

    runtime_configuration = vars(parser.parse_args())
    return runtime_configuration


def _main():
    data_dirs = config.DataDirectoriesConfigurationRepresentation()

    cli_arguments = get_cli_arguments()

    command = cli_arguments[""command""]
    if command == ""config"":
        output_config_location = cli_arguments[""output_config""]
        overwrite_mode = cli_arguments[""f""]
        config.generate_default_config_file(output_location=output_config_location,
                                            overwrite=overwrite_mode)
    elif command == ""setup"":
        print(""[Setup] Setting up benchmark data."")

        # Clear out existing files in VCF and Zarr directories
        data_service.remove_directory_tree(data_dirs.vcf_dir)
        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        # Get FTP module settings from runtime config
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        if ftp_config.enabled:
            print(""[Setup][FTP] FTP module enabled. Running FTP download..."")
            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)
        else:
            print(""[Setup][FTP] FTP module disabled. Skipping FTP download..."")

        # Process/Organize downloaded files
        data_service.process_data_files(input_dir=data_dirs.input_dir,
                                        temp_dir=data_dirs.temp_dir,
                                        output_dir=data_dirs.vcf_dir)

        # Convert VCF files to Zarr format if the module is enabled
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)
        if vcf_to_zarr_config.enabled:
            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,
                                           output_zarr_dir=data_dirs.zarr_dir_setup,
                                           conversion_config=vcf_to_zarr_config)
    elif command == ""exec"":
        print(""[Exec] Executing benchmark tool."")

        # Clear out existing files in Zarr benchmark directory
        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        benchmark_label = cli_arguments[""label""]

        # Get Benchmark module settings from runtime config
        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)

        # Setup the benchmark runner
        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)

        # Run the benchmark
        benchmark.run_benchmark()
    else:
        print(""Error: Unexpected command specified. Exiting..."")
        sys.exit(1)


def main():
    try:
        _main()
    except KeyboardInterrupt:
        print(""Program interrupted. Exiting..."")
        sys.exit(1)
/n/n/nbenchmark/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_allele_count = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_allele_count"" in runtime_config.benchmark:
                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[""benchmark_allele_count""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    default_config_file_location = ""doc/benchmark.conf.default""

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        if os.path.exists(default_config_file_location):
            # Write the default configuration file to specified location
            copyfile(default_config_file_location, output_location)

            # Check whether configuration file now exists and report status
            if os.path.exists(output_location):
                print(""[Config] Configuration file has been generated successfully."")
            else:
                print(""[Config] Configuration file was not generated."")
        else:
            print(""[Config] Default configuration file could not be found. File should be located at:\n\t{}""
                  .format(default_config_file_location))
/n/n/nbenchmark/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import allel
import zarr
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import os
import pandas as pd
from collections import OrderedDict
from benchmark import config, data_service


class BenchmarkResultsData:
    run_number = None
    operation_name = None
    start_time = None
    exec_time = None

    def to_dict(self):
        return OrderedDict([(""Log Timestamp"", datetime.datetime.fromtimestamp(self.start_time)),
                            (""Run Number"", self.run_number),
                            (""Operation"", self.operation_name),
                            (""Execution Time"", self.exec_time)])

    def to_pandas(self):
        data = self.to_dict()
        df = pd.DataFrame(data, index=[1])
        df.index.name = '#'
        return df


class BenchmarkProfiler:
    benchmark_running = False

    def __init__(self, benchmark_label):
        self.results = BenchmarkResultsData()
        self.benchmark_label = benchmark_label

    def set_run_number(self, run_number):
        if not self.benchmark_running:
            self.results.run_number = run_number

    def start_benchmark(self, operation_name):
        if not self.benchmark_running:
            self.results.operation_name = operation_name

            self.benchmark_running = True

            # Start the benchmark timer
            self.results.start_time = time.time()

    def end_benchmark(self):
        if self.benchmark_running:
            end_time = time.time()

            # Calculate the execution time from start and end times
            self.results.exec_time = end_time - self.results.start_time

            # Save benchmark results
            self._record_runtime(self.results, ""{}.psv"".format(self.benchmark_label))

            self.benchmark_running = False

    def get_benchmark_results(self):
        return self.results

    def _record_runtime(self, benchmark_results, output_filename):
        """"""
        Records the benchmark results data entry to the specified PSV file.
        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data
        :param output_filename: Which file to output the benchmark results to
        :type benchmark_results: BenchmarkResultsData
        :type output_filename: str
        """"""
        output_filename = str(output_filename)

        psv_header = not os.path.isfile(output_filename)

        # Open the output file in append mode
        with open(output_filename, ""a"") as psv_file:
            pd_results = benchmark_results.to_pandas()
            pd_results.to_csv(psv_file, sep=""|"", header=psv_header, index=False)


class Benchmark:
    benchmark_zarr_dir = """"  # Directory for which to use data from for benchmark process
    benchmark_zarr_file = """"  # File within benchmark_zarr_dir for which to use for benchmark process

    def __init__(self, bench_conf, data_dirs, benchmark_label):
        """"""
        Sets up a Benchmark object which is used to execute benchmarks.
        :param bench_conf: Benchmark configuration data that controls the benchmark execution
        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories
        :param benchmark_label: label to use when saving benchmark results to file
        :type bench_conf: config.BenchmarkConfigurationRepresentation
        :type data_dirs: config.DataDirectoriesConfigurationRepresentation
        :type benchmark_label: str
        """"""
        self.bench_conf = bench_conf
        self.data_dirs = data_dirs
        self.benchmark_label = benchmark_label

        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)

    def run_benchmark(self):
        """"""
        Executes the benchmarking process.
        """"""
        if self.bench_conf is not None and self.data_dirs is not None:
            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):
                # Clear out existing files in Zarr benchmark directory
                # (Should be done every single run)
                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)

                # Update run number in benchmark profiler (for results tracking)
                self.benchmark_profiler.set_run_number(run_number)

                # Prepare data directory and file locations for benchmarks
                if self.bench_conf.benchmark_data_input == ""vcf"":
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark

                    # Convert VCF data to Zarr format as part of benchmark
                    self._benchmark_convert_to_zarr()

                elif self.bench_conf.benchmark_data_input == ""zarr"":
                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup
                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset

                else:
                    print(""[Exec] Error: Invalid option supplied for benchmark data input format."")
                    print(""  - Expected data input formats: vcf, zarr"")
                    print(""  - Provided data input format: {}"".format(self.bench_conf.benchmark_data_input))
                    exit(1)

                # Ensure Zarr dataset exists and can be used for upcoming benchmarks
                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)
                if (benchmark_zarr_path != """") and (os.path.isdir(benchmark_zarr_path)):
                    # TODO: Run remaining benchmarks (e.g. loading into memory, allele counting, PCA, etc.)
                    pass
                else:
                    # Zarr dataset doesn't exist. Print error message and exit
                    print(""[Exec] Error: Zarr dataset could not be found for benchmarking."")
                    print(""  - Zarr dataset location: {}"".format(benchmark_zarr_path))

    def _benchmark_convert_to_zarr(self):
        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark
        input_vcf_file = self.bench_conf.benchmark_dataset
        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)

        if os.path.isfile(input_vcf_path):
            output_zarr_file = input_vcf_file
            output_zarr_file = output_zarr_file[
                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename
            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)

            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=self.bench_conf.vcf_to_zarr_config,
                                         benchmark_runner=self.benchmark_profiler)

            self.benchmark_zarr_file = output_zarr_file
        else:
            print(""[Exec] Error: Dataset specified in configuration file does not exist. Exiting..."")
            print(""  - Dataset file specified in configuration: {}"".format(input_vcf_file))
            print(""  - Expected file location: {}"".format(input_vcf_path))
            exit(1)
/n/n/nbenchmark/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import urllib.request
from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urllib.request.urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    :type benchmark_runner: core.BenchmarkProfiler
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get fields to extract (for unit testing only)
        fields = conversion_config.fields

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")
            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)
            numalt = callset['variants/numalt']
            alt_number = np.max(numalt)
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        if benchmark_runner is not None:
            benchmark_runner.start_benchmark(operation_name=""Convert VCF to Zarr"")

        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)

        if benchmark_runner is not None:
            benchmark_runner.end_benchmark()


GENOTYPE_ARRAY_NORMAL = 0
GENOTYPE_ARRAY_DASK = 1
GENOTYPE_ARRAY_CHUNKED = 2


def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):
    genotype_ref_name = ''

    # Ensure 'calldata' is within the callset
    if 'calldata' in callset:
        # Try to find either GT or genotype in calldata
        if 'GT' in callset['calldata']:
            genotype_ref_name = 'GT'
        elif 'genotype' in callset['calldata']:
            genotype_ref_name = 'genotype'
        else:
            return None
    else:
        return None

    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:
        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_DASK:
        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:
        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])
    else:
        return None
/n/n/ntests/test_cli.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run:  
    python -m unittest tests.test_cli 

""""""
import unittest
import sys
from unittest.mock import patch
from benchmark import cli

class TestCommandLineInterface(unittest.TestCase):


    def run_subparser_test(self,subparser_cmd,parameter,expected, default_key=None, default_value=None):  
        """""" Tests subparsers for missing arguments and default values. """"""
        testargs = [""prog"", subparser_cmd, ""--""+parameter, expected]
        with patch.object(sys, 'argv', testargs):
            args = cli.get_cli_arguments()
            self.assertEqual(args[parameter],expected, 
                subparser_cmd + "" subparser did not parse right config file arg."")
            self.assertEqual(args[""command""],subparser_cmd,  subparser_cmd + "" command was not interpreted properly"")
            if default_key:
               self.assertEqual(args[default_key],default_value, 
                subparser_cmd + "" command parser did not setup the right default key "" + default_key + 
                "" to "" + default_value )


    def test_getting_command_arguments(self):
        """""" Tests for reading args and storing values for running all benchmark options from the command line.""""""
        # Test group 1 -- config
        self.run_subparser_test(""config"",""output_config"",""./benchmark.conf"")
        # Test group 2 -- setup
        self.run_subparser_test(""setup"",""config_file"",""./benhcmark.conf"")  
        # Test group 3 - Tests if it the argparser is setting default values """"""
        self.run_subparser_test(""exec"",""config_file"",""./benchmark.conf"")


    def test_parser_expected_failing(self):
        """""" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (""something"") """"""
        testargs = [""prog""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises( SystemExit ) as cm: 
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code, 
                    ""CLI handler was supposed to fail on the missing command line argument."")

        testargs = [""prog"",""something""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises( SystemExit ) as cm: 
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code, 
                    ""CLI handler was supposed to fail on the wrong command line argument."")


if __name__ == '__main__':
    unittest.main()/n/n/ntests/test_core.py/n/nimport unittest
from benchmark.core import *
from time import sleep
import os


class TestCoreBenchmark(unittest.TestCase):
    def test_benchmark_profiler_results(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_profiler_results'
        profiler = BenchmarkProfiler(profiler_label)

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = 'Sleep {} seconds'.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            # Grab benchmark results
            results = profiler.get_benchmark_results()
            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals
            results_operation_name = results.operation_name
            results_run_number = results.run_number

            # Ensure benchmark results match expected values
            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')
            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')
            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')

            i += 1

        # Delete *.psv file created when running benchmark
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_results_psv(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_results_psv'

        # Delete *.psv file created from any previous unit testing
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

        profiler = BenchmarkProfiler(profiler_label)

        operation_name_format = 'Sleep {} seconds'

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = operation_name_format.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            i += 1

        # Read results psv file
        psv_file = '{}.psv'.format(profiler_label)

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)
            num_lines = len(psv_lines)
            num_lines_expected = len(benchmark_times) + 1
            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')

            # Ensure header (first line) of psv file is correct
            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'
            header_actual = psv_lines[0]
            self.assertEqual(header_expected, header_actual)

            # Ensure contents (benchmark data) of psv file is correct
            i = 1
            for line_number in range(1, num_lines):
                content = psv_lines[line_number].split('|')

                # Ensure column count is correct
                num_columns = len(content)
                num_columns_expected = 4
                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')

                # Ensure run number is correct
                run_number_psv = int(content[1])
                run_number_expected = i
                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')

                # Ensure operation name is correct
                operation_name_psv = content[2]
                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])
                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')

                # Ensure execution time is correct
                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals
                execution_time_expected = benchmark_times[i - 1]
                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')

                i += 1

        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Delete *.psv file created when running benchmark
        if os.path.exists(psv_file):
            os.remove(psv_file)


if __name__ == ""__main__"":
    unittest.main()
/n/n/ntests/test_data_service.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run from the home directory:  
    python -m unittest tests.test_data_service
""""""
import unittest
import os.path
import shutil
import zarr
import numpy as np
import pathlib

from benchmark import data_service, config


class TestDataServices(unittest.TestCase):
    def test_fetch_data_via_ftp(self):
        local_directory = ""./""

        test_config_location = ""./tests/data/ftp_test_fetch_data.conf""
        runtime_config = config.read_configuration(location=test_config_location)
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        # Attempt to remove local files in case a previous unit test failed to do so (avoid false positive)
        for file in ftp_config.files:
            if os.path.isfile(file):
                os.remove(file)

        data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=local_directory)

        flag = True
        for file in ftp_config.files:
            if not os.path.isfile(file):
                flag = False
                break
        self.assertTrue(flag)

        # Remove the downloaded files
        for file in ftp_config.files:
            if os.path.isfile(file):
                os.remove(file)

    def test_fetch_file_from_url(self):
        """""" Tests fetching a data to be used in benchmarking from a remote URL. """"""
        remote_file_url = ""ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/pilot_data/release/2010_07/trio/snps/trio.2010_06.ychr.sites.vcf.gz""
        local_filename = ""trio.2010_06.ychr.sites.vcf.gz""

        # Attempt to remove local file in case a previous unit test failed to do so (prevents false positive)
        if os.path.isfile(local_filename):
            os.remove(local_filename)

        data_service.fetch_file_from_url(remote_file_url, local_filename)
        self.assertTrue(os.path.isfile(local_filename), ""No local file retrieved"")

        # Remove the downloaded file
        if os.path.isfile(local_filename):
            os.remove(local_filename)

    def test_decompress_gzip(self):
        """""" Tests decompressing the fetched file. """"""
        local_file_gz = ""./tests/data/trio.2010_06.ychr.sites.vcf.gz""
        local_filename = ""trio.2010_06.ychr.sites.vcf""

        # Attempt to remove local file in case a previous unit test failed to do so (prevents false positive)
        if os.path.isfile(local_filename):
            os.remove(local_filename)

        data_service.decompress_gzip(local_file_gz, local_filename)
        self.assertTrue(os.path.isfile(local_filename), ""No local file decompressed."")

        # Remove the downloaded file
        if os.path.isfile(local_filename):
            os.remove(local_filename)

    def test_read_file_contents_existing_file(self):
        local_filepath = ""./tests/data/test_read_file_contents_data.txt""

        if os.path.isfile(local_filepath):
            results = data_service.read_file_contents(local_filepath)
            self.assertEqual(results, ""test data"")
        else:
            self.fail(""Test data file does not exist. Please ensure the file exists and try running test again"")

    def test_read_file_contents_missing_file(self):
        local_filepath = ""./tests/data/test_read_file_contents_data_nonexistent.txt""

        if not os.path.isfile(local_filepath):
            results = data_service.read_file_contents(local_filepath)
            self.assertEqual(results, None)
        else:
            self.fail(""File should not exist on filesystem. Please remove the file and try running test again."")

    def test_process_data_files(self):
        # Define test input files
        test_dir = ""./tests/data/""
        test_files_input = [""trio.2010_06.ychr.sites.vcf.gz""]
        test_files_expected = [""trio.2010_06.ychr.sites.vcf""]

        # Setup test processing directories
        process_data_files_test_dir = ""./data/unittest/""
        input_dir_test = process_data_files_test_dir + ""input/""
        temp_dir_test = process_data_files_test_dir + ""temp/""
        output_dir_test = process_data_files_test_dir + ""vcf/""

        # Remove the test directory created for this unittest (from any previous unit testing)
        if os.path.exists(process_data_files_test_dir):
            shutil.rmtree(process_data_files_test_dir)

        # Create input directory
        pathlib.Path(input_dir_test).mkdir(parents=True, exist_ok=True)

        # Copy test files into input directory to test data processing
        for test_file in test_files_input:
            test_file_expected = test_dir + test_file
            test_file_output = input_dir_test + test_file
            if os.path.exists(test_file_expected):
                shutil.copy(test_file_expected, test_file_output)

        # Process the test files
        data_service.process_data_files(input_dir=input_dir_test, temp_dir=temp_dir_test, output_dir=output_dir_test)

        # Check the results to ensure corresponding vcf files exist in output directory
        error_flag = False
        for test_file_expected in test_files_expected:
            if not os.path.exists(output_dir_test + test_file_expected):
                error_flag = True

        # Remove the test directory created for this unittest
        shutil.rmtree(process_data_files_test_dir)

        # Return an error if the test failed
        if error_flag:
            self.fail(msg=""One or more test files were not processed and placed in output directory."")

    def test_convert_to_zarr(self):
        input_vcf_path = ""./tests/data/trio.2010_06.ychr.sites.vcf""
        output_zarr_path = ""trio.2010_06.ychr.sites.zarr""

        # Attempt to remove local file in case a previous unit test failed to do so (prevents false positive)
        if os.path.isdir(output_zarr_path):
            shutil.rmtree(output_zarr_path)

        if os.path.isfile(input_vcf_path):
            # Setup test settings for Zarr conversion
            vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation()
            vcf_to_zarr_config.fields = 'variants/numalt'
            vcf_to_zarr_config.enabled = True
            vcf_to_zarr_config.compressor = ""Blosc""
            vcf_to_zarr_config.blosc_compression_algorithm = ""zstd""
            vcf_to_zarr_config.blosc_compression_level = 1
            vcf_to_zarr_config.blosc_shuffle_mode = -1

            # Convert VCF file to Zarr
            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=vcf_to_zarr_config)

            # Load the Zarr data from storage for testing
            callset = zarr.open_group(output_zarr_path, mode=""r"")
            numalt = callset['variants/numalt']
            self.assertEqual(np.size(numalt), 959)
            self.assertEqual(np.max(numalt), 1)
        else:
            self.fail(""Test data file does not exist. Please ensure the file exists and try running test again"")

        # Remove the Zarr test data
        if os.path.isdir(output_zarr_path):
            shutil.rmtree(output_zarr_path)


if __name__ == ""__main__"":
    unittest.main()
/n/n/n",0
79,5577269b7283a48d0b421dabd44687460955a0c8,"/benchmark/cli.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import argparse  # for command line parsing
import time  # for benchmark timer
import csv  # for writing results
import logging
import sys
import shutil
from benchmark import config, data_service


def get_cli_arguments():
    """""" Returns command line arguments. 

    Returns:
    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), 
    and config argument for where is the config file. """"""

    logging.debug('Getting cli arguments')

    parser = argparse.ArgumentParser(description=""A benchmark for genomics routines in Python."")

    # Enable three exclusive groups of options (using subparsers)
    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525

    subparser = parser.add_subparsers(title=""commands"", dest=""command"")
    subparser.required = True

    config_parser = subparser.add_parser(""config"",
                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')
    config_parser.add_argument(""--output_config"", type=str, required=True,
                               help=""Specify the output path to a configuration file."", metavar=""FILEPATH"")
    config_parser.add_argument(""-f"", action=""store_true"", help=""Overwrite the destination file if it already exists."")

    data_setup_parser = subparser.add_parser(""setup"",
                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')
    data_setup_parser.add_argument(""--config_file"", required=True, help=""Location of the configuration file"",
                                   metavar=""FILEPATH"")

    benchmark_exec_parser = subparser.add_parser(""exec"",
                                                 help='Execution of the benchmark modes. It requires a configuration file.')
    # TODO: use run_(timestamp) as default
    benchmark_exec_parser.add_argument(""--label"", type=str, default=""run"", metavar=""RUN_LABEL"",
                                       help=""Label for the benchmark run."")
    benchmark_exec_parser.add_argument(""--config_file"", type=str, required=True,
                                       help=""Specify the path to a configuration file."", metavar=""FILEPATH"")

    runtime_configuration = vars(parser.parse_args())
    return runtime_configuration


def _main():
    input_directory = ""./data/input/""
    download_directory = input_directory + ""download/""
    temp_directory = ""./data/temp/""
    vcf_directory = ""./data/vcf/""
    zarr_directory_setup = ""./data/zarr/""
    zarr_directory_benchmark = ""./data/zarr_benchmark/""

    cli_arguments = get_cli_arguments()

    command = cli_arguments[""command""]
    if command == ""config"":
        output_config_location = cli_arguments[""output_config""]
        overwrite_mode = cli_arguments[""f""]
        config.generate_default_config_file(output_location=output_config_location,
                                            overwrite=overwrite_mode)
    elif command == ""setup"":
        print(""[Setup] Setting up benchmark data."")

        # Clear out existing files in VCF and Zarr directories
        data_service.remove_directory_tree(vcf_directory)
        data_service.remove_directory_tree(zarr_directory_setup)

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        # Get FTP module settings from runtime config
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        if ftp_config.enabled:
            print(""[Setup][FTP] FTP module enabled. Running FTP download..."")
            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)
        else:
            print(""[Setup][FTP] FTP module disabled. Skipping FTP download..."")

        # Process/Organize downloaded files
        data_service.process_data_files(input_dir=input_directory,
                                        temp_dir=temp_directory,
                                        output_dir=vcf_directory)

        # Convert VCF files to Zarr format if the module is enabled
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)
        if vcf_to_zarr_config.enabled:
            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,
                                           output_zarr_dir=zarr_directory_setup,
                                           conversion_config=vcf_to_zarr_config)
    elif command == ""exec"":
        print(""[Exec] Executing benchmark tool."")

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        # Get VCF to Zarr conversion settings from runtime config
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)

        # TODO: Convert necessary VCF files to Zarr format
        # data_service.convert_to_zarr(""./data/vcf/chr22.1000.vcf"", ""./data/zarr/chr22.1000.zarr"", vcf_to_zarr_config)
    else:
        print(""Error: Unexpected command specified. Exiting..."")
        sys.exit(1)


def main():
    try:
        _main()
    except KeyboardInterrupt:
        print(""Program interrupted. Exiting..."")
        sys.exit(1)
/n/n/n/benchmark/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import time  # for benchmark timer
import csv  # for writing results
import logging


def run_benchmark(bench_conf):
    pass


def run_dynamic(ftp_location):
    pass


def run_static():
    pass


def get_remote_files(ftp_server, ftp_directory, files=None):
    pass


def record_runtime(benchmark, timestamp):
    pass


# temporary here
def main():
    pass
/n/n/n/benchmark/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import urllib.request
from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc, LZ4, LZMA
from benchmark import config

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urllib.request.urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")
            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)
            numalt = callset['variants/numalt']
            alt_number = np.max(numalt)
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        print(""[VCF-Zarr] Using {} compressor."".format(conversion_config.compressor))

        print(""[VCF-Zarr] Performing VCF to Zarr conversion..."")
        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)
        print(""[VCF-Zarr] Done."")
/n/n/n",1
80,18609fa3b8b1e8cca95f1021d60750628abf7433,"genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_aggregations = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_aggregations"" in runtime_config.benchmark:
                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[""benchmark_aggregations""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/ngenomics_benchmarks/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import allel
import zarr
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import os
import pandas as pd
from collections import OrderedDict
from genomics_benchmarks import config, data_service


class BenchmarkResultsData:
    run_number = None
    operation_name = None
    start_time = None
    exec_time = None

    def to_dict(self):
        return OrderedDict([(""Log Timestamp"", datetime.datetime.fromtimestamp(self.start_time)),
                            (""Run Number"", self.run_number),
                            (""Operation"", self.operation_name),
                            (""Execution Time"", self.exec_time)])

    def to_pandas(self):
        data = self.to_dict()
        df = pd.DataFrame(data, index=[1])
        df.index.name = '#'
        return df


class BenchmarkProfiler:
    benchmark_running = False

    def __init__(self, benchmark_label):
        self.results = BenchmarkResultsData()
        self.benchmark_label = benchmark_label

    def set_run_number(self, run_number):
        if not self.benchmark_running:
            self.results.run_number = run_number

    def start_benchmark(self, operation_name):
        if not self.benchmark_running:
            self.results.operation_name = operation_name

            self.benchmark_running = True

            # Start the benchmark timer
            self.results.start_time = time.time()

    def end_benchmark(self):
        if self.benchmark_running:
            end_time = time.time()

            # Calculate the execution time from start and end times
            self.results.exec_time = end_time - self.results.start_time

            # Save benchmark results
            self._record_runtime(self.results, ""{}.psv"".format(self.benchmark_label))

            self.benchmark_running = False

    def get_benchmark_results(self):
        return self.results

    def _record_runtime(self, benchmark_results, output_filename):
        """"""
        Records the benchmark results data entry to the specified PSV file.
        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data
        :param output_filename: Which file to output the benchmark results to
        :type benchmark_results: BenchmarkResultsData
        :type output_filename: str
        """"""
        output_filename = str(output_filename)

        psv_header = not os.path.isfile(output_filename)

        # Open the output file in append mode
        with open(output_filename, ""a"") as psv_file:
            pd_results = benchmark_results.to_pandas()
            pd_results.to_csv(psv_file, sep=""|"", header=psv_header, index=False)


class Benchmark:
    benchmark_zarr_dir = """"  # Directory for which to use data from for benchmark process
    benchmark_zarr_file = """"  # File within benchmark_zarr_dir for which to use for benchmark process

    def __init__(self, bench_conf, data_dirs, benchmark_label):
        """"""
        Sets up a Benchmark object which is used to execute benchmarks.
        :param bench_conf: Benchmark configuration data that controls the benchmark execution
        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories
        :param benchmark_label: label to use when saving benchmark results to file
        :type bench_conf: config.BenchmarkConfigurationRepresentation
        :type data_dirs: config.DataDirectoriesConfigurationRepresentation
        :type benchmark_label: str
        """"""
        self.bench_conf = bench_conf
        self.data_dirs = data_dirs
        self.benchmark_label = benchmark_label

        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)

    def run_benchmark(self):
        """"""
        Executes the benchmarking process.
        """"""
        if self.bench_conf is not None and self.data_dirs is not None:
            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):
                # Clear out existing files in Zarr benchmark directory
                # (Should be done every single run)
                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)

                # Update run number in benchmark profiler (for results tracking)
                self.benchmark_profiler.set_run_number(run_number)

                # Prepare data directory and file locations for benchmarks
                if self.bench_conf.benchmark_data_input == ""vcf"":
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark

                    # Convert VCF data to Zarr format as part of benchmark
                    self._benchmark_convert_to_zarr()

                elif self.bench_conf.benchmark_data_input == ""zarr"":
                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup
                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset

                else:
                    print(""[Exec] Error: Invalid option supplied for benchmark data input format."")
                    print(""  - Expected data input formats: vcf, zarr"")
                    print(""  - Provided data input format: {}"".format(self.bench_conf.benchmark_data_input))
                    exit(1)

                # Ensure Zarr dataset exists and can be used for upcoming benchmarks
                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)
                if (benchmark_zarr_path != """") and (os.path.isdir(benchmark_zarr_path)):
                    # Load Zarr dataset into memory
                    self._benchmark_load_zarr_dataset(benchmark_zarr_path)

                    if self.bench_conf.benchmark_aggregations:
                        self._benchmark_simple_aggregations(benchmark_zarr_path)
                else:
                    # Zarr dataset doesn't exist. Print error message and exit
                    print(""[Exec] Error: Zarr dataset could not be found for benchmarking."")
                    print(""  - Zarr dataset location: {}"".format(benchmark_zarr_path))
                    exit(1)

    def _benchmark_convert_to_zarr(self):
        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark
        input_vcf_file = self.bench_conf.benchmark_dataset
        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)

        if os.path.isfile(input_vcf_path):
            output_zarr_file = input_vcf_file
            output_zarr_file = output_zarr_file[
                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename
            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)

            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=self.bench_conf.vcf_to_zarr_config,
                                         benchmark_profiler=self.benchmark_profiler)

            self.benchmark_zarr_file = output_zarr_file
        else:
            print(""[Exec] Error: Dataset specified in configuration file does not exist. Exiting..."")
            print(""  - Dataset file specified in configuration: {}"".format(input_vcf_file))
            print(""  - Expected file location: {}"".format(input_vcf_path))
            exit(1)

    def _benchmark_load_zarr_dataset(self, zarr_path):
        self.benchmark_profiler.start_benchmark(operation_name=""Load Zarr Dataset"")
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)
        self.benchmark_profiler.end_benchmark()

    def _benchmark_simple_aggregations(self, zarr_path):
        # Load Zarr dataset
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)

        gtz = callset['calldata/GT']

        # Setup genotype Dask array for computations
        gt = allel.GenotypeDaskArray(gtz)

        # Run benchmark for allele count
        self.benchmark_profiler.start_benchmark(operation_name=""Allele Count (All Samples)"")
        gt.count_alleles().compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Variant"")
        gt.count_het(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Variant"")
        gt.count_hom(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Sample"")
        gt.count_het(axis=0).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Sample"")
        gt.count_hom(axis=0).compute()
        self.benchmark_profiler.end_benchmark()
/n/n/ngenomics_benchmarks/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import sys

# Support Python 2.x and 3.x
if sys.version_info[0] >= 3:
    from urllib.request import urlretrieve
else:
    from urllib import urlretrieve

from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc
from genomics_benchmarks import config

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    try:
        pathlib.Path(path).mkdir(parents=True)
    except OSError:  # Catch if directory already exists
        pass


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_profiler=None):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    :type benchmark_runner: core.BenchmarkProfiler
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get fields to extract (for unit testing only)
        fields = conversion_config.fields

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Read VCF file into memory for alt number"")

            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()

            numalt = callset['variants/numalt']

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Determine maximum alt number"")

            alt_number = np.max(numalt)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        if benchmark_profiler is not None:
            benchmark_profiler.start_benchmark(operation_name=""Convert VCF to Zarr"")

        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)

        if benchmark_profiler is not None:
            benchmark_profiler.end_benchmark()


GENOTYPE_ARRAY_NORMAL = 0
GENOTYPE_ARRAY_DASK = 1
GENOTYPE_ARRAY_CHUNKED = 2


def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):
    genotype_ref_name = ''

    # Ensure 'calldata' is within the callset
    if 'calldata' in callset:
        # Try to find either GT or genotype in calldata
        if 'GT' in callset['calldata']:
            genotype_ref_name = 'GT'
        elif 'genotype' in callset['calldata']:
            genotype_ref_name = 'genotype'
        else:
            return None
    else:
        return None

    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:
        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_DASK:
        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:
        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])
    else:
        return None
/n/n/ntests/test_cli.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run:  
    python -m unittest tests.test_cli 

""""""
import unittest
import sys

try:
    from unittest.mock import patch
except ImportError:
    from mock import patch
from genomics_benchmarks import cli


class TestCommandLineInterface(unittest.TestCase):

    def run_subparser_test(self, subparser_cmd, parameter, expected, default_key=None, default_value=None):
        """""" Tests subparsers for missing arguments and default values. """"""
        testargs = [""prog"", subparser_cmd, ""--"" + parameter, expected]
        with patch.object(sys, 'argv', testargs):
            args = cli.get_cli_arguments()
            self.assertEqual(args[parameter], expected,
                             subparser_cmd + "" subparser did not parse right config file arg."")
            self.assertEqual(args[""command""], subparser_cmd, subparser_cmd + "" command was not interpreted properly"")
            if default_key:
                self.assertEqual(args[default_key], default_value,
                                 subparser_cmd + "" command parser did not setup the right default key "" + default_key +
                                 "" to "" + default_value)

    def test_getting_command_arguments(self):
        """""" Tests for reading args and storing values for running all benchmark options from the command line.""""""
        # Test group 1 -- config
        self.run_subparser_test(""config"", ""output_config"", ""./benchmark.conf"")
        # Test group 2 -- setup
        self.run_subparser_test(""setup"", ""config_file"", ""./benhcmark.conf"")
        # Test group 3 - Tests if it the argparser is setting default values """"""
        self.run_subparser_test(""exec"",""config_file"",""./benchmark.conf"")

    def test_parser_expected_failing(self):
        """""" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (""something"") """"""
        testargs = [""prog""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the missing command line argument."")

        testargs = [""prog"", ""something""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the wrong command line argument."")


if __name__ == '__main__':
    unittest.main()
/n/n/ntests/test_core.py/n/nimport unittest
from genomics_benchmarks.core import *
from genomics_benchmarks.config import \
    BenchmarkConfigurationRepresentation, \
    VCFtoZarrConfigurationRepresentation, \
    DataDirectoriesConfigurationRepresentation
from time import sleep
import os
import shutil


class TestCoreBenchmark(unittest.TestCase):
    def test_benchmark_profiler_results(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_profiler_results'
        profiler = BenchmarkProfiler(profiler_label)

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = 'Sleep {} seconds'.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            # Grab benchmark results
            results = profiler.get_benchmark_results()
            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals
            results_operation_name = results.operation_name
            results_run_number = results.run_number

            # Ensure benchmark results match expected values
            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')
            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')
            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')

            i += 1

        # Delete *.psv file created when running benchmark
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_results_psv(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_results_psv'

        # Delete *.psv file created from any previous unit testing
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

        profiler = BenchmarkProfiler(profiler_label)

        operation_name_format = 'Sleep {} seconds'

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = operation_name_format.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            i += 1

        # Read results psv file
        psv_file = '{}.psv'.format(profiler_label)

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)
            num_lines = len(psv_lines)
            num_lines_expected = len(benchmark_times) + 1
            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')

            # Ensure header (first line) of psv file is correct
            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'
            header_actual = psv_lines[0]
            self.assertEqual(header_expected, header_actual)

            # Ensure contents (benchmark data) of psv file is correct
            i = 1
            for line_number in range(1, num_lines):
                content = psv_lines[line_number].split('|')

                # Ensure column count is correct
                num_columns = len(content)
                num_columns_expected = 4
                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')

                # Ensure run number is correct
                run_number_psv = int(content[1])
                run_number_expected = i
                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')

                # Ensure operation name is correct
                operation_name_psv = content[2]
                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])
                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')

                # Ensure execution time is correct
                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals
                execution_time_expected = benchmark_times[i - 1]
                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')

                i += 1

        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Delete *.psv file created when running benchmark
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_simple_aggregations(self):
        test_dir = './tests_temp/'
        benchmark_label = 'test_benchmark_simple_aggregations'
        psv_file = '{}.psv'.format(benchmark_label)

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from any previous unit tests
        if os.path.isfile(psv_file):
            os.remove(psv_file)

        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()
        vcf_to_zar_config.enabled = True

        bench_conf = BenchmarkConfigurationRepresentation()
        bench_conf.vcf_to_zarr_config = vcf_to_zar_config
        bench_conf.benchmark_number_runs = 1
        bench_conf.benchmark_data_input = 'vcf'
        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'
        bench_conf.benchmark_aggregations = True

        data_dirs = DataDirectoriesConfigurationRepresentation()
        data_dirs.vcf_dir = './tests/data/'
        data_dirs.zarr_dir_setup = './tests_temp/zarr/'
        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'
        data_dirs.temp_dir = './tests_temp/temp/'

        # Run the benchmark and ensure nothing fails
        benchmark = Benchmark(bench_conf=bench_conf,
                              data_dirs=data_dirs,
                              benchmark_label='test_benchmark_simple_aggregations')
        benchmark.run_benchmark()

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file
            num_lines = len(psv_lines)
            num_lines_expected = 10
            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')

            psv_operation_names = []

            for psv_line in psv_lines:
                line_split = psv_line.split('|')
                line_cols_actual = len(line_split)
                line_cols_expected = 4

                # Ensure correct number of data columns exist for current line of data
                self.assertEqual(line_cols_expected, line_cols_actual,
                                 msg='Unexpected number of columns in resulting psv file')

                operation_name = line_split[2]
                psv_operation_names.append(operation_name)

            # Ensure all aggregations were run
            test_operation_names = ['Allele Count (All Samples)',
                                    'Genotype Count: Heterozygous per Variant',
                                    'Genotype Count: Homozygous per Variant',
                                    'Genotype Count: Heterozygous per Sample',
                                    'Genotype Count: Homozygous per Sample']

            for test_operation_name in test_operation_names:
                if test_operation_name not in psv_operation_names:
                    self.fail(msg='Operation \""{}\"" was not run during the benchmark.'.format(test_operation_name))
        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from this unit test
        if os.path.isfile(psv_file):
            os.remove(psv_file)


if __name__ == ""__main__"":
    unittest.main()
/n/n/n",0
81,18609fa3b8b1e8cca95f1021d60750628abf7433,"/genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_allele_count = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_allele_count"" in runtime_config.benchmark:
                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[""benchmark_allele_count""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/n",1
82,e5195bc7bcf1060f2f727acf9f0cee033262caaa,"genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]

PCA_DATA_SCALER_STANDARD = 0
PCA_DATA_SCALER_PATTERSON = 1
PCA_DATA_SCALER_NONE = 2
benchmark_pca_data_scaler_types = {PCA_DATA_SCALER_STANDARD: 'standard',
                                   PCA_DATA_SCALER_PATTERSON: 'patterson',
                                   PCA_DATA_SCALER_NONE: None}

GENOTYPE_ARRAY_NORMAL = 0
GENOTYPE_ARRAY_DASK = 1
GENOTYPE_ARRAY_CHUNKED = 2
benchmark_pca_genotype_array_types = {GENOTYPE_ARRAY_NORMAL,
                                      GENOTYPE_ARRAY_DASK,
                                      GENOTYPE_ARRAY_CHUNKED}


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_aggregations = False
    benchmark_pca = False
    vcf_to_zarr_config = None

    # PCA-specific settings
    pca_number_components = 10
    pca_data_scaler = benchmark_pca_data_scaler_types[PCA_DATA_SCALER_PATTERSON]
    pca_genotype_array_type = GENOTYPE_ARRAY_DASK
    pca_subset_size = 100000
    pca_ld_pruning_number_iterations = 2
    pca_ld_pruning_size = 100
    pca_ld_pruning_step = 20
    pca_ld_pruning_threshold = 0.01

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_aggregations"" in runtime_config.benchmark:
                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[""benchmark_aggregations""])
                if ""benchmark_pca"" in runtime_config.benchmark:
                    self.benchmark_pca = config_str_to_bool(runtime_config.benchmark[""benchmark_pca""])
                if ""pca_number_components"" in runtime_config.benchmark:
                    pca_number_components_str = runtime_config.benchmark[""pca_number_components""]
                    if isint(pca_number_components_str) and (int(pca_number_components_str) > 0):
                        self.pca_number_components = int(pca_number_components_str)
                    else:
                        raise ValueError(""Invalid value for pca_number_components in configuration.\n""
                                         ""pca_number_components must be a valid integer greater than 0."")
                if ""pca_data_scaler"" in runtime_config.benchmark:
                    pca_data_scaler_str = runtime_config.benchmark[""pca_data_scaler""]
                    if isint(pca_data_scaler_str) and (int(pca_data_scaler_str) in benchmark_pca_data_scaler_types):
                        self.pca_data_scaler = benchmark_pca_data_scaler_types[int(pca_data_scaler_str)]
                    else:
                        raise ValueError(""Invalid value for pca_data_scaler in configuration.\n""
                                         ""pca_data_scaler must be a valid integer between 0 and 2"")
                if ""pca_genotype_array_type"" in runtime_config.benchmark:
                    pca_genotype_array_type_str = runtime_config.benchmark[""pca_genotype_array_type""]
                    if isint(pca_genotype_array_type_str) and (
                            int(pca_genotype_array_type_str) in benchmark_pca_genotype_array_types):
                        self.pca_genotype_array_type = int(pca_genotype_array_type_str)
                    else:
                        raise ValueError(""Invalid value for pca_genotype_array_type in configuration.\n""
                                         ""pca_genotype_array_type must be a valid integer between 0 and 2"")
                if ""pca_subset_size"" in runtime_config.benchmark:
                    pca_subset_size_str = runtime_config.benchmark[""pca_subset_size""]
                    if isint(pca_subset_size_str) and (int(pca_subset_size_str) > 0):
                        self.pca_subset_size = int(pca_subset_size_str)
                    else:
                        raise ValueError(""Invalid value for pca_subset_size in configuration.\n""
                                         ""pca_subset_size must be a valid integer greater than 0."")
                if ""pca_ld_pruning_number_iterations"" in runtime_config.benchmark:
                    pca_ld_pruning_number_iterations_str = runtime_config.benchmark[""pca_ld_pruning_number_iterations""]
                    if isint(pca_ld_pruning_number_iterations_str) and (int(pca_ld_pruning_number_iterations_str) > 0):
                        self.pca_ld_pruning_number_iterations = int(pca_ld_pruning_number_iterations_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_number_iterations in configuration.\n""
                                         ""pca_ld_pruning_number_iterations must be a valid integer greater than 0."")
                if ""pca_ld_pruning_size"" in runtime_config.benchmark:
                    pca_ld_pruning_size_str = runtime_config.benchmark[""pca_ld_pruning_size""]
                    if isint(pca_ld_pruning_size_str) and (int(pca_ld_pruning_size_str) > 0):
                        self.pca_ld_pruning_size = int(pca_ld_pruning_size_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_size in configuration.\n""
                                         ""pca_ld_pruning_size must be a valid integer greater than 0."")
                if ""pca_ld_pruning_step"" in runtime_config.benchmark:
                    pca_ld_pruning_step_str = runtime_config.benchmark[""pca_ld_pruning_step""]
                    if isint(pca_ld_pruning_step_str) and (int(pca_ld_pruning_step_str) > 0):
                        self.pca_ld_pruning_step = int(pca_ld_pruning_step_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_step in configuration.\n""
                                         ""pca_ld_pruning_step must be a valid integer greater than 0."")
                if ""pca_ld_pruning_threshold"" in runtime_config.benchmark:
                    pca_ld_pruning_threshold_str = runtime_config.benchmark[""pca_ld_pruning_threshold""]
                    if isfloat(pca_ld_pruning_threshold_str) and (float(pca_ld_pruning_threshold_str) > 0):
                        self.pca_ld_pruning_threshold = float(pca_ld_pruning_threshold_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_threshold in configuration.\n""
                                         ""pca_ld_pruning_threshold must be a valid float greater than 0."")

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/ngenomics_benchmarks/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import allel
import zarr
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import numpy as np
import os
import pandas as pd
from collections import OrderedDict
from genomics_benchmarks import config, data_service


class BenchmarkResultsData:
    run_number = None
    operation_name = None
    start_time = None
    exec_time = None

    def to_dict(self):
        return OrderedDict([(""Log Timestamp"", datetime.datetime.fromtimestamp(self.start_time)),
                            (""Run Number"", self.run_number),
                            (""Operation"", self.operation_name),
                            (""Execution Time"", self.exec_time)])

    def to_pandas(self):
        data = self.to_dict()
        df = pd.DataFrame(data, index=[1])
        df.index.name = '#'
        return df


class BenchmarkProfiler:
    benchmark_running = False

    def __init__(self, benchmark_label):
        self.results = BenchmarkResultsData()
        self.benchmark_label = benchmark_label

    def set_run_number(self, run_number):
        if not self.benchmark_running:
            self.results.run_number = run_number

    def start_benchmark(self, operation_name):
        if not self.benchmark_running:
            self.results.operation_name = operation_name

            self.benchmark_running = True

            # Start the benchmark timer
            self.results.start_time = time.time()

    def end_benchmark(self):
        if self.benchmark_running:
            end_time = time.time()

            # Calculate the execution time from start and end times
            self.results.exec_time = end_time - self.results.start_time

            # Save benchmark results
            self._record_runtime(self.results, ""{}.psv"".format(self.benchmark_label))

            self.benchmark_running = False

    def get_benchmark_results(self):
        return self.results

    def _record_runtime(self, benchmark_results, output_filename):
        """"""
        Records the benchmark results data entry to the specified PSV file.
        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data
        :param output_filename: Which file to output the benchmark results to
        :type benchmark_results: BenchmarkResultsData
        :type output_filename: str
        """"""
        output_filename = str(output_filename)

        psv_header = not os.path.isfile(output_filename)

        # Open the output file in append mode
        with open(output_filename, ""a"") as psv_file:
            pd_results = benchmark_results.to_pandas()
            pd_results.to_csv(psv_file, sep=""|"", header=psv_header, index=False)


class Benchmark:
    benchmark_zarr_dir = """"  # Directory for which to use data from for benchmark process
    benchmark_zarr_file = """"  # File within benchmark_zarr_dir for which to use for benchmark process

    def __init__(self, bench_conf, data_dirs, benchmark_label):
        """"""
        Sets up a Benchmark object which is used to execute benchmarks.
        :param bench_conf: Benchmark configuration data that controls the benchmark execution
        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories
        :param benchmark_label: label to use when saving benchmark results to file
        :type bench_conf: config.BenchmarkConfigurationRepresentation
        :type data_dirs: config.DataDirectoriesConfigurationRepresentation
        :type benchmark_label: str
        """"""
        self.bench_conf = bench_conf
        self.data_dirs = data_dirs
        self.benchmark_label = benchmark_label

        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)

    def run_benchmark(self):
        """"""
        Executes the benchmarking process.
        """"""
        if self.bench_conf is not None and self.data_dirs is not None:
            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):
                # Clear out existing files in Zarr benchmark directory
                # (Should be done every single run)
                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)

                # Update run number in benchmark profiler (for results tracking)
                self.benchmark_profiler.set_run_number(run_number)

                # Prepare data directory and file locations for benchmarks
                if self.bench_conf.benchmark_data_input == ""vcf"":
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark

                    # Convert VCF data to Zarr format as part of benchmark
                    self._benchmark_convert_to_zarr()

                elif self.bench_conf.benchmark_data_input == ""zarr"":
                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup
                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset

                else:
                    print(""[Exec] Error: Invalid option supplied for benchmark data input format."")
                    print(""  - Expected data input formats: vcf, zarr"")
                    print(""  - Provided data input format: {}"".format(self.bench_conf.benchmark_data_input))
                    exit(1)

                # Ensure Zarr dataset exists and can be used for upcoming benchmarks
                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)
                if (benchmark_zarr_path != """") and (os.path.isdir(benchmark_zarr_path)):
                    # Load Zarr dataset into memory
                    self._benchmark_load_zarr_dataset(benchmark_zarr_path)

                    if self.bench_conf.benchmark_aggregations:
                        self._benchmark_simple_aggregations(benchmark_zarr_path)

                    if self.bench_conf.benchmark_pca:
                        self._benchmark_pca(benchmark_zarr_path)
                else:
                    # Zarr dataset doesn't exist. Print error message and exit
                    print(""[Exec] Error: Zarr dataset could not be found for benchmarking."")
                    print(""  - Zarr dataset location: {}"".format(benchmark_zarr_path))
                    exit(1)

    def _benchmark_convert_to_zarr(self):
        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark
        input_vcf_file = self.bench_conf.benchmark_dataset
        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)

        if os.path.isfile(input_vcf_path):
            output_zarr_file = input_vcf_file
            output_zarr_file = output_zarr_file[
                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename
            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)

            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=self.bench_conf.vcf_to_zarr_config,
                                         benchmark_profiler=self.benchmark_profiler)

            self.benchmark_zarr_file = output_zarr_file
        else:
            print(""[Exec] Error: Dataset specified in configuration file does not exist. Exiting..."")
            print(""  - Dataset file specified in configuration: {}"".format(input_vcf_file))
            print(""  - Expected file location: {}"".format(input_vcf_path))
            exit(1)

    def _benchmark_load_zarr_dataset(self, zarr_path):
        self.benchmark_profiler.start_benchmark(operation_name=""Load Zarr Dataset"")
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)
        self.benchmark_profiler.end_benchmark()

    def _benchmark_simple_aggregations(self, zarr_path):
        # Load Zarr dataset
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)

        gtz = callset['calldata/GT']

        # Setup genotype Dask array for computations
        gt = allel.GenotypeDaskArray(gtz)

        # Run benchmark for allele count
        self.benchmark_profiler.start_benchmark(operation_name=""Allele Count (All Samples)"")
        gt.count_alleles().compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Variant"")
        gt.count_het(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Variant"")
        gt.count_hom(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Sample"")
        gt.count_het(axis=0).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Sample"")
        gt.count_hom(axis=0).compute()
        self.benchmark_profiler.end_benchmark()

    def _benchmark_pca(self, zarr_path):
        # Load Zarr dataset
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)

        # Get genotype data from data set
        genotype_array_type = self.bench_conf.pca_genotype_array_type
        g = data_service.get_genotype_data(callset=callset, genotype_array_type=genotype_array_type)

        # Count alleles at each variant
        self.benchmark_profiler.start_benchmark('PCA: Count alleles')
        ac = g.count_alleles()[:]
        self.benchmark_profiler.end_benchmark()

        # Count number of multiallelic SNPs
        self.benchmark_profiler.start_benchmark('PCA: Count multiallelic SNPs')
        num_multiallelic_snps = np.count_nonzero(ac.max_allele() > 1)
        self.benchmark_profiler.end_benchmark()

        # Count number of biallelic singletons
        self.benchmark_profiler.start_benchmark('PCA: Count biallelic singletons')
        num_biallelic_singletons = np.count_nonzero((ac.max_allele() == 1) & ac.is_singleton(1))
        self.benchmark_profiler.end_benchmark()

        # Apply filtering to remove singletons and multiallelic SNPs
        flt = (ac.max_allele() == 1) & (ac[:, :2].min(axis=1) > 1)
        flt_count = np.count_nonzero(flt)
        self.benchmark_profiler.start_benchmark('PCA: Remove singletons and multiallelic SNPs')
        if flt_count > 0:
            gf = g.compress(flt, axis=0)
        else:
            # Don't apply filtering
            print('[Exec][PCA] Cannot remove singletons and multiallelic SNPs as no data would remain. Skipping...')
            gf = g
        self.benchmark_profiler.end_benchmark()

        # Transform genotype data into 2-dim matrix
        self.benchmark_profiler.start_benchmark('PCA: Transform genotype data for PCA')
        gn = gf.to_n_alt()
        self.benchmark_profiler.end_benchmark()

        # Randomly choose subset of SNPs
        n = min(gn.shape[0], self.bench_conf.pca_subset_size)
        vidx = np.random.choice(gn.shape[0], n, replace=False)
        vidx.sort()
        gnr = gn.take(vidx, axis=0)

        # Apply LD pruning to subset of SNPs
        size = self.bench_conf.pca_ld_pruning_size
        step = self.bench_conf.pca_ld_pruning_step
        threshold = self.bench_conf.pca_ld_pruning_threshold
        n_iter = self.bench_conf.pca_ld_pruning_number_iterations

        self.benchmark_profiler.start_benchmark('PCA: Apply LD pruning')
        gnu = self._pca_ld_prune(gnr, size=size, step=step, threshold=threshold, n_iter=n_iter)
        self.benchmark_profiler.end_benchmark()

        # If data is chunked, move to memory for PCA
        self.benchmark_profiler.start_benchmark('PCA: Move data set to memory')
        gnu = gnu[:]
        self.benchmark_profiler.end_benchmark()

        # Run PCA analysis
        pca_num_components = self.bench_conf.pca_number_components
        scaler = self.bench_conf.pca_data_scaler

        # Run conventional PCA analysis
        self.benchmark_profiler.start_benchmark(
            'PCA: Run conventional PCA analysis (scaler: {})'.format(scaler if scaler is not None else 'none'))
        allel.pca(gnu, n_components=pca_num_components, scaler=scaler)
        self.benchmark_profiler.end_benchmark()

        # Run randomized PCA analysis
        self.benchmark_profiler.start_benchmark(
            'PCA: Run randomized PCA analysis (scaler: {})'.format(scaler if scaler is not None else 'none'))
        allel.randomized_pca(gnu, n_components=pca_num_components, scaler=scaler)
        self.benchmark_profiler.end_benchmark()

    @staticmethod
    def _pca_ld_prune(gn, size, step, threshold=.1, n_iter=1):
        blen = size * 10
        for i in range(n_iter):
            loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold, blen=blen)
            n = np.count_nonzero(loc_unlinked)
            n_remove = gn.shape[0] - n
            print(
                '[Exec][PCA][LD Prune] Iteration {}/{}: Retaining {} and removing {} variants.'.format(i + 1,
                                                                                                       n_iter,
                                                                                                       n,
                                                                                                       n_remove))
            gn = gn.compress(loc_unlinked, axis=0)
        return gn
/n/n/ngenomics_benchmarks/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import sys

# Support Python 2.x and 3.x
if sys.version_info[0] >= 3:
    from urllib.request import urlretrieve
else:
    from urllib import urlretrieve

from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc
from genomics_benchmarks import config

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    try:
        pathlib.Path(path).mkdir(parents=True)
    except OSError:  # Catch if directory already exists
        pass


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_profiler=None):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    :type benchmark_runner: core.BenchmarkProfiler
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get fields to extract (for unit testing only)
        fields = conversion_config.fields

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Read VCF file into memory for alt number"")

            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()

            numalt = callset['variants/numalt']

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Determine maximum alt number"")

            alt_number = np.max(numalt)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        if benchmark_profiler is not None:
            benchmark_profiler.start_benchmark(operation_name=""Convert VCF to Zarr"")

        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)

        if benchmark_profiler is not None:
            benchmark_profiler.end_benchmark()


def get_genotype_data(callset, genotype_array_type=config.GENOTYPE_ARRAY_DASK):
    genotype_ref_name = ''

    # Ensure 'calldata' is within the callset
    if 'calldata' in callset:
        # Try to find either GT or genotype in calldata
        if 'GT' in callset['calldata']:
            genotype_ref_name = 'GT'
        elif 'genotype' in callset['calldata']:
            genotype_ref_name = 'genotype'
        else:
            return None
    else:
        return None

    if genotype_array_type == config.GENOTYPE_ARRAY_NORMAL:
        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == config.GENOTYPE_ARRAY_DASK:
        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == config.GENOTYPE_ARRAY_CHUNKED:
        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])
    else:
        return None
/n/n/ntests/test_cli.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run:  
    python -m unittest tests.test_cli 

""""""
import unittest
import sys

try:
    from unittest.mock import patch
except ImportError:
    from mock import patch
from genomics_benchmarks import cli


class TestCommandLineInterface(unittest.TestCase):

    def run_subparser_test(self, subparser_cmd, parameter, expected, default_key=None, default_value=None):
        """""" Tests subparsers for missing arguments and default values. """"""
        testargs = [""prog"", subparser_cmd, ""--"" + parameter, expected]
        with patch.object(sys, 'argv', testargs):
            args = cli.get_cli_arguments()
            self.assertEqual(args[parameter], expected,
                             subparser_cmd + "" subparser did not parse right config file arg."")
            self.assertEqual(args[""command""], subparser_cmd, subparser_cmd + "" command was not interpreted properly"")
            if default_key:
                self.assertEqual(args[default_key], default_value,
                                 subparser_cmd + "" command parser did not setup the right default key "" + default_key +
                                 "" to "" + default_value)

    def test_getting_command_arguments(self):
        """""" Tests for reading args and storing values for running all benchmark options from the command line.""""""
        # Test group 1 -- config
        self.run_subparser_test(""config"", ""output_config"", ""./benchmark.conf"")
        # Test group 2 -- setup
        self.run_subparser_test(""setup"", ""config_file"", ""./benhcmark.conf"")
        # Test group 3 - Tests if it the argparser is setting default values """"""
        self.run_subparser_test(""exec"", ""config_file"", ""./benchmark.conf"")

    def test_parser_expected_failing(self):
        """""" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (""something"") """"""
        testargs = [""prog""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the missing command line argument."")

        testargs = [""prog"", ""something""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the wrong command line argument."")


if __name__ == '__main__':
    unittest.main()
/n/n/ntests/test_core.py/n/nimport unittest
from genomics_benchmarks.core import *
from genomics_benchmarks.config import \
    BenchmarkConfigurationRepresentation, \
    VCFtoZarrConfigurationRepresentation, \
    DataDirectoriesConfigurationRepresentation
from time import sleep
import os
import shutil


class TestCoreBenchmark(unittest.TestCase):
    def test_benchmark_profiler_results(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_profiler_results'
        profiler = BenchmarkProfiler(profiler_label)

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = 'Sleep {} seconds'.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            # Grab benchmark results
            results = profiler.get_benchmark_results()
            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals
            results_operation_name = results.operation_name
            results_run_number = results.run_number

            # Ensure benchmark results match expected values
            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')
            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')
            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')

            i += 1

        # Delete *.psv file created when running benchmark
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_results_psv(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_results_psv'

        # Delete *.psv file created from any previous unit testing
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

        profiler = BenchmarkProfiler(profiler_label)

        operation_name_format = 'Sleep {} seconds'

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = operation_name_format.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            i += 1

        # Read results psv file
        psv_file = '{}.psv'.format(profiler_label)

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)
            num_lines = len(psv_lines)
            num_lines_expected = len(benchmark_times) + 1
            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')

            # Ensure header (first line) of psv file is correct
            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'
            header_actual = psv_lines[0]
            self.assertEqual(header_expected, header_actual)

            # Ensure contents (benchmark data) of psv file is correct
            i = 1
            for line_number in range(1, num_lines):
                content = psv_lines[line_number].split('|')

                # Ensure column count is correct
                num_columns = len(content)
                num_columns_expected = 4
                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')

                # Ensure run number is correct
                run_number_psv = int(content[1])
                run_number_expected = i
                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')

                # Ensure operation name is correct
                operation_name_psv = content[2]
                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])
                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')

                # Ensure execution time is correct
                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals
                execution_time_expected = benchmark_times[i - 1]
                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')

                i += 1

        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Delete *.psv file created when running benchmark
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_simple_aggregations(self):
        test_dir = './tests_temp/'
        benchmark_label = 'test_benchmark_simple_aggregations'
        psv_file = '{}.psv'.format(benchmark_label)

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from any previous unit tests
        if os.path.isfile(psv_file):
            os.remove(psv_file)

        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()
        vcf_to_zar_config.enabled = True

        bench_conf = BenchmarkConfigurationRepresentation()
        bench_conf.vcf_to_zarr_config = vcf_to_zar_config
        bench_conf.benchmark_number_runs = 1
        bench_conf.benchmark_data_input = 'vcf'
        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'
        bench_conf.benchmark_aggregations = True

        data_dirs = DataDirectoriesConfigurationRepresentation()
        data_dirs.vcf_dir = './tests/data/'
        data_dirs.zarr_dir_setup = './tests_temp/zarr/'
        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'
        data_dirs.temp_dir = './tests_temp/temp/'

        # Run the benchmark and ensure nothing fails
        benchmark = Benchmark(bench_conf=bench_conf,
                              data_dirs=data_dirs,
                              benchmark_label='test_benchmark_simple_aggregations')
        benchmark.run_benchmark()

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file
            num_lines = len(psv_lines)
            num_lines_expected = 10
            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')

            psv_operation_names = []

            for psv_line in psv_lines:
                line_split = psv_line.split('|')
                line_cols_actual = len(line_split)
                line_cols_expected = 4

                # Ensure correct number of data columns exist for current line of data
                self.assertEqual(line_cols_expected, line_cols_actual,
                                 msg='Unexpected number of columns in resulting psv file')

                operation_name = line_split[2]
                psv_operation_names.append(operation_name)

            # Ensure all aggregations were run
            test_operation_names = ['Allele Count (All Samples)',
                                    'Genotype Count: Heterozygous per Variant',
                                    'Genotype Count: Homozygous per Variant',
                                    'Genotype Count: Heterozygous per Sample',
                                    'Genotype Count: Homozygous per Sample']

            for test_operation_name in test_operation_names:
                if test_operation_name not in psv_operation_names:
                    self.fail(msg='Operation \""{}\"" was not run during the benchmark.'.format(test_operation_name))
        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from this unit test
        if os.path.isfile(psv_file):
            os.remove(psv_file)

    def test_benchmark_pca(self):
        test_dir = './tests_temp/'
        benchmark_label = 'test_benchmark_pca'
        psv_file = '{}.psv'.format(benchmark_label)

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from any previous unit tests
        if os.path.isfile(psv_file):
            os.remove(psv_file)

        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()
        vcf_to_zar_config.enabled = True

        bench_conf = BenchmarkConfigurationRepresentation()
        bench_conf.vcf_to_zarr_config = vcf_to_zar_config
        bench_conf.benchmark_number_runs = 1
        bench_conf.benchmark_data_input = 'vcf'
        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'
        bench_conf.benchmark_pca = True
        bench_conf.pca_data_scaler = config.benchmark_pca_data_scaler_types[config.PCA_DATA_SCALER_PATTERSON]
        bench_conf.pca_genotype_array_type = config.GENOTYPE_ARRAY_CHUNKED

        data_dirs = DataDirectoriesConfigurationRepresentation()
        data_dirs.vcf_dir = './tests/data/'
        data_dirs.zarr_dir_setup = './tests_temp/zarr/'
        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'
        data_dirs.temp_dir = './tests_temp/temp/'

        # Run the benchmark and ensure nothing fails
        benchmark = Benchmark(bench_conf=bench_conf,
                              data_dirs=data_dirs,
                              benchmark_label=benchmark_label)
        benchmark.run_benchmark()

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file
            num_lines = len(psv_lines)
            num_lines_expected = 14
            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')

            psv_operation_names = []

            for psv_line in psv_lines:
                line_split = psv_line.split('|')
                line_cols_actual = len(line_split)
                line_cols_expected = 4

                # Ensure correct number of data columns exist for current line of data
                self.assertEqual(line_cols_expected, line_cols_actual,
                                 msg='Unexpected number of columns in resulting psv file')

                operation_name = line_split[2]
                psv_operation_names.append(operation_name)

            # Ensure all aggregations were run
            test_operation_names = ['PCA: Count alleles',
                                    'PCA: Count multiallelic SNPs',
                                    'PCA: Count biallelic singletons',
                                    'PCA: Remove singletons and multiallelic SNPs',
                                    'PCA: Transform genotype data for PCA',
                                    'PCA: Apply LD pruning',
                                    'PCA: Move data set to memory',
                                    'PCA: Run conventional PCA analysis (scaler: patterson)',
                                    'PCA: Run randomized PCA analysis (scaler: patterson)']

            for test_operation_name in test_operation_names:
                if test_operation_name not in psv_operation_names:
                    self.fail(msg='Operation \""{}\"" was not run during the benchmark.'.format(test_operation_name))
        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from this unit test
        if os.path.isfile(psv_file):
            os.remove(psv_file)


if __name__ == ""__main__"":
    unittest.main()
/n/n/n",0
83,e5195bc7bcf1060f2f727acf9f0cee033262caaa,"/genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_aggregations = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_aggregations"" in runtime_config.benchmark:
                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[""benchmark_aggregations""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/n",1
84,42b020edfe6b23b245938d23ff7a0484333d6450,"evproxy.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
import wzrpc
from sup.ticker import Ticker

class EvaluatorProxy:
    def __init__(self, ev_init, *args, **kvargs):
        super().__init__()
        self.ev_init = ev_init
        self.bind_kt_ticker = Ticker()
        self.bind_kt = 5

    def handle_evaluate(self, reqid, interface, method, data):
        domain, page = data
        self.p.log.info('Recvd page %s, working on', reqid)
        res = self.ev.solve_capage(domain, page)
        self.p.log.info('Done, sending answer: %s', res)
        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])

    def send_keepalive(self):
        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],
            self.handle_keepalive_reply)
        msg.insert(0, b'')
        self.p.wz_sock.send_multipart(msg)

    def handle_keepalive_reply(self, reqid, seqnum, status, data):
        if status == wzrpc.status.success:
            self.p.log.debug('Keepalive was successfull')
        elif status == wzrpc.status.e_req_denied:
            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.
                format(wzrpc.name_status(status)))
            self.p.auth_requests()
            self.p.bind_methods()
        elif status == wzrpc.status.e_timeout:
            self.p.log.warn('Keepalive timeout')
        else:
            self.p.log.warn('Keepalive status {0}'.
                format(wzrpc.name_status(status)))

    def __call__(self, parent):
        self.p = parent
        self.p.wz_connect()
        self.p.wz_auth_requests = [
            (b'Router', b'auth-bind-route'),
            (b'Router', b'auth-unbind-route'),
            (b'Router', b'auth-set-route-type')]
        self.p.wz_bind_methods = [
            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]
        self.p.auth_requests()
        self.p.bind_methods()
        self.ev = self.ev_init()
        self.bind_kt_ticker.tick()
        while self.p.running.is_set():
            self.p.poll()
            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:
                self.bind_kt_ticker.tick()
                self.send_keepalive()
/n/n/nlib/wzrpc/wzbase.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from . import *

class WZBase(object):
    def make_error_msg(self, iden, status):
        msg = []
        if iden:
            msg.extend(iden)
            msg.append(b'')
        msg.append(header_struct.pack(wzstart, wzversion, msgtype.err))
        msg.append(error_struct.pack(status))
        return msg

    def parse_msg(self, iden, msg):
        if len(msg) == 0 or not msg[0].startswith(wzstart):
            raise WZENoWZ('Not a WZRPC message {0} from {1}'.format(msg, repr(iden)))
        try:
            hsize = header_struct.size # locals are faster
            wz, ver, type_ = header_struct.unpack(msg[0][:hsize])
        except Exception as e:
            raise
        if int(ver) != wzversion:
            raise WZEWrongVersion(iden, 'Wrong message version')
        if type_ == msgtype.req:
            unpacked = []
            for v in req_struct.unpack(msg[0][hsize:]):
                if type(v) == bytes:
                    v = v.partition(b'\0')[0]
                unpacked.append(v)
            return self._parse_req(iden, msg, *unpacked)
        elif type_ == msgtype.rep:
            unpacked = rep_struct.unpack(msg[0][hsize:])
            return self._parse_rep(iden, msg, *unpacked)
        elif type_ == msgtype.sig:
            unpacked = []
            for v in sig_struct.unpack(msg[0][hsize:]):
                if type(v) == bytes:
                    v = v.partition(b'\0')[0]
                unpacked.append(v)
            return self._parse_sig(iden, msg, *unpacked)
        elif type_ == msgtype.err:
            unpacked = error_struct.unpack(msg[0][hsize:])
            return self._parse_err(iden, msg, *unpacked)
        elif type_ == msgtype.nil:
            return self._handle_nil(iden, msg)
        else:
            raise WZEUnknownType(iden, 'Unknown message type')
        
    def parse_router_msg(self, frames):
        base, msg = split_frames(frames)
        return self.parse_msg(base[:-1], msg)
/n/n/nlib/wzrpc/wzhandler.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from . import *
from .wzbase import WZBase

class WZHandler(WZBase):
    def __init__(self):
        self.req_handlers = {}
        self.response_handlers = {}
        self.sig_handlers = {}
        self.iden_reqid_map = BijectiveSetMap()

    def set_req_handler(self, interface, method, fun):
        self.req_handlers[(interface, method)] = fun

    def set_response_handler(self, reqid, fun):
        self.response_handlers[reqid] = fun

    def set_sig_handler(self, interface, method, fun):
        self.sig_handlers[(interface, method)] = fun

    def del_req_handler(self, interface, method):
        del self.req_handlers[(interface, method)]

    def del_response_handler(self, reqid):
        del self.response_handlers[reqid]

    def del_sig_handler(self, interface, method):
        del self.sig_handlers[(interface, method)]

    def _parse_req(self, iden, msg, reqid, interface, method):
        try:
            handler = self.req_handlers[(interface, method)]
        except KeyError:
            try:
                handler = self.req_handlers[(interface, None)]
            except KeyError:
                raise WZENoReqHandler(iden, reqid,
                    'No req handler for %s,%s'%(interface, method))
        if iden:
            self.iden_reqid_map.add_value(tuple(iden), reqid)
        handler(reqid, interface, method, msg[1:])
        return ()

    def _parse_rep(self, iden, msg, reqid, seqnum, status):
        try:
            handler = self.response_handlers[reqid]
            if seqnum == 0:
                del self.response_handlers[reqid]
        except KeyError:
            raise WZENoHandler(iden, 'No rep handler for reqid')
        handler(reqid, seqnum, status, msg[1:])
        return ()

    def _parse_sig(self, iden, msg, interface, method):
        try:
            handler = self.sig_handlers[(interface, method)]
        except KeyError:
            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))
        handler(interface, method, msg[1:])
        return ()

    def make_req_msg(self, interface, method, args, fun, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        msg = make_req_msg(interface, method, args, reqid)
        self.set_response_handler(reqid, fun)
        return msg

    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):
        msg = iden[:]
        msg.append(b'')
        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))
        return msg

    def make_router_rep_msg(self, reqid, seqnum, status, answer):
        iden = self.iden_reqid_map.get_key(reqid)
        if seqnum == 0:
            self.iden_reqid_map.del_value(iden, reqid)
        msg = list(iden)
        msg.append(b'')
        msg.extend(make_rep_msg(reqid, seqnum, status, answer))
        return msg

    def get_iden(self, reqid):
        return self.iden_reqid_map.get_key(reqid)

    def get_reqids(self, iden):
        return self.iden_reqid_map.get_values(iden)

    def make_reqid(self):
        while True:
            reqid = random.randint(1, (2**64)-1)
            if reqid not in self.response_handlers:
                return reqid

    def make_auth_req_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-request', args, reqid)

    def make_auth_bind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-bind-route', args, reqid)

    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-unbind-route', args, reqid)

    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, struct.pack('!B', type_),
                make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-set-route-type', args, reqid)

    def make_auth_clear_data(self, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        return (b'Router', b'auth-clear', [], reqid)

    def req_from_data(self, d, fun):
        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])

    def _parse_err(self, iden, msg, status):
        pass

    def _handle_nil(self, iden, msg):
        pass
/n/n/nlib/wzworkers.py/n/nimport zmq
import threading, multiprocessing
import logging
from sup.ticker import Ticker
# from sup import split_frames
import wzrpc
import exceptions
from wzrpc.wzhandler import WZHandler
import wzauth_data

class WorkerInterrupt(Exception):
    '''Exception to raise when self.running is cleared'''
    def __init__(self):
        super().__init__('Worker was interrupted at runtime')

class Suspend(Exception):
    # if we need this at all.
    '''Exception to raise on suspend signal'''
    def __init__(self, interval, *args, **kvargs):
        self.interval = interval
        super().__init__(*args, **kvargs)

class Resume(Exception):
    '''Exception to raise when suspend sleep is interrupted'''

class WZWorkerBase:
    def __init__(self, wz_addr, fun, args=(), kvargs={},
            name=None, start_timer=None, poll_timeout=None,
            pargs=(), pkvargs={}):
        super().__init__(*pargs, **pkvargs)
        self.name = name if name else type(self).__name__
        self.start_timer = start_timer
        self.poll_timeout = poll_timeout if poll_timeout else 5*1000
        self.call = (fun, args, kvargs)

        self.wz_addr = wz_addr
        self.wz_auth_requests = []
        self.wz_bind_methods = []
        self.wz_poll_timeout = 30 * 1000
        self.wz_retry_timeout = 5

    def __sinit__(self):
        '''Initializes thread-local interface on startup'''
        self.log = logging.getLogger(self.name)
        self.running = threading.Event()
        self.sleep_ticker = Ticker()
        self.poller = zmq.Poller()

        s = self.ctx.socket(zmq.SUB)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        s.connect(self.sig_addr)
        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')
        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')
        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))
        self.sig_sock = s

        s = self.ctx.socket(zmq.DEALER)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        self.wz_sock = s

        self.wz = WZHandler()

        def term_handler(i, m, d):
            self.log.info(
                'Termination signal %s recieved',
                repr((i, m, d)))
            self.term()
            raise WorkerInterrupt()
        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)

        def execute_handler(i, m, d):
            if len(d) < 1:
                return
            try:
                exec(d[0].decode('utf-8'))
            except Exception as e:
                self.log.exception(e)
        self.wz.set_sig_handler(b'WZWorker', b'execute', execute_handler)

        def suspend_handler(i, m, d):
            if len(d) != 1:
                self.log.waring('Suspend signal without a time recieved, ignoring')
            self.log.info('Suspend signal %s recieved', repr((i, m, d)))
            try:
                t = int(d[0])
                # raise Suspend(t)
                self.inter_sleep(t)
            except Resume as e:
                self.log.info(e)
            except Exception as e:
                self.log.error(e)
        self.wz.set_sig_handler(b'WZWorker', b'suspend', suspend_handler)

        def resume_handler(i, m, d):
            self.log.info('Resume signal %s recieved', repr((i, m, d)))
            raise Resume()
        self.wz.set_sig_handler(b'WZWorker', b'resume', resume_handler)

        self.running.set()

    def wz_connect(self):
        self.wz_sock.connect(self.wz_addr)

    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):
        s, p, t = self.wz_sock, self.poll, self.sleep_ticker
        timeout = timeout if timeout else self.wz_poll_timeout
        rs = wzrpc.RequestState(fun)
        msg = self.wz.make_req_msg(interface, method, data,
                                   rs.accept, reqid)
        msg.insert(0, b'')
        s.send_multipart(msg)
        t.tick()
        while self.running.is_set():
            p(timeout*1000)
            if rs.finished:
                if rs.retry:
                    self.inter_sleep(self.wz_retry_timeout)
                    msg = self.wz.make_req_msg(interface, method, data,
                        rs.accept, reqid)
                    msg.insert(0, b'')
                    s.send_multipart(msg)
                    rs.finished = False
                    rs.retry = False
                    continue
                return
            elapsed = t.elapsed(False)
            if elapsed >= timeout:
                t.tick()
                # Notify fun about the timeout
                rs.accept(None, 0, 255, [elapsed])
                # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()

    def wz_multiwait(self, requests):
        # TODO: rewrite the retry loop
        s, p, t = self.wz_sock, self.poll, self.sleep_ticker
        timeout = self.wz_poll_timeout
        rslist = []
        msgdict = {}
        for request in requests:
            rs = wzrpc.RequestState(request[0])
            rslist.append(rs)
            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],
                                    rs.accept, request[1][3])
            msg.insert(0, b'')
            msgdict[rs] = msg
            s.send_multipart(msg)
        while self.running.is_set():
            flag = 0
            for rs in rslist:
                if rs.finished:
                    if not rs.retry:
                        del msgdict[rs]
                        continue
                    s.send_multipart(msgdict[rs])
                    rs.finished = False
                    rs.retry = False
                flag = 1
            if not flag:
                return
            # check rs before polling, since we don't want to notify finished one
            # about the timeout
            t.tick()
            p(timeout*1000)
            if t.elapsed(False) >= timeout:
                for rs in rslist:
                    if not rs.finished:
                        rs.accept(None, 0, 255, []) # Notify fun about the timeout
                        rs.finished = True # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()

    def auth_requests(self):
        for i, m in self.wz_auth_requests:
            def accept(that, reqid, seqnum, status, data):
                if status == wzrpc.status.success:
                    self.log.debug('Successfull auth for (%s, %s)', i, m)
                elif status == wzrpc.status.e_auth_wrong_hash:
                    raise exceptions.PermanentError(
                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\
                        format(i, m, wzrpc.name_status(status), repr(data)))
                elif wzrpc.status.e_timeout:
                    self.log.warn('Timeout {0}, retrying'.format(data[0]))
                    that.retry = True
                else:
                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,
                        wzrpc.name_status(status), repr(data))
            self.wz_wait_reply(accept,
                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))


    def bind_route(self, i, m, f):
        self.log.debug('Binding %s,%s route', i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.wz.set_req_handler(i, m, f)
                self.log.debug('Succesfully binded route (%s, %s)', i, m)
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            elif wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))

    def set_route_type(self, i, m, t):
        self.log.debug('Setting %s,%s type to %d', i, m, t)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,
                    wzrpc.name_route_type(t))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_set_route_type_data(i, m, t,
                wzauth_data.set_route_type[i, m]))

    def unbind_route(self, i, m):
        if not (i, m) in self.wz.req_handlers:
            self.log.debug('Route %s,%s was not bound', i, m)
            return
        self.log.debug('Unbinding route %s,%s', i, m)
        self.wz.del_req_handler(i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Route unbinded for (%s, %s)', i, m)
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))

    def clear_auth(self):
        self.log.debug('Clearing our auth records')
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Auth records on router were cleared')
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())

    def bind_methods(self):
        for i, m, f, t in self.wz_bind_methods:
            self.set_route_type(i, m, t)
            self.bind_route(i, m, f)

    def unbind_methods(self):
        for i, m, f, t in self.wz_bind_methods:
            self.unbind_route(i, m)
        # self.clear_auth()

    def send_rep(self, reqid, seqnum, status, data):
        self.wz_sock.send_multipart(
            self.wz.make_router_rep_msg(reqid, seqnum, status, data))

    def send_success_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.success, data)

    def send_error_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.error, data)

    def send_wz_error(self, reqid, data, seqid=0):
        msg = self.wz.make_dealer_rep_msg(
            reqid, seqid, wzrpc.status.error, data)
        self.wz_sock.send_multipart(msg)

    def send_to_router(self, msg):
        msg.insert(0, b'')
        self.wz_sock.send_multipart(msg)
    
    # def bind_sig_route(self, routetype, interface, method, fun):
    #     self.log.info('Binding %s,%s as type %d signal route',
    #                   interface, method, routetype)
    #     self.wz.set_signal_handler(interface, method, fun)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    # def unbind_sig_route(self, interface, method):
    #     self.log.info('Deleting %s,%s signal route', interface, method)
    #     self.wz.del_signal_handler(interface, method)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    def inter_sleep(self, timeout):
        self.sleep_ticker.tick()
        while self.sleep_ticker.elapsed(False) < timeout:
            try:
                self.poll(timeout * 1000)
            except Resume:
                return

    def poll(self, timeout=None):
        try:
            socks = dict(self.poller.poll(timeout if timeout is not None
                else self.poll_timeout))
        except zmq.ZMQError as e:
            self.log.error(e)
            return
        if socks.get(self.sig_sock) == zmq.POLLIN:
            # No special handling or same-socket replies are necessary for signals.
            # Backwards socket replies may be added here.
            frames = self.sig_sock.recv_multipart()
            try:
                self.wz.parse_msg(frames[0], frames[1:])
            except wzrpc.WZError as e:
                self.log.warn(e)
        if socks.get(self.wz_sock) == zmq.POLLIN:
            self.process_wz_msg(self.wz_sock.recv_multipart())
        return socks

    def process_wz_msg(self, frames):
        try:
            for nfr in self.wz.parse_router_msg(frames):
                # Send replies from the handler, for cases when its methods were rewritten
                self.wz_sock.send_multipart(nfr)
        except wzrpc.WZErrorRep as e:
            self.log.info(e)
            self.wz_sock.send_multipart(e.rep_msg)
        except wzrpc.WZError as e:
            self.log.warn(e)

    def run(self):
        self.__sinit__()
        if self.start_timer:
            self.inter_sleep(self.start_timer)
        if self.running:
            self.log.info('Starting')
            try:
                self.child = self.call[0](*self.call[1], **self.call[2])
                self.child(self)
            except WorkerInterrupt as e:
                self.log.warn(e)
            except Exception as e:
                self.log.exception(e)
            self.log.info('Terminating')
        else:
            self.log.info('Aborted')
        self.running.set() # wz_multiwait needs this to avoid another state check.
        self.unbind_methods()
        self.running.clear()
        self.wz_sock.close()
        self.sig_sock.close()

    def term(self):
        self.running.clear()


class WZWorkerThread(WZWorkerBase, threading.Thread):
    def start(self, ctx, sig_addr, *args, **kvargs):
        self.ctx = ctx
        self.sig_addr = sig_addr
        threading.Thread.start(self, *args, **kvargs)

class WZWorkerProcess(WZWorkerBase, multiprocessing.Process):
    def start(self, sig_addr, *args, **kvargs):
        self.sig_addr = sig_addr
        multiprocessing.Process.start(self, *args, **kvargs)

    def __sinit__(self):
        self.ctx = zmq.Context()
        super().__sinit__()
/n/n/nunistart.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -*- mode: python -*-
import sys
if 'lib' not in sys.path:
    sys.path.append('lib')
import os, signal, logging, threading, re, traceback, time
import random
import zmq
from queue import Queue
import sup
import wzworkers as workers
from dataloader import DataLoader
from uniwipe import UniWipe
from wipeskel import *
import wzrpc
from beon import regexp
import pickle

from logging import config
from logconfig import logging_config
config.dictConfig(logging_config)
logger = logging.getLogger()

ctx = zmq.Context()
sig_addr = 'ipc://signals'
sig_sock = ctx.socket(zmq.PUB)
sig_sock.bind(sig_addr)

# Settings for you
domains = set() # d.witch_domains
targets = dict() # d.witch_targets
protected = set() # will be removed later
forums = dict() # target forums

# from lib import textgen
# with open('data.txt', 'rt') as f:
#     model = textgen.train(f.read())
# def mesasge():
#     while True:
#         s = textgen.generate_sentence(model)
#         try:
#             s.encode('cp1251')
#             break
#         except Exception:
#             continue
#     return s

def message():
    msg = []
    # msg.append('[video-youtube-'+
    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',
    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))
    #            +']')
    msg.append('[image-original-none-http://simg4.gelbooru.com/'
               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')
    msg.append('     .')
    # msg.append('[video-youtube-'+random.choice(
    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',
    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',
    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))
    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))
    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',
    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))
    # +']')
    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg
    # msg.append('Enjoy the view!')
    msg.append(str(random.randint(0, 9999999999)))
    return '\n'.join(msg)

def sbjfun():
    # return 'Out of the darkness we will rise, into the light we will dwell'
    return sup.randstr(1, 30)

# End
import argparse

parser = argparse.ArgumentParser(add_help=True)
parser.add_argument('--only-cache', '-C', action='store_true',
    help=""Disables any requests in DataLoader (includes Witch)"")
parser.add_argument('--no-shell', '-N', action='store_true',
    help=""Sleep instead of starting the shell"")
parser.add_argument('--tcount', '-t', type=int, default=10,
    help='WipeThread count')
parser.add_argument('--ecount', '-e', type=int, default=0,
    help='EvaluatorProxy count')
parser.add_argument('--upload-avatar', action='store_true', default=False,
    help='Upload random avatar after registration')
parser.add_argument('--av-dir', default='randav', help='Directory with avatars')
parser.add_argument('--rp-timeout', '-T', type=int, default=10,
    help='Default rp timeout in seconds')
parser.add_argument('--conlimit', type=int, default=3,
    help='http_request conlimit')
parser.add_argument('--noproxy-timeout', type=int, default=5,
    help='noproxy_rp timeout')

parser.add_argument('--caprate_minp', type=int, default=5,
    help='Cap rate minimum possible count for limit check')
parser.add_argument('--caprate_limit', type=float, default=0.8,
    help='Captcha rate limit')

parser.add_argument('--comment_successtimeout', type=float, default=0.8,
    help='Comment success timeout')
parser.add_argument('--topic_successtimeout', type=float, default=0.1,
    help='Topic success timeout')
parser.add_argument('--errortimeout', type=float, default=3,
    help='Error timeout')


parser.add_argument('--stop-on-closed', action='store_true', default=False,
    help='Forget about closed topics')
parser.add_argument('--die-on-neterror', action='store_true', default=False,
    help='Terminate spawn in case of too many NetErrors')

c = parser.parse_args()

# rps = {}

noproxy_rp = sup.net.RequestPerformer()
noproxy_rp.proxy = ''
noproxy_rp.timeout = c.noproxy_timeout
noproxy_rp.timeout = c.rp_timeout

# rps[''] = noproxy_rp

# Achtung: DataLoader probably isn't thread-safe.
d = DataLoader(noproxy_rp, c.only_cache)
c.router_addr = d.addrs['rpcrouter']
noproxy_rp.useragent = random.choice(d.ua_list)

def terminate():
    logger.info('Shutdown initiated')
    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])
    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])
    for t in threading.enumerate():
        if isinstance(t, threading.Timer):
            t.cancel()
    # try:
    #     wm.term()
    #     wm.join()
    # except: # WM instance is not created yet.
    #     pass
    logger.info('Exiting')

def interrupt_handler(signal, frame):
    pass # Just do nothing

def terminate_handler(signal, frame):
    terminate()

signal.signal(signal.SIGINT, interrupt_handler)
signal.signal(signal.SIGTERM, terminate_handler)

def make_net(proxy, proxytype):
    # if proxy in rps:
    #     return rps[proxy]
    net = sup.net.RequestPerformer()
    net.proxy = proxy
    if proxytype == 'HTTP' or proxytype == 'HTTPS':
        net.proxy_type = sup.proxytype.http
    elif proxytype == 'SOCKS4':
        net.proxy_type = sup.proxytype.socks4
    elif proxytype == 'SOCKS5':
        net.proxy_type = sup.proxytype.socks5
    else:
        raise TypeError('Invalid proxytype %s' % proxytype)
    # rps[proxy] = net
    net.useragent = random.choice(d.ua_list)
    net.timeout = c.rp_timeout
    return net

# UniWipe patching start
def upload_avatar(self, ud):
    if ('avatar_uploaded' in ud[0] and
        ud[0]['avatar_uploaded'] is True):
        return
    files = []
    for sd in os.walk(c.av_dir):
        files.extend(sd[2])
    av = os.path.join(sd[0], random.choice(files))
    self.log.info('Uploading %s as new avatar', av)
    self.site.uploadavatar('0', av)
    ud[0]['avatar'] = av
    ud[0]['avatar_uploaded'] = True

from lib.mailinator import Mailinator
# from lib.tempmail import TempMail as Mailinator

# Move this to WipeManager
def create_spawn(proxy, proxytype, pc, uq=None):
    for domain in domains:
        if domain in targets:
            tlist = targets[domain]
        else:
            tlist = list()
            targets[domain] = tlist
        if domain in forums:
            fset = forums[domain]
        else:
            fset = set()
            forums[domain] = fset
        net = make_net(proxy, proxytype)
        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain
        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,
            uq(domain) if uq else None)
        w.stoponclose = c.stop_on_closed
        w.die_on_neterror = c.die_on_neterror
        w.caprate_minp = c.caprate_minp
        w.caprate_limit = c.caprate_limit
        w.conlimit = c.conlimit
        w.comment_successtimeout = 0.2
        if c.upload_avatar:
            w.hooks['post_login'].append(upload_avatar)
        yield w

# UniWipe patching end

class WipeManager:
    def __init__(self, config, *args, **kvargs):
        super().__init__(*args, **kvargs)
        self.newproxyfile = 'newproxies.txt'
        self.proxylist = set()
        self.c = config
        self.threads = []
        self.processes = []
        self.th_sa = 'inproc://wm-wth.sock'
        self.th_ba = 'inproc://wm-back.sock'
        self.pr_sa = 'ipc://wm-wpr.sock'
        self.pr_ba = 'ipc://wm-back.sock'
        self.userqueues = {}
        self.usersfile = 'wm_users.pickle'
        self.targetsfile = 'wm_targets.pickle'
        self.bumplimitfile = 'wm_bumplimit.pickle'

    def init_th_sock(self):
        self.log.info(
            'Initializing intraprocess signal socket %s', self.th_sa)
        self.th_sock = self.p.ctx.socket(zmq.PUB)
        self.th_sock.bind(self.th_sa)

    def init_th_back_sock(self):
        self.log.info(
            'Initializing intraprocess backward socket %s', self.th_ba)
        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.th_back_sock.bind(self.th_ba)

    def init_pr_sock(self):
        self.log.info(
            'Initializing interprocess signal socket %s', self.pr_sa)
        self.pr_sock = self.p.ctx.socket(zmq.PUB)
        self.pr_sock.bind(self.pr_sa)

    def init_pr_back_sock(self):
        self.log.info(
            'Initializing interprocess backward socket %s', self.pr_ba)
        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.pr_back_sock.bind(self.pr_ba)

    def read_newproxies(self):
        if not os.path.isfile(self.newproxyfile):
            return
        newproxies = set()
        with open(self.newproxyfile, 'rt') as f:
            for line in f:
                try:
                    line = line.rstrip('\n')
                    proxypair = tuple(line.split(' '))
                    if len(proxypair) < 2:
                        self.log.warning('Line %s has too few spaces', line)
                        continue
                    if len(proxypair) > 2:
                        self.log.debug('Line %s has too much spaces', line)
                        proxypair = (proxypair[0], proxypair[1])
                    newproxies.add(proxypair)
                except Exception as e:
                    self.log.exception('Line %s raised exception %s', line, e)
        # os.unlink(self.newproxyfile)
        return newproxies.difference(self.proxylist)

    def add_spawns(self, proxypairs):
        while self.running.is_set():
            try:
                try:
                    proxypair = proxypairs.pop()
                except Exception:
                    return
                self.proxylist.add(proxypair)
                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,
                        self.get_userqueue):
                    self.log.info('Created spawn %s', spawn.name)
                    self.spawnqueue.put(spawn, False)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on create_spawn', e)

    def spawn_workers(self, wclass, count, args=(), kvargs={}):
        wname = str(wclass.__name__)
        self.log.info('Starting %s(s)', wname)
        if issubclass(wclass, workers.WZWorkerThread):
            type_ = 0
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif issubclass(wclass, workers.WZWorkerProcess):
            type_ = 1
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                w = wclass(*args, name='.'.join(
                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),
                    **kvargs)
                if type_ == 0:
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):
        wname = str(fun.__name__)
        self.log.info('Starting %s(s)', wname)
        if type_ == 0:
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif type_ == 1:
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                if type_ == 0:
                    w = workers.WZWorkerThread(
                        self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'th{0}'.format(i))))
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'pr{0}'.format(i))))
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_wipethreads(self):
        return self.spawn_nworkers(0, WipeThread, self.c.tcount,
                                  (self.pc, self.spawnqueue))

    def spawn_evaluators(self):
        self.log.info('Initializing Evaluator')
        from evproxy import EvaluatorProxy
        def ev_init():
            from lib.evaluators.PyQt4Evaluator import Evaluator
            return Evaluator()
        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,
                                  (ev_init,))

    def load_users(self):
        if not os.path.isfile(self.usersfile):
            return
        with open(self.usersfile, 'rb') as f:
            users = pickle.loads(f.read())
        try:
            for domain in users.keys():
                uq = Queue()
                for ud in users[domain]:
                    self.log.debug('Loaded user %s:%s', domain, ud['login'])
                    uq.put(ud)
                self.userqueues[domain] = uq
        except Exception as e:
            self.log.exception(e)
            self.log.error('Failed to load users')

    def save_users(self):
        users = {}
        for d, uq in self.userqueues.items():
            uqsize = uq.qsize()
            uds = []
            for i in range(uqsize):
                uds.append(uq.get(False))
            users[d] = uds
        with open(self.usersfile, 'wb') as f:
            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))
        self.log.info('Saved users')

    def get_userqueue(self, domain):
        try:
            uq = self.userqueues[domain]
        except KeyError:
            self.log.info('Created userqueue for %s', domain)
            uq = Queue()
            self.userqueues[domain] = uq
        return uq

    def load_targets(self):
        fname = self.targetsfile
        if not os.path.isfile(fname):
            return
        with open(fname, 'rb') as f:
            data = pickle.loads(f.read())
        if 'targets' in data:
            self.log.debug('Target list was loaded')
            targets.update(data['targets'])
        if 'forums' in data:
            self.log.debug('Forum set was loaded')
            forums.update(data['forums'])
        if 'domains' in data:
            self.log.debug('Domain set was loaded')
            domains.update(data['domains'])
        if 'sets' in data:
            self.log.debug('Other sets were loaded')
            self.pc.sets.update(data['sets'])

    def load_bumplimit_set(self):
        if not os.path.isfile(self.bumplimitfile):
            return
        with open(self.bumplimitfile, 'rb') as f:
            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))

    def save_targets(self):
        data = {
            'targets': targets,
            'forums': forums,
            'domains': domains,
            'sets': self.pc.sets,
        }
        with open(self.targetsfile, 'wb') as f:
            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))

    def targets_from_witch(self):
        for t in d.witch_targets:
            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':
                try:
                    add_target_exc(t['id'], t['user'])
                except ValueError:
                    pass

    def terminate(self):
        msg = [b'GLOBAL']
        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))
        if hasattr(self, 'th_sock'):
            self.th_sock.send_multipart(msg)
        if hasattr(self, 'pr_sock'):
            self.pr_sock.send_multipart(msg)

    def join_threads(self):
        for t in self.threads:
            t.join()

    def send_passthrough(self, interface, method, frames):
        msg = [frames[0]]
        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
        self.th_sock.send_multipart(msg)
        self.pr_sock.send_multipart(msg)

    def __call__(self, parent):
        self.p = parent
        self.log = parent.log
        self.inter_sleep = parent.inter_sleep
        self.running = parent.running
        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')
        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)
        if self.c.tcount > 0:
            self.pc = ProcessContext(self.p.name, self.p.ctx,
                self.c.router_addr, noproxy_rp)
            self.spawnqueue = Queue()
            self.load_bumplimit_set()
            self.load_targets()
            self.load_users()
            self.spawn_wipethreads()
        if self.c.ecount > 0:
            self.spawn_evaluators()
        try:
            while self.running.is_set():
                # self.targets_from_witch()
                if self.c.tcount == 0:
                    self.inter_sleep(5)
                    continue
                self.pc.check_waiting()
                new = self.read_newproxies()
                if not new:
                    self.inter_sleep(5)
                    continue
                self.add_spawns(new)
        except WorkerInterrupt:
            pass
        except Exception as e:
            self.log.exception(e)
        self.terminate()
        self.join_threads()
        if self.c.tcount > 0:
            self.save_users()
            self.save_targets()

wm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),
    name='SpaghettiMonster')
wm.start(ctx, sig_addr)

def add_target(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Appending %s to targets[%s]', repr(t), domain)
    tlist.append(t)

def remove_target(domain, id_, tuser=None):
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Removing %s from targets[%s]', repr(t), domain)
    tlist.remove(t)

def add_target_exc(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    if t in protected:
        raise ValueError('%s is protected' % repr(t))
    if t not in tlist:
        logger.info('Appending %s to targets[%s]', repr(t), domain)
        tlist.append(t)

r_di = re.compile(regexp.f_udi)

def atfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        add_target(domain, id_, user)

def rtfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        remove_target(domain, id_, user)

def get_forum_id(name):
    id_ = d.bm_id_forum.get_key(name)
    int(id_, 10)  # id is int with base 10
    return id_

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s (%s) to forums', name, id_)
#     forums.append(id_)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s (%s) from forums', name, id_)
#     forums.remove(id_)

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s to forums', name)
#     forums.add(name)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s from forums', name)
#     forums.remove(name)

r_udf = re.compile(regexp.udf_prefix)

def affu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if domain not in forums:
            forums[domain] = set()
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)
        forums[domain].add((user, forum))

def rffu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)
        forums[domain].remove((user, forum))

def add_user(domain, login, passwd):
    uq = wm.get_userqueue(domain)
    uq.put({'login': login, 'passwd': passwd}, False)

def send_to_wm(frames):
    msg = [frames[0]]
    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
    sig_sock.send_multipart(msg)

def send_passthrough(frames):
    msg = [b'WipeManager']
    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))
    sig_sock.send_multipart(msg)

def get_pasted_lines(sentinel):
    'Yield pasted lines until the user enters the given sentinel value.'
    print(""Pasting code; enter '{0}' alone on the line to stop."".format(sentinel))
    while True:
        l = input(':')
        if l == sentinel:
            return
        else:
            yield l

def send_execute_to_wm(code):
    msg = [b'WipeManager']
    msg.extend((b'WZWorker', b'execute', code))
    send_to_wm(msg)

def send_execute_to_ev(code):
    msg = [b'EVProxy']
    msg.extend((b'WZWorker', b'execute', code))
    send_passthrough(msg)

def send_execute(name, code):
    msg = [name.encode('utf-8')]
    msg.extend((b'WZWorker', b'execute', code))
    send_passthrough(msg)

def pexecute_in(name):
    send_execute(name, '\n'.join(get_pasted_lines('--')).encode('utf-8'))

def pexecute_in_wm():
    send_execute_to_wm('\n'.join(get_pasted_lines('--')).encode('utf-8'))

def pexecute_in_ev():
    send_execute_to_ev('\n'.join(get_pasted_lines('--')).encode('utf-8'))

def drop_users():
    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])

def log_spawn_name():
    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])

try:
    import IPython
    if c.no_shell:
        IPython.embed_kernel()
    else:
        IPython.embed()
except ImportError:
    # fallback shell
    if c.no_shell:
        while True:
            time.sleep(1)
    else:
        while True:
            try:
                exec(input('> '))
            except KeyboardInterrupt:
                print(""KeyboardInterrupt"")
            except SystemExit:
                break
            except:
                print(traceback.format_exc())

terminate()
/n/n/nuniwipe.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from sup.net import NetError
from wzworkers import WorkerInterrupt
from wipeskel import WipeSkel, WipeState, cstate
from beon import exc, regexp
from collections import ChainMap
import re

class UniWipe(WipeSkel):
    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):
        self.sbjfun = sbjfun
        self.msgfun = msgfun
        self.forums = forums
        self.targets = (type(targets) == str and [('', targets)]
                        or type(targets) == tuple and list(targets)
                        or targets)
        super().__init__(*args, **kvargs)
        self.ignore_map = ChainMap(
            self.pc.sets['closed'], self.pc.sets['bumplimit'],
            self.pc.sets['bugged'], self.pc.sets['protected'],
            self.targets)

    def on_caprate_limit(self, rate):
        if not self.logined:
            self._capdata = (0, 0)
            return
        self.log.warning('Caprate limit reached, calling dologin() for now')
        self.dologin()
        # super().on_caprate_limit(rate)

    def comment_loop(self):
        for t in self.targets:
            self.schedule(self.add_comment, (t, self.msgfun()))
        if len(self.targets) == 0:
            self.schedule(self.scan_targets_loop)
        else:
            self.schedule(self.comment_loop)

    def add_comment(self, t, msg):
        # with cstate(self, WipeState.posting_comment):
        if True: # Just a placeholder
            try:
                # self.counter_tick()
                self.postmsg(t[1], msg, t[0])
            except exc.Success as e:
                self.counters['comments'] += 1
                self.w.sleep(self.comment_successtimeout)
            except exc.Antispam as e:
                self.w.sleep(self.comment_successtimeout)
                self.schedule(self.add_comment, (t, msg))
            except (exc.Closed, exc.UserDeny) as e:
                try:
                    self.targets.remove(t)
                except ValueError:
                    pass
                self.w.sleep(self.comment_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.schedule(self.add_comment, (t, msg))
            except exc.UnknownAnswer as e:
                self.log.warn('%s: %s', e, e.answer)
                self.schedule(self.add_comment, (t, msg))
            except exc.Wait5Min as e:
                self.schedule(self.add_comment, (t, msg))
                self.schedule_first(self.switch_user)
            except exc.EmptyAnswer as e:
                self.log.info('Removing %s from targets and adding to bugged', t)
                self.pc.sets['bugged'].add(t)
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except exc.TopicDoesNotExist as e:
                self.log.info('Removing %s from targets and adding to bugged', t)
                self.pc.sets['bugged'].add(t)
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.schedule(self.add_comment, (t, msg))
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except UnicodeDecodeError as e:
                self.log.exception(e)
                self.w.sleep(self.errortimeout)

    def forumwipe_loop(self):
        for f in self.forums.copy():
            self.counter_tick()
            try:
                self.addtopic(self.msgfun(), self.sbjfun(), f)
            except exc.Success as e:
                self.counters['topics'] += 1
                self.w.sleep(self.topic_successtimeout)
            except exc.Wait5Min as e:
                self.topic_successtimeout = self.topic_successtimeout + 0.1
                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',
                    self.topic_successtimeout)
                self.w.sleep(self.topic_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.long_sleep(10)
            except exc.UnknownAnswer as e:
                self.log.warning('%s: %s', e, e.answer)
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                self.log.error(e)
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.log.warn(e)
                self.w.sleep(self.errortimeout)

    def get_targets(self):
        found_count = 0
        for user, forum in self.forums:
            targets = []
            self.log.debug('Scanning first page of the forum %s:%s', user, forum)
            page = self.site.get_page('1', forum, user)
            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))
            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))
            for t in found:
                if t in self.ignore_map:
                    continue
                targets.append(t)
            lt = len(targets)
            found_count += lt
            if lt > 0:
                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)
            else:
                self.log.debug('Found no new targets in forum %s:%s', user, forum)
            self.targets.extend(targets)
        return found_count

    def scan_targets_loop(self):
        with cstate(self, WipeState.scanning_for_targets):
            while len(self.targets) == 0:
                c = self.get_targets()
                if c == 0:
                    self.log.info('No targets found at all, sleeping for 30 seconds')
                    self.long_sleep(30)
            self.schedule(self.comment_loop)
        if len(self.forums) == 0:
            self.schedule(self.wait_loop)

    def wait_loop(self):
        if len(self.targets) > 0:
            self.schedule(self.comment_loop)
            return
        if len(self.forums) == 0:
            with cstate(self, WipeState.waiting_for_targets):
                while len(self.forums) == 0:
                    # To prevent a busy loop.
                    self.counter_tick()
                    self.w.sleep(1)
        self.schedule(self.scan_targets_loop)

    def _run(self):
        self.schedule(self.dologin)
        self.schedule(self.wait_loop)
        self.schedule(self.counter_ticker.tick)
        try:
            self.perform_tasks()
        except NetError as e:
            self.log.error(e)
        except WorkerInterrupt as e:
            self.log.warning(e)
        except Exception as e:
            self.log.exception(e)
        self.return_user()
# tw_flag = False
# if len(self.targets) > 0:
#     with cstate(self, WipeState.posting_comment):
#         while len(self.targets) > 0:
#             self.threadwipe_loop()
#     if not tw_flag:
#         tw_flag = True
# if tw_flag:
#     # Sleep for topic_successtimeout after last comment
#     # to prevent a timeout spike
#     self.w.sleep(self.topic_successtimeout)
#     tw_flag = False
# with cstate(self, WipeState.posting_topic):
# self.forumwipe_loop()
/n/n/nwipeskel.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
import logging, re
from queue import Queue, Empty
import zmq
import beon, sup, wzrpc
from beon import regexp
from wzworkers import WorkerInterrupt
from ocr import OCRError, PermOCRError, TempOCRError
from sup.ticker import Ticker
from userdata import short_wordsgen
from enum import Enum
from collections import Counter, deque

class ProcessContext:
    def __init__(self, name, ctx, wz_addr, noproxy_rp):
        self.log = logging.getLogger('.'.join((name, type(self).__name__)))
        self.zmq_ctx = ctx
        self.ticker = Ticker()
        self.sets = {}
        self.sets['waiting'] = dict()
        self.sets['pending'] = set()

        self.sets['targets'] = set()
        self.sets['closed'] = set()
        self.sets['bumplimit'] = set()
        self.sets['protected'] = set()
        self.sets['bugged'] = set()

        self.wz_addr = wz_addr
        self.noproxy_rp = noproxy_rp

    def make_wz_sock(self):
        self.log.debug('Initializing WZRPC socket')
        wz_sock = self.zmq_ctx.socket(zmq.DEALER)
        wz_sock.setsockopt(zmq.IPV6, True)
        wz_sock.connect(self.wz_addr)
        return wz_sock

    def check_waiting(self):
        elapsed = self.ticker.elapsed()
        waiting = self.sets['waiting']
        for k, v in waiting.copy().items():
            rem = v - elapsed
            if rem <= 0:
                del waiting[k]
                self.log.info('Removing %s from %s', k[0], k[1])
                try:
                    self.sets[k[1]].remove(k[0])
                except KeyError:
                    self.log.error('No %s in %s', k[0], k[1])
            else:
                waiting[k] = rem

    def add_waiting(self, sname, item, ttl):
        self.sets['waiting'][(item, sname)] = ttl

class WTState(Enum):
    null = 0
    starting = 2
    empty = 3
    sleeping = 4
    running = 5

class WipeState(Enum):
    null = 0
    starting = 2
    terminating = 3
    sleeping = 4
    running = 5

    logging_in = 6
    post_login_hooks = 7
    registering = 8
    pre_register_hooks = 9
    post_register_hooks = 10
    deobfuscating_capage = 11
    solving_captcha = 12
    reporting_code = 13

    operation = 50
    waiting_for_targets = 51
    scanning_for_targets = 52
    posting_comment = 53
    posting_topic = 54

class state:
    def __init__(self, defstate):
        self.defstate = defstate
        self.state = defstate

    def __call__(self, state):
        self.state = state
        return self

    def __enter__(self):
        pass

    def __exit__(self, exception_type, exception_value, traceback):
        self.state = self.defstate

    @property
    def name(self):
        return self.state.name

    @property
    def value(self):
        return self.state.value

class cstate:
    def __init__(self, obj, state):
        self.obj = obj
        self.backstate = obj.state
        self.newstate = state

    def __enter__(self):
        self.obj.log.info('Switching state to %s', repr(self.newstate))
        self.obj.state = self.newstate

    def __exit__(self, exception_type, exception_value, traceback):
        self.obj.log.info('Switching state to %s', repr(self.backstate))
        self.obj.state = self.backstate


class WipeThread:
    def __init__(self, pc, spawnqueue, *args, **kvargs):
        self.pc = pc
        self.spawnqueue = spawnqueue
        self.spawn = None
        self.state = WTState.null
        self.wz_reply = None

    def deobfuscate_capage(self, domain, page):
        result = []
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success or status == wzrpc.status.error:
                result.extend(map(lambda x: x.decode('utf-8'), data))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.
                    format(wzrpc.name_status(status)))
                self.p.auth_requests()
                that.retry = True
            elif status == wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        self.p.wz_wait_reply(accept,
            b'Evaluator', b'evaluate', (domain.encode('utf-8'), page.encode('utf-8')),
            timeout=60)
        return tuple(result)

    def solve_captcha(self, img):
        result = []
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success or status == wzrpc.status.error:
                result.extend(map(lambda x:x.decode('utf-8'), data))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.p.auth_requests()
                that.retry = True
            elif status == wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        self.p.wz_wait_reply(accept,
            b'Solver', b'solve', (b'inbound', img), timeout=300)
        if len(result) == 2: # Lame and redundant check. Rewrite this part someday.
            return result
        else:
            raise OCRError('Solver returned error %s', result)
        return tuple(result)

    def report_code(self, cid, status):
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Successfully reported captcha status')
            elif status == wzrpc.status.error:
                self.log.error('Solver returned error on report: %s', repr(data))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.p.auth_requests()
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        self.p.wz_wait_reply(accept,
            b'Solver', b'report', (status.encode('utf-8'), cid.encode('utf-8')))

    def __call__(self, parent):
        self.p = parent
        self.log = parent.log
        self.running = parent.running
        self.sleep = parent.inter_sleep
        self.p.wz_auth_requests = [
            (b'Evaluator', b'evaluate'),
            (b'Solver', b'solve'),
            (b'Solver', b'report')]
        cst = cstate(self, WTState.starting)
        cst.__enter__()
        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeThread')
        def handle_lsn(interface, method, data):
            if hasattr(self, 'spawn') and self.spawn:
                self.log.info('My current spawn is %s, state %s',
                    self.spawn.name, self.spawn.state.name)
            else:
                self.log.debug('Currently I do not have spawn')
        self.p.wz.set_sig_handler(b'WipeThread', b'log-spawn-name', handle_lsn)
        def handle_te(interface, method, data):
            if self.state is WTState.empty:
                self.p.term()
        self.p.wz.set_sig_handler(b'WipeThread', b'terminate-empty', handle_te)

        try:
            self.p.wz_connect()
            self.p.auth_requests()
        except WorkerInterrupt as e:
            self.log.error(e)
            return
        with cstate(self, WTState.empty):
            while self.running.is_set():
                try:
                    self.spawn = self.spawnqueue.get(False)
                except Empty:
                    self.sleep(1)
                    continue
                with cstate(self, WTState.running):
                    try:
                        self.spawn.run(self)
                    except WorkerInterrupt as e:
                        self.log.error(e)
                    except Exception as e:
                        self.log.exception('Spawn throwed exception %s, requesting new', e)
                    del self.spawn
                    self.spawn = None
                    self.spawnqueue.task_done()
        cst.__exit__(None, None, None)

class WipeSkel(object):
    reglimit = 10
    loglimit = 10
    conlimit = 3
    catrymax = 3
    _capdata = (0, 0)
    caprate = 0
    caprate_minp = 10
    caprate_limit = 0.9
    successtimeout = 1
    comment_successtimeout = 0
    topic_successtimeout = 0.8
    counter_report_interval = 60
    errortimeout = 3
    uqtimeout = 5  # Timeout for userqueue
    stoponclose = True
    die_on_neterror = False

    def __init__(self, pc, rp, domain, mrc, userqueue=None):
        self.pc = pc
        self.rp = rp
        self.state = WipeState.null
        self.site = beon.Beon(domain, self.http_request)
        self.name = '.'.join((
            type(self).__name__,
            self.rp.proxy.replace('.', '_') if self.rp.proxy
            else 'noproxy',
            self.site.domain.replace('.', '_')))
        self.rp.default_encoding = 'cp1251'
        self.rp.default_decoding = 'cp1251'
        self.rp.def_referer = self.site.ref  # Referer for net.py
        self.hooks = {
            'pre_register_new_user': [],
            'post_register_new_user': [],
            'post_login': [],
            'check_new_user': [],
        }
        self.counter_ticker = Ticker()
        self.counters = Counter()
        self.task_deque = deque()
        self.logined = False
        self.noproxy_rp = self.pc.noproxy_rp
        self.mrc = mrc
        if userqueue:
            self.userqueue = userqueue
        else:
            self.userqueue = Queue()

    def schedule(self, task, args=(), kvargs={}):
        self.task_deque.appendleft((task, args, kvargs))

    def schedule_first(self, task, args=(), kvargs={}):
        self.task_deque.append((task, args, kvargs))

    def perform_tasks(self):
        with cstate(self, WipeState.running):
            while self.w.running.is_set():
                self.counter_tick()
                try:
                    t = self.task_deque.pop()
                except IndexError:
                    return
                t[0](*t[1], **t[2])

    def long_sleep(self, time):
        time = int(time)
        with cstate(self, WipeState.sleeping):
            step = int(time/10 if time > 10 else 1)
            for s in range(0, time, step):
                self.w.sleep(step)
                self.counter_tick()

    def http_request(self, url, postdata=None, onlyjar=False, referer=None,
                     encoding=None, decoding=None):
        _conc = 0
        while self.w.running.is_set():
            _conc += 1
            try:
                return self.rp.http_req(
                    url, postdata, onlyjar, referer, encoding, decoding)
            except sup.NetError as e:
                if isinstance(e, sup.ConnError):
                    if self.die_on_neterror and _conc > self.conlimit:
                        raise
                    self.log.warn('%s, waiting. t: %s', e.args[0], _conc)
                    self.w.sleep(self.errortimeout)
                else:
                    self.log.error('%d %s', e.ec, e.args[0])
                    if self.die_on_neterror:
                        raise
                    else:
                        self.w.sleep(10)
        else:
            raise WorkerInterrupt()

    def gen_userdata(self):
        return short_wordsgen()

    def update_caprate(self, got):
        p, g = self._capdata
        p += 1
        if got is True:
            self.counters['captchas'] += 1
            g += 1
        if p >= 255:
            p = p/2
            g = g/2
        self._capdata = (p, g)
        self.caprate = g/p
        self.log.debug('Caprate: pos:%f got:%f rate:%f',
                       p, g, self.caprate)
        if (self.caprate_limit > 0
            and p > self.caprate_minp
            and self.caprate > self.caprate_limit):
            self.on_caprate_limit(self.caprate)
            # if self.getuser() == 'guest':
            #     self.log.info(""lol, we were trying to post from guest"")
            #     while not self.relogin(): self.w.sleep(self.errortimeout)
            # else:
            #     while not self.dologin(): self.w.sleep(self.errortimeout)

    def counter_tick(self):
        if self.counter_report_interval == 0:
            return
        e = self.counter_ticker.elapsed(False)
        if e > self.counter_report_interval:
            self.counter_ticker.tick()
            ccount = self.counters['comments']
            tcount = self.counters['topics']
            if ccount > 0:
                self.log.info('%d comments in %d seconds, %0.2f cps, %0.2f caprate',
                    ccount, e, ccount/e, self.caprate)
                self.counters['comments'] = 0
            if tcount > 0:
                self.log.info('%d topics in %d seconds, %0.2f tps, %0.2f caprate',
                    tcount, e, tcount/e, self.caprate)
                self.counters['topics'] = 0

    def on_caprate_limit(self, rate):
        if not self.logined:
            self._capdata = (0, 0)
            return
        self.log.warn('Caprate %f is over the limit', rate)
        raise Exception('Caprate limit reached')

    def captcha_wrapper(self, inc_fun, fin_fun, *args, **kvargs):
        # TODO: report codes after solving cycle instead of scheduling them.
        try:
            self.log.debug('captcha_wrapper: calling inc_fun %s', repr(inc_fun))
            self.log.error('captcha_wrapper: inc_fun returned %s',
                           repr(inc_fun(*args, **kvargs)))
        except beon.Success as e:
            self.update_caprate(False)
            raise
        except beon.Captcha as e:
            self.log.warn(e)
            _page = e.page
            _catry = e.catry
            # Don't update caprate with positives if not logined
            if self.logined is True:
                try:
                    user = self.find_login(_page)
                except beon.PermanentError:
                    self.log.debug(e)
                else:
                    if user != self.site.ud['login']:
                        self.log.warn('We were posting as %s, but our login is %s',
                                      user, self.site.ud['login'])
                        self.schedule_first(self.relogin)
                        return
            self.update_caprate(True)
            reports = []
            def r():
                if len(reports) > 0:
                    with cstate(self, WipeState.reporting_code):
                        for cid, status in reports:
                            self.report_code(cid, status)
                    reports.clear()
            while self.w.running.is_set():
                _requested_new = False
                try:
                    with cstate(self, WipeState.solving_captcha):
                        cahash, cacode, cid = self.solve_captcha(_page)
                except TempOCRError as e:
                    self.log.error('OCRError: %s, retrying', e)
                    self.w.sleep(self.errortimeout)
                    continue
                except OCRError as e:
                    self.log.error('OCRError: %s, requesting new captcha', e)
                    _requested_new = True
                    cahash, cacode, cid = e.cahash, '', None
                else:
                    self.log.info('code: %s', cacode)
                try:
                    self.log.debug('captcha_wrapper calling fin_fun %s', repr(fin_fun))
                    self.log.error('captcha_wrapper: fin_fun returned %s',
                        repr(fin_fun(cahash, cacode, *args, catry=_catry, **kvargs)))
                    break
                except beon.Success as e:
                    self.counters['captchas_solved'] += 1
                    if cid:
                        reports.append((cid, 'good'))
                    r()
                    raise
                except beon.Captcha as e:
                    _catry = e.catry
                    _page = e.page
                    if _requested_new:
                        self.log.warn('New captcha requested c:%d', _catry)
                        continue
                    self.log.warn('%s c:%d', e, _catry)
                    self.counters['captchas_wrong'] += 1
                    if cid:
                        reports.append((cid, 'bad'))
                    if _catry > self.catrymax:
                        r()
                        raise
                except Exception as e:
                    if cid:
                        reports.append((cid, 'bad'))
                    r()
                    raise

    def adaptive_timeout_wrapper(self, fun, *args, **kvargs):
        try:
            return fun(*args, **kvargs)
        except beon.Antispam as e:
            self.log.info('Antispam exc caught, successtimeout + 0.1, cur: %f',
                          self.successtimeout)
            self.successtimeout = self.successtimeout + 0.1
            raise

    def register_new_user(self):
        with cstate(self, WipeState.registering):
            _regcount = 0
            while self.w.running.is_set():
                self.w.p.poll(0)
                ud = self.gen_userdata()
                self.request_email(ud)
                for c in self.hooks['pre_register_new_user']:
                    c(self, ud)
                self.log.info('Generated new userdata: %s, registering', ud['login'])
                self.log.debug('Userdata: %s', repr(ud))
                try:
                    udc = ud.copy()
                    if 0 in udc:
                        del udc[0]
                    self.register(**udc)
                except beon.Success as e:
                    self.validate_email(ud)
                    for c in self.hooks['post_register_new_user']:
                        c(self, ud)
                    return ud
                except (beon.EmptyAnswer, beon.Wait5Min) as e:
                    self.log.error('%s, sleeping for 100 seconds', e)
                    self.long_sleep(100)
                except beon.Captcha as e:
                    self.log.error('Too much wrong answers to CAPTCHA')
                    continue
                except beon.UnknownAnswer as e:
                    _regcount += 1
                    if not _regcount < self.reglimit:
                        raise beon.RegRetryLimit('Cannot register new user')
                    self.log.error('%s, userdata may be invalid, retrying c:%d',
                                e, _regcount)
                    self.w.sleep(self.errortimeout)
            else:
                raise WorkerInterrupt()

    def get_new_user(self):
        ud = self.userqueue.get(True, self.uqtimeout)
        self.userqueue.task_done()
        for c in self.hooks['check_new_user']:
            c(self, ud)
        return ud

    def login(self, login, passwd, **kvargs):
        if not self.site.login_lock.acquire(False):
            with self.site.login_lock.acquire():
                return
        self.logined = False
        try:
            self.captcha_wrapper(self.site.logininc, self.site.loginfin,
                                 login, passwd, **kvargs)
        except beon.Success as e:
            self.logined = True
            self.counters['logged_in'] += 1
            self.log.info(e)
            raise
        finally:
            self.site.login_lock.release()

    def find_login(self, rec):
        try:
            return re.findall(regexp.var_login, rec)[0]
        except IndexError:
            raise beon.PermanentError('No users in here')

    def get_current_login(self):
        return self.find_login(self.site.get_page('1'))

    def dologin(self):
        '''Choose user, do login and return it.'''
        while self.w.running.is_set():
            self.site.ud = None
            try:
                self.site.ud = self.get_new_user()
            except Empty:
                self.log.info('No users in queue')
                self.site.ud = self.register_new_user()
                return
            try:
                with cstate(self, WipeState.logging_in):
                    self.login(self.site.ud['login'], self.site.ud['passwd'])
            except beon.Success as e:
                self.site.postuser = self.site.ud['login']
                self.site.postpass = self.site.ud['passwd']
                self.validate_email(self.site.ud)
                for c in self.hooks['post_login']:
                    c(self, self.site.ud)
                self.w.sleep(self.successtimeout)
                return
            except beon.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.schedule(self.long_sleep, (10,))
                self.schedule(self.dologin)
            except beon.InvalidLogin as e:
                self.log.error(""Invalid login, passing here"")
                self.schedule(self.dologin)
                self.w.sleep(self.errortimeout)
            except beon.TemporaryError as e:
                self.userqueue.put(self.site.ud)
                self.log.warn(e)
                self.schedule(self.dologin)
                self.w.sleep(self.errortimeout)
        # else:
        #     pending = len(self.pc.sets['pending'])
        #     self.log.warn(""No more logins here, %s pending.""%pending)
        #     if pending == 0: return False

    def relogin(self):
        '''Relogin with current user or do login'''
        if 'login' in self.site.ud:
            while self.w.running.is_set():
                try:
                    with cstate(self, WipeState.logging_in):
                        self.login(self.site.ud['login'], self.site.ud['passwd'])
                except beon.Success as e:
                    for c in self.hooks['post_login']:
                        c(self, self.site.ud)
                    self.w.sleep(self.successtimeout)
                    return
                except beon.InvalidLogin as e:
                    self.log.error(e)
                    self.w.sleep(self.errortimeout)
                    break
                except beon.TemporaryError as e:
                    self.log.warn(e)
                    self.w.sleep(self.errortimeout)
                    continue
        self.dologin()

    def request_email(self, ud):
        ud['email'] = self.mailrequester.gen_addr()
        ud[0]['email_service'] = type(self.mailrequester).__name__
        ud[0]['email_requested'] = False
        ud[0]['email_validated'] = False

    def validate_email(self, ud):
        if ('email' not in ud or
            'email_service' not in ud[0] or
            'email_requested' not in ud[0] or
            'email_validated' not in ud[0] or
            not ud[0]['email_service'] == type(self.mailrequester).__name__
            or ud[0]['email_validated'] is True):
            return
        if not ud[0]['email_requested']:
            try:
                self.site.validate_email_inc()
            except beon.Success as e:
                ud[0]['email_requested'] = True
                self.log.info(e)
        self.log.info('Requesting messages for %s', ud['email'])
        messages = self.mailrequester.get_messages(ud['email'])
        for msg in messages:
            if not msg['mail_from'].find('<reminder@{0}>'.format(self.site.domain)):
                continue
            h = re.findall(regexp.hashinmail.format(self.site.domain),
                msg['mail_html'])
            if len(h) > 0:
                try:
                    self.site.validate_email_fin(h[0])
                except beon.Success as e:
                    ud[0]['email_validated'] = True
                    self.log.info(e)

    def switch_user(self):
        '''Log in with new user, but return the previous one'''
        if 'login' in self.site.ud:
            self.log.info('Switching user %s', self.site.ud['login'])
            self.return_user()
        self.site.ud = self.register_new_user()

    def return_user(self, ud=None):
        if not ud:
            if (hasattr(self.site, 'ud') and self.site.ud):
                ud = self.site.ud
                self.site.ud = None
            else:
                return
        self.log.info('Returning user %s to userqueue', ud['login'])
        self.userqueue.put(ud, False)

    def postmsg(self, target, msg, tuser=None, **kvargs):
        tpair = (tuser, target)
        target = target.lstrip('0')
        try:
            try:
                self.site.ajax_addcomment(target, msg, tuser, **kvargs)
            except beon.Success as e:
                self.update_caprate(False)
                raise
            except beon.Redir as e:
                self.log.warn(e)
                self.log.warn('Using non-ajax addcomment')
                self.captcha_wrapper(self.site.addcomment, self.site.addcommentfin,
                                     target, msg, tuser, **kvargs)
        except beon.Success as e:
            self.counters['comments_added'] += 1
            self.log.debug(e)
            raise
        except beon.Antispam as e:
            self.counters['antispam'] += 1
            self.comment_successtimeout = self.comment_successtimeout + 0.1
            self.log.info('Antispam exc caught, comment_successtimeout + 0.1, cur: %f',
                self.comment_successtimeout)
            raise
        except beon.GuestDeny as e:
            self.counters['delogin'] += 1
            self.log.warn('%s, trying to log in', e)
            self.schedule_first(self.relogin)
            raise
        except beon.Bumplimit as e:
            self.log.info(e)
            self.pc.sets['bumplimit'].add(tpair)
            raise
        except (beon.Closed, beon.UserDeny) as e:
            self.pc.sets['closed'].add(tpair)
            if self.stoponclose:
                self.log.info(e)
                raise beon.PermClosed(""%s:%s is closed"", tpair, e.answer)
            else:
                self.log.info('%s, starting 300s remove timer', e)
                self.pc.add_waiting('closed', tpair, 300)
                raise
        except beon.Wait5Min as e:
            self.counters['wait5mincount'] += 1
            self.log.warn(e)
            raise
        except beon.TemporaryError as e:
            self.log.warn(e)
            raise
        except beon.PermanentError as e:
            self.log.error(e)
            raise

    def addtopic(self, msg, subj, forum='1', tuser=None, **kvargs):
        try:
            self.captcha_wrapper(self.site.addtopicinc, self.site.addtopicfin,
                                 msg, forum, subj, tuser, **kvargs)
        except beon.Success as e:
            self.counters['topics_added'] += 1
            self.log.debug(e)
            raise
        except beon.Wait5Min as e:
            self.counters['wait5min'] += 1
            raise
            # self._bancount += 1
            # if 'login' in self.site.ud:
            #     self.log.warn(e)
            #     self.log.warn('Trying to change user')
            #     self.pc.sets['pending'].add(self.site.ud['login'])
            #     self.pc.add_waiting('pending', self.site.ud['login'], 300)
            #     self.dologin()
            # else:
            #     raise
        except beon.GuestDeny as e:
            if 'login' not in self.site.ud:
                raise
            self.counters['delogin'] += 1
            self.log.warn('%s, trying to log in', e)
            self.schedule_first(self.dologin)
            raise

    def register(self, login, passwd, name, email, **kvargs):
        self.logined = False
        try:
            self.captcha_wrapper(self.site.reginc, self.site.regfin,
                                 login, passwd, name, email, **kvargs)
        except beon.Success as e:
            self.log.info(e)
            self.logined = True
            self.counters['users_registered'] += 1
            raise

    def solve_captcha(self, page):
        # with cstate(self, WipeState.deobfuscating_capage):
        self.log.info('Deobfuscating capage')
        capair = self.w.deobfuscate_capage(self.site.domain, page)
        self.log.info('Answer: %s', repr(capair))
        if len(capair) != 2:
            raise PermOCRError('Invalid answer from Evaluator')
        self.log.info('Downloading captcha image')
        try:
            img = self.http_request(capair[1])
        except sup.net.HTTPError as e:
            # check error code here
            self.log.error(e)
            raise PermOCRError('404 Not Found on caurl', cahash=capair[0])
        self.log.info('Sending captcha image to solver')
        try:
            result, cid = self.w.solve_captcha(img)
        except OCRError as e:
            e.cahash = capair[0]
            raise
        return capair[0], result, cid

    def report_code(self, cid, status):
        self.log.info('Reporting %s code for %s', status, cid)
        self.w.report_code(cid, status)
        self.counters['captcha_codes_reported'] += 1

    def run(self, caller):
        self.w = caller
        self.log = logging.getLogger(self.name)
        self.run_time = Ticker()
        cst = cstate(self, WipeState.starting)
        cst.__enter__()
        self.mailrequester = self.mrc(self.noproxy_rp, self.w.running, self.w.sleep)

        # Get our own logger here, or use worker's?
        self.log.info('Starting')
        self.run_time.tick()

        def drop_user_handler(interface, method, data):
            self.log.info('drop-user signal recieved')
            self.dologin()

        self.w.p.wz.set_sig_handler(b'WipeSkel', b'drop-user', drop_user_handler)

        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeSkel')
        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))

        try:
            self._run()
        except Exception as e:
            self.log.exception(e)
        cst.__exit__(None, None, None)
        with cstate(self, WipeState.terminating):
            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, b'WipeSkel')
            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, bytes(self.name, 'utf-8'))
            self.w.p.wz.del_sig_handler(b'WipeSkel', b'drop-user')
            self.log.info(repr(self.counters))
        self.log.info('Terminating, runtime is %ds', self.run_time.elapsed(False))
/n/n/n",0
85,42b020edfe6b23b245938d23ff7a0484333d6450,"/evproxy.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
import wzrpc
from sup.ticker import Ticker

class EvaluatorProxy:
    def __init__(self, ev_init, *args, **kvargs):
        super().__init__()
        self.ev_init = ev_init
        self.bind_kt_ticker = Ticker()
        self.bind_kt = 5

    def handle_evaluate(self, reqid, interface, method, data):
        domain, page = data
        self.p.log.info('Recvd page %s, working on', reqid)
        res = self.ev.solve_capage(domain, page)
        self.p.log.info('Done, sending answer: %s', res)
        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])

    def send_keepalive(self):
        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],
            self.handle_keepalive_reply)
        msg.insert(0, b'')
        self.p.wz_sock.send_multipart(msg)

    def handle_keepalive_reply(self, reqid, seqnum, status, data):
        if status == wzrpc.status.success:
            self.p.log.debug('Keepalive was successfull')
        elif status == wzrpc.status.e_req_denied:
            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.
                format(wzrpc.name_status(status)))
            self.p.auth_requests()
            self.p.bind_methods()
        elif status == wzrpc.status.e_timeout:
            self.p.log.warn('Keepalive timeout')
        else:
            self.p.log.warn('Keepalive status {0}'.
                format(wzrpc.name_status(status)))

    def __call__(self, parent):
        self.p = parent
        self.p.wz_connect()
        self.p.wz_auth_requests = [
            (b'Router', b'auth-bind-route'),
            (b'Router', b'auth-unbind-route'),
            (b'Router', b'auth-set-route-type')]
        self.p.wz_bind_methods = [
            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]
        self.p.auth_requests()
        self.p.bind_methods()
        self.ev = self.ev_init()
        self.bind_kt_ticker.tick()
        while self.p.running.is_set():
            socks = self.p.poll()
            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:
                self.bind_kt_ticker.tick()
                self.send_keepalive()
/n/n/n/lib/wzrpc/wzhandler.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from . import *
from .wzbase import WZBase

class WZHandler(WZBase):
    def __init__(self):
        self.req_handlers = {}
        self.response_handlers = {}
        self.sig_handlers = {}
        self.iden_reqid_map = BijectiveSetMap()

    def set_req_handler(self, interface, method, fun):
        self.req_handlers[(interface, method)] = fun

    def set_response_handler(self, reqid, fun):
        self.response_handlers[reqid] = fun

    def set_sig_handler(self, interface, method, fun):
        self.sig_handlers[(interface, method)] = fun
    
    def del_req_handler(self, interface, method):
        del self.req_handlers[(interface, method)]

    def del_response_handler(self, reqid):
        del self.response_handlers[reqid]

    def del_sig_handler(self, interface, method):
        del self.sig_handlers[(interface, method)]

    def _parse_req(self, iden, msg, reqid, interface, method):
        try:
            handler = self.req_handlers[(interface, method)]
        except KeyError:
            try:
                handler = self.req_handlers[(interface, None)]
            except KeyError:
                raise WZENoReqHandler(iden, reqid,
                    'No req handler for %s,%s'%(interface, method))
        if iden:
            self.iden_reqid_map.add_value(tuple(iden), reqid)
        handler(reqid, interface, method, msg[1:])
        return ()

    def _parse_rep(self, iden, msg, reqid, seqnum, status):
        try:
            handler = self.response_handlers[reqid]
            if seqnum == 0:
                del self.response_handlers[reqid]
        except KeyError:
            raise WZENoHandler(iden, 'No rep handler for reqid')
        handler(reqid, seqnum, status, msg[1:])
        return ()

    def _parse_sig(self, iden, msg, interface, method):
        try:
            handler = self.sig_handlers[(interface, method)]
        except KeyError:
            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))
        handler(interface, method, msg[1:])
        return ()

    def make_req_msg(self, interface, method, args, fun, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        msg = make_req_msg(interface, method, args, reqid)
        self.set_response_handler(reqid, fun)
        return msg
    
    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):
        msg = iden[:]
        msg.append(b'')
        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))
        return msg
    
    def make_router_rep_msg(self, reqid, seqnum, status, answer):
        iden = self.iden_reqid_map.get_key(reqid)
        if seqnum == 0:
            self.iden_reqid_map.del_value(iden, reqid)
        msg = list(iden)
        msg.append(b'')
        msg.extend(make_rep_msg(reqid, seqnum, status, answer))
        return msg

    def get_iden(self, reqid):
        return self.iden_reqid_map.get_key(reqid)

    def get_reqids(self, iden):
        return self.iden_reqid_map.get_values(iden)

    def make_reqid(self):
        while True:
            reqid = random.randint(1, (2**64)-1)
            if not reqid in self.response_handlers:
                return reqid
        
    def make_auth_req_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-request', args, reqid)

    def make_auth_bind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        
        return (b'Router', b'auth-bind-route', args, reqid)

    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        
        return (b'Router', b'auth-unbind-route', args, reqid)

    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, struct.pack('!B', type_),
                make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-set-route-type', args, reqid)

    def make_auth_clear_data(self, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        return (b'Router', b'auth-clear', [], reqid)

    def req_from_data(self, d, fun):
        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])
  
    def _parse_err(self, iden, msg, status):
        pass

    def _handle_nil(self, iden, msg):
        pass
/n/n/n/lib/wzworkers.py/n/nimport zmq
import threading, multiprocessing
import logging
from sup.ticker import Ticker
# from sup import split_frames
import wzrpc
from wzrpc.wzhandler import WZHandler
import wzauth_data

class WorkerInterrupt(Exception):
    '''Exception to raise when self.running is cleared'''
    def __init__(self):
        super().__init__('Worker was interrupted at runtime')

class Suspend(Exception):
    # if we need this at all.
    '''Exception to raise on suspend signal'''
    def __init__(self, interval, *args, **kvargs):
        self.interval = interval
        super().__init__(*args, **kvargs)

class Resume(Exception):
    '''Exception to raise when suspend sleep is interrupted'''

class WZWorkerBase:
    def __init__(self, wz_addr, fun, args=(), kvargs={},
            name=None, start_timer=None, poll_timeout=None,
            pargs=(), pkvargs={}):
        super().__init__(*pargs, **pkvargs)
        self.name = name if name else type(self).__name__
        self.start_timer = start_timer
        self.poll_timeout = poll_timeout if poll_timeout else 5*1000
        self.call = (fun, args, kvargs)

        self.wz_addr = wz_addr
        self.wz_auth_requests = []
        self.wz_bind_methods = []
        self.wz_poll_timeout = 30

    def __sinit__(self):
        '''Initializes thread-local interface on startup'''
        self.log = logging.getLogger(self.name)
        self.running = threading.Event()
        self.sleep_ticker = Ticker()
        self.poller = zmq.Poller()

        s = self.ctx.socket(zmq.SUB)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        s.connect(self.sig_addr)
        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')
        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')
        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))
        self.sig_sock = s

        s = self.ctx.socket(zmq.DEALER)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        self.wz_sock = s

        self.wz = WZHandler()

        def term_handler(interface, method, data):
            self.log.info(
                'Termination signal %s recieved',
                repr((interface, method, data)))
            self.term()
            raise WorkerInterrupt()
        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)

        def resumehandler(interface, method, data):
            self.log.info('Resume signal %s recieved',
                repr((interface, method, data)))
            raise Resume()

        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)
        self.running.set()

    def wz_connect(self):
        self.wz_sock.connect(self.wz_addr)

    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):
        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz
        timeout = timeout if timeout else self.wz_poll_timeout
        rs = wzrpc.RequestState(fun)
        msg = self.wz.make_req_msg(interface, method, data,
                                   rs.accept, reqid)
        msg.insert(0, b'')
        s.send_multipart(msg)
        t.tick()
        while self.running.is_set():
            p(timeout*1000)
            if rs.finished:
                if rs.retry:
                    msg = self.wz.make_req_msg(interface, method, data,
                        rs.accept, reqid)
                    msg.insert(0, b'')
                    s.send_multipart(msg)
                    rs.finished = False
                    rs.retry = False
                    continue
                return
            elapsed = t.elapsed(False)
            if elapsed >= timeout:
                t.tick()
                # Notify fun about the timeout
                rs.accept(None, 0, 255, [elapsed])
                # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()
    
    def wz_multiwait(self, requests):
        # TODO: rewrite the retry loop
        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz
        timeout = self.wz_poll_timeout
        rslist = []
        msgdict = {}
        for request in requests:
            rs = wzrpc.RequestState(request[0])
            rslist.append(rs)
            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],
                                    rs.accept, request[1][3])
            msg.insert(0, b'')
            msgdict[rs] = msg
            s.send_multipart(msg)
        while self.running.is_set():
            flag = 0
            for rs in rslist:
                if rs.finished:
                    if not rs.retry:
                        del msgdict[rs]
                        continue
                    s.send_multipart(msgdict[rs])
                    rs.finished = False
                    rs.retry = False
                flag = 1
            if not flag:
                return
            # check rs before polling, since we don't want to notify finished one
            # about the timeout
            t.tick()
            p(timeout*1000)
            if t.elapsed(False) >= timeout:
                for rs in rslist:
                    if not rs.finished:
                        rs.accept(None, 0, 255, []) # Notify fun about the timeout
                        rs.finished = True # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()

    def auth_requests(self):
        for i, m in self.wz_auth_requests:
            def accept(that, reqid, seqnum, status, data):
                if status == wzrpc.status.success:
                    self.log.debug('Successfull auth for (%s, %s)', i, m)
                elif status == wzrpc.status.e_auth_wrong_hash:
                    raise beon.PermanentError(
                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\
                        format(i, m, wzrpc.name_status(status), repr(data)))
                elif wzrpc.status.e_timeout:
                    self.log.warn('Timeout {0}, retrying'.format(data[0]))
                    that.retry = True
                else:
                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,
                        wzrpc.name_status(status), repr(data))
            self.wz_wait_reply(accept,
                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))


    def bind_route(self, i, m, f):
        self.log.debug('Binding %s,%s route', i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.wz.set_req_handler(i, m, f)
                self.log.debug('Succesfully binded route (%s, %s)', i, m)
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            elif wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))

    def set_route_type(self, i, m, t):
        self.log.debug('Setting %s,%s type to %d', i, m, t)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,
                    wzrpc.name_route_type(t))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_set_route_type_data(i, m, t,
                wzauth_data.set_route_type[i, m]))

    def unbind_route(self, i, m):
        if not (i, m) in self.wz.req_handlers:
            self.log.debug('Route %s,%s was not bound', i, m)
            return
        self.log.debug('Unbinding route %s,%s', i, m)
        self.wz.del_req_handler(i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Route unbinded for (%s, %s)', i, m)
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))
    
    def clear_auth(self):
        self.log.debug('Clearing our auth records')
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Auth records on router were cleared')
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())

    def bind_methods(self):
        for i, m, f, t in self.wz_bind_methods:
            self.set_route_type(i, m, t)
            self.bind_route(i, m, f)
    
    def unbind_methods(self):  
        for i, m, f, t in self.wz_bind_methods:
            self.unbind_route(i, m)
        #self.clear_auth()

    def send_rep(self, reqid, seqnum, status, data):
        self.wz_sock.send_multipart(
            self.wz.make_router_rep_msg(reqid, seqnum, status, data))

    def send_success_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.success, data)
    
    def send_error_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.error, data)

    def send_wz_error(self, reqid, data, seqid=0):
        msg = self.wz.make_dealer_rep_msg(
            reqid, seqid, wzrpc.status.error, data)
        self.wz_sock.send_multipart(msg)
        
    def send_to_router(self, msg):
        msg.insert(0, b'')
        self.wz_sock.send_multipart(msg)
    
    # def bind_sig_route(self, routetype, interface, method, fun):
    #     self.log.info('Binding %s,%s as type %d signal route',
    #                   interface, method, routetype)
    #     self.wz.set_signal_handler(interface, method, fun)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    # def unbind_sig_route(self, interface, method):
    #     self.log.info('Deleting %s,%s signal route', interface, method)
    #     self.wz.del_signal_handler(interface, method)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    def inter_sleep(self, timeout):
        self.sleep_ticker.tick()
        self.poll(timeout * 1000)
        while self.sleep_ticker.elapsed(False) < timeout:
            try:
                self.poll(timeout * 1000)
            except Resume as e:
                return

    def poll(self, timeout=None):
        try:
            socks = dict(self.poller.poll(timeout if timeout != None
                else self.poll_timeout))
        except zmq.ZMQError as e:
            self.log.error(e)
            return
        if socks.get(self.sig_sock) == zmq.POLLIN:
            # No special handling or same-socket replies are necessary for signals.
            # Backwards socket replies may be added here.
            frames = self.sig_sock.recv_multipart()
            try:
                self.wz.parse_msg(frames[0], frames[1:])
            except wzrpc.WZError as e:
                self.log.warn(e)
        if socks.get(self.wz_sock) == zmq.POLLIN:
            self.process_wz_msg(self.wz_sock.recv_multipart())
        return socks

    def process_wz_msg(self, frames):
        try:
            for nfr in self.wz.parse_router_msg(frames):
                # Send replies from the handler, for cases when it's methods were rewritten.
                self.wz_sock.send_multipart(nfr)
        except wzrpc.WZErrorRep as e:
            self.log.info(e)
            self.wz_sock.send_multipart(e.rep_msg)
        except wzrpc.WZError as e:
            self.log.warn(e)

    def run(self):
        self.__sinit__()
        if self.start_timer:
            self.inter_sleep(self.start_timer)
        if self.running:
            self.log.info('Starting')
            try:
                self.child = self.call[0](*self.call[1], **self.call[2])
                self.child(self)
            except WorkerInterrupt as e:
                self.log.warn(e)
            except Exception as e:
                self.log.exception(e)
            self.log.info('Terminating')
        else:
            self.log.info('Aborted')
        self.running.set() # wz_multiwait needs this to avoid another state check.
        self.unbind_methods()
        self.running.clear()
        self.wz_sock.close()
        self.sig_sock.close()
    
    def term(self):
        self.running.clear()


class WZWorkerThread(WZWorkerBase, threading.Thread):
    def start(self, ctx, sig_addr, *args, **kvargs):
        self.ctx = ctx
        self.sig_addr = sig_addr
        threading.Thread.start(self, *args, **kvargs)

class WZWorkerProcess(WZWorkerBase, multiprocessing.Process):
    def start(self, sig_addr, *args, **kvargs):
        self.sig_addr = sig_addr
        multiprocessing.Process.start(self, *args, **kvargs)
    
    def __sinit__(self):
        self.ctx = zmq.Context()
        super().__sinit__()
/n/n/n/unistart.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -*- mode: python -*-
import sys
if 'lib' not in sys.path:
    sys.path.append('lib')
import os, signal, logging, threading, re, traceback, time
import random
import zmq
from queue import Queue
import sup
import wzworkers as workers
from dataloader import DataLoader
from uniwipe import UniWipe
from wipeskel import *
import wzrpc
from beon import regexp
import pickle

from logging import config
from logconfig import logging_config
config.dictConfig(logging_config)
logger = logging.getLogger()

ctx = zmq.Context()
sig_addr = 'ipc://signals'
sig_sock = ctx.socket(zmq.PUB)
sig_sock.bind(sig_addr)

# Settings for you
domains = set() # d.witch_domains
targets = dict() # d.witch_targets
protected = set() # will be removed later
forums = dict() # target forums

# from lib import textgen
# with open('data.txt', 'rt') as f:
#     model = textgen.train(f.read())
# def mesasge():
#     while True:
#         s = textgen.generate_sentence(model)
#         try:
#             s.encode('cp1251')
#             break
#         except Exception:
#             continue
#     return s

def message():
    msg = []
    # msg.append('[video-youtube-'+
    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',
    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))
    #            +']')
    msg.append('[image-original-none-http://simg4.gelbooru.com/'
               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')
    msg.append('     .')
    # msg.append('[video-youtube-'+random.choice(
    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',
    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',
    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))
    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))
    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',
    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))
    # +']')
    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg
    # msg.append('Enjoy the view!')
    msg.append(str(random.randint(0, 9999999999)))
    return '\n'.join(msg)

def sbjfun():
    # return 'Out of the darkness we will rise, into the light we will dwell'
    return sup.randstr(1, 30)

# End
import argparse

parser = argparse.ArgumentParser(add_help=True)
parser.add_argument('--only-cache', '-C', action='store_true',
    help=""Disables any requests in DataLoader (includes Witch)"")
parser.add_argument('--no-shell', '-N', action='store_true',
    help=""Sleep instead of starting the shell"")
parser.add_argument('--tcount', '-t', type=int, default=10,
    help='WipeThread count')
parser.add_argument('--ecount', '-e', type=int, default=0,
    help='EvaluatorProxy count')
parser.add_argument('--upload-avatar', action='store_true', default=False,
    help='Upload random avatar after registration')
parser.add_argument('--av-dir', default='randav', help='Directory with avatars')
parser.add_argument('--rp-timeout', '-T', type=int, default=10,
    help='Default rp timeout in seconds')
parser.add_argument('--conlimit', type=int, default=3,
    help='http_request conlimit')
parser.add_argument('--noproxy-timeout', type=int, default=5,
    help='noproxy_rp timeout')

parser.add_argument('--caprate_minp', type=int, default=5,
    help='Cap rate minimum possible count for limit check')
parser.add_argument('--caprate_limit', type=float, default=0.8,
    help='Captcha rate limit')

parser.add_argument('--comment_successtimeout', type=float, default=0.8,
    help='Comment success timeout')
parser.add_argument('--topic_successtimeout', type=float, default=0.1,
    help='Topic success timeout')
parser.add_argument('--errortimeout', type=float, default=3,
    help='Error timeout')


parser.add_argument('--stop-on-closed', action='store_true', default=False,
    help='Forget about closed topics')
parser.add_argument('--die-on-neterror', action='store_true', default=False,
    help='Terminate spawn in case of too many NetErrors')

c = parser.parse_args()

# rps = {}

noproxy_rp = sup.net.RequestPerformer()
noproxy_rp.proxy = ''
noproxy_rp.timeout = c.noproxy_timeout
noproxy_rp.timeout = c.rp_timeout

# rps[''] = noproxy_rp

# Achtung: DataLoader probably isn't thread-safe.
d = DataLoader(noproxy_rp, c.only_cache)
c.router_addr = d.addrs['rpcrouter']
noproxy_rp.useragent = random.choice(d.ua_list)

def terminate():
    logger.info('Shutdown initiated')
    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])
    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])
    for t in threading.enumerate():
        if isinstance(t, threading.Timer):
            t.cancel()
    # try:
    #     wm.term()
    #     wm.join()
    # except: # WM instance is not created yet.
    #     pass
    logger.info('Exiting')

def interrupt_handler(signal, frame):
    pass # Just do nothing

def terminate_handler(signal, frame):
    terminate()

signal.signal(signal.SIGINT, interrupt_handler)
signal.signal(signal.SIGTERM, terminate_handler)

def make_net(proxy, proxytype):
    # if proxy in rps:
    #     return rps[proxy]
    net = sup.net.RequestPerformer()
    net.proxy = proxy
    if proxytype == 'HTTP' or proxytype == 'HTTPS':
        net.proxy_type = sup.proxytype.http
    elif proxytype == 'SOCKS4':
        net.proxy_type = sup.proxytype.socks4
    elif proxytype == 'SOCKS5':
        net.proxy_type = sup.proxytype.socks5
    else:
        raise TypeError('Invalid proxytype %s' % proxytype)
    # rps[proxy] = net
    net.useragent = random.choice(d.ua_list)
    net.timeout = c.rp_timeout
    return net

# UniWipe patching start
def upload_avatar(self, ud):
    if ('avatar_uploaded' in ud[0] and
        ud[0]['avatar_uploaded'] is True):
        return
    files = []
    for sd in os.walk(c.av_dir):
        files.extend(sd[2])
    av = os.path.join(sd[0], random.choice(files))
    self.log.info('Uploading %s as new avatar', av)
    self.site.uploadavatar('0', av)
    ud[0]['avatar'] = av
    ud[0]['avatar_uploaded'] = True

from lib.mailinator import Mailinator
# from lib.tempmail import TempMail as Mailinator

# Move this to WipeManager
def create_spawn(proxy, proxytype, pc, uq=None):
    for domain in domains:
        if domain in targets:
            tlist = targets[domain]
        else:
            tlist = list()
            targets[domain] = tlist
        if domain in forums:
            fset = forums[domain]
        else:
            fset = set()
            forums[domain] = fset
        net = make_net(proxy, proxytype)
        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain
        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,
            uq(domain) if uq else None)
        w.stoponclose = c.stop_on_closed
        w.die_on_neterror = c.die_on_neterror
        w.caprate_minp = c.caprate_minp
        w.caprate_limit = c.caprate_limit
        w.conlimit = c.conlimit
        w.comment_successtimeout = 0.2
        if c.upload_avatar:
            w.hooks['post_login'].append(upload_avatar)
        yield w

# UniWipe patching end

class WipeManager:
    def __init__(self, config, *args, **kvargs):
        super().__init__(*args, **kvargs)
        self.newproxyfile = 'newproxies.txt'
        self.proxylist = set()
        self.c = config
        self.threads = []
        self.processes = []
        self.th_sa = 'inproc://wm-wth.sock'
        self.th_ba = 'inproc://wm-back.sock'
        self.pr_sa = 'ipc://wm-wpr.sock'
        self.pr_ba = 'ipc://wm-back.sock'
        self.userqueues = {}
        self.usersfile = 'wm_users.pickle'
        self.targetsfile = 'wm_targets.pickle'
        self.bumplimitfile = 'wm_bumplimit.pickle'

    def init_th_sock(self):
        self.log.info(
            'Initializing intraprocess signal socket %s', self.th_sa)
        self.th_sock = self.p.ctx.socket(zmq.PUB)
        self.th_sock.bind(self.th_sa)

    def init_th_back_sock(self):
        self.log.info(
            'Initializing intraprocess backward socket %s', self.th_ba)
        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.th_back_sock.bind(self.th_ba)

    def init_pr_sock(self):
        self.log.info(
            'Initializing interprocess signal socket %s', self.pr_sa)
        self.pr_sock = self.p.ctx.socket(zmq.PUB)
        self.pr_sock.bind(self.pr_sa)

    def init_pr_back_sock(self):
        self.log.info(
            'Initializing interprocess backward socket %s', self.pr_ba)
        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.pr_back_sock.bind(self.pr_ba)

    def read_newproxies(self):
        if not os.path.isfile(self.newproxyfile):
            return
        newproxies = set()
        with open(self.newproxyfile, 'rt') as f:
            for line in f:
                try:
                    line = line.rstrip('\n')
                    proxypair = tuple(line.split(' '))
                    if len(proxypair) < 2:
                        self.log.warning('Line %s has too few spaces', line)
                        continue
                    if len(proxypair) > 2:
                        self.log.debug('Line %s has too much spaces', line)
                        proxypair = (proxypair[0], proxypair[1])
                    newproxies.add(proxypair)
                except Exception as e:
                    self.log.exception('Line %s raised exception %s', line, e)
        # os.unlink(self.newproxyfile)
        return newproxies.difference(self.proxylist)

    def add_spawns(self, proxypairs):
        while self.running.is_set():
            try:
                try:
                    proxypair = proxypairs.pop()
                except Exception:
                    return
                self.proxylist.add(proxypair)
                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,
                        self.get_userqueue):
                    self.log.info('Created spawn %s', spawn.name)
                    self.spawnqueue.put(spawn, False)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on create_spawn', e)

    def spawn_workers(self, wclass, count, args=(), kvargs={}):
        wname = str(wclass.__name__)
        self.log.info('Starting %s(s)', wname)
        if issubclass(wclass, workers.WZWorkerThread):
            type_ = 0
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif issubclass(wclass, workers.WZWorkerProcess):
            type_ = 1
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                w = wclass(*args, name='.'.join(
                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),
                    **kvargs)
                if type_ == 0:
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):
        wname = str(fun.__name__)
        self.log.info('Starting %s(s)', wname)
        if type_ == 0:
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif type_ == 1:
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                if type_ == 0:
                    w = workers.WZWorkerThread(
                        self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'th{0}'.format(i))))
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'pr{0}'.format(i))))
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_wipethreads(self):
        return self.spawn_nworkers(0, WipeThread, self.c.tcount,
                                  (self.pc, self.spawnqueue))

    def spawn_evaluators(self):
        self.log.info('Initializing Evaluator')
        from evproxy import EvaluatorProxy
        def ev_init():
            from lib.evaluators.PyQt4Evaluator import Evaluator
            return Evaluator()
        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,
                                  (ev_init,))

    def load_users(self):
        if not os.path.isfile(self.usersfile):
            return
        with open(self.usersfile, 'rb') as f:
            users = pickle.loads(f.read())
        try:
            for domain in users.keys():
                uq = Queue()
                for ud in users[domain]:
                    self.log.debug('Loaded user %s:%s', domain, ud['login'])
                    uq.put(ud)
                self.userqueues[domain] = uq
        except Exception as e:
            self.log.exception(e)
            self.log.error('Failed to load users')

    def save_users(self):
        users = {}
        for d, uq in self.userqueues.items():
            uqsize = uq.qsize()
            uds = []
            for i in range(uqsize):
                uds.append(uq.get(False))
            users[d] = uds
        with open(self.usersfile, 'wb') as f:
            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))
        self.log.info('Saved users')

    def get_userqueue(self, domain):
        try:
            uq = self.userqueues[domain]
        except KeyError:
            self.log.info('Created userqueue for %s', domain)
            uq = Queue()
            self.userqueues[domain] = uq
        return uq

    def load_targets(self):
        fname = self.targetsfile
        if not os.path.isfile(fname):
            return
        with open(fname, 'rb') as f:
            data = pickle.loads(f.read())
        if 'targets' in data:
            self.log.debug('Target list was loaded')
            targets.update(data['targets'])
        if 'forums' in data:
            self.log.debug('Forum set was loaded')
            forums.update(data['forums'])
        if 'domains' in data:
            self.log.debug('Domain set was loaded')
            domains.update(data['domains'])
        if 'sets' in data:
            self.log.debug('Other sets were loaded')
            self.pc.sets.update(data['sets'])

    def load_bumplimit_set(self):
        if not os.path.isfile(self.bumplimitfile):
            return
        with open(self.bumplimitfile, 'rb') as f:
            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))

    def save_targets(self):
        data = {
            'targets': targets,
            'forums': forums,
            'domains': domains,
            'sets': self.pc.sets,
            }
        with open(self.targetsfile, 'wb') as f:
            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))

    def targets_from_witch(self):
        for t in d.witch_targets:
            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':
                try:
                    add_target_exc(t['id'], t['user'])
                except ValueError:
                    pass

    def terminate(self):
        msg = [b'GLOBAL']
        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))
        if hasattr(self, 'th_sock'):
            self.th_sock.send_multipart(msg)
        if hasattr(self, 'pr_sock'):
            self.pr_sock.send_multipart(msg)

    def join_threads(self):
        for t in self.threads:
            t.join()

    def send_passthrough(self, interface, method, frames):
        msg = [frames[0]]
        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
        self.th_sock.send_multipart(msg)
        self.pr_sock.send_multipart(msg)

    def __call__(self, parent):
        self.p = parent
        self.log = parent.log
        self.inter_sleep = parent.inter_sleep
        self.running = parent.running
        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')
        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)
        if self.c.tcount > 0:
            self.pc = ProcessContext(self.p.name, self.p.ctx,
                self.c.router_addr, noproxy_rp)
            self.spawnqueue = Queue()
            self.load_bumplimit_set()
            self.load_targets()
            self.load_users()
            self.spawn_wipethreads()
        if self.c.ecount > 0:
            self.spawn_evaluators()
        try:
            while self.running.is_set():
                # self.targets_from_witch()
                if self.c.tcount == 0:
                    self.inter_sleep(5)
                    continue
                self.pc.check_waiting()
                new = self.read_newproxies()
                if not new:
                    self.inter_sleep(5)
                    continue
                self.add_spawns(new)
        except WorkerInterrupt:
            pass
        except Exception as e:
            self.log.exception(e)
        self.terminate()
        self.join_threads()
        if self.c.tcount > 0:
            self.save_users()
            self.save_targets()

wm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),
    name='SpaghettiMonster')
wm.start(ctx, sig_addr)

def add_target(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Appending %s to targets[%s]', repr(t), domain)
    tlist.append(t)

def remove_target(domain, id_, tuser=None):
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Removing %s from targets[%s]', repr(t), domain)
    tlist.remove(t)

def add_target_exc(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    if t in protected:
        raise ValueError('%s is protected' % repr(t))
    if t not in tlist:
        logger.info('Appending %s to targets[%s]', repr(t), domain)
        tlist.append(t)

r_di = re.compile(regexp.f_udi)

def atfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        add_target(domain, id_, user)

def rtfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        remove_target(domain, id_, user)

def get_forum_id(name):
    id_ = d.bm_id_forum.get_key(name)
    int(id_, 10)  # id is int with base 10
    return id_

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s (%s) to forums', name, id_)
#     forums.append(id_)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s (%s) from forums', name, id_)
#     forums.remove(id_)

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s to forums', name)
#     forums.add(name)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s from forums', name)
#     forums.remove(name)

r_udf = re.compile(regexp.udf_prefix)

def affu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if domain not in forums:
            forums[domain] = set()
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)
        forums[domain].add((user, forum))

def rffu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)
        forums[domain].remove((user, forum))

def add_user(domain, login, passwd):
    uq = wm.get_userqueue(domain)
    uq.put({'login': login, 'passwd': passwd}, False)

def send_to_wm(frames):
    msg = [frames[0]]
    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
    sig_sock.send_multipart(msg)

def send_passthrough(frames):
    msg = [b'WipeManager']
    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))
    sig_sock.send_multipart(msg)

def drop_users():
    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])

def log_spawn_name():
    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])

if c.no_shell:
    while True:
        time.sleep(1)
else:
    try:
        import IPython
        IPython.embed()
    except ImportError:
        # fallback shell
        while True:
            try:
                exec(input('> '))
            except KeyboardInterrupt:
                print(""KeyboardInterrupt"")
            except SystemExit:
                break
            except:
                print(traceback.format_exc())

terminate()
/n/n/n/uniwipe.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from sup.net import NetError
from wzworkers import WorkerInterrupt
from wipeskel import WipeSkel, WipeState, cstate
from beon import exc, regexp
import re

class UniWipe(WipeSkel):
    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):
        self.sbjfun = sbjfun
        self.msgfun = msgfun
        self.forums = forums
        self.targets = (type(targets) == str and [('', targets)]
                        or type(targets) == tuple and list(targets)
                        or targets)
        super().__init__(*args, **kvargs)

    def on_caprate_limit(self, rate):
        if not self.logined:
            self._capdata = (0, 0)
            return
        self.log.warning('Caprate limit reached, calling dologin() for now')
        self.dologin()
        # super().on_caprate_limit(rate)

    def comment_loop(self):
        for t in self.targets:
            self.schedule(self.add_comment, (t, self.msgfun()))
        if len(self.targets) == 0:
            self.schedule(self.scan_targets_loop)
        else:
            self.schedule(self.comment_loop)

    def add_comment(self, t, msg):
        # with cstate(self, WipeState.posting_comment):
        if True: # Just a placeholder
            try:
                # self.counter_tick()
                self.postmsg(t[1], msg, t[0])
            except exc.Success as e:
                self.counters['comments'] += 1
                self.w.sleep(self.comment_successtimeout)
            except exc.Antispam as e:
                self.w.sleep(self.comment_successtimeout)
                self.schedule(self.add_comment, (t, msg))
            except (exc.Closed, exc.UserDeny) as e:
                try:
                    self.targets.remove(t)
                except ValueError:
                    pass
                self.w.sleep(self.comment_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.schedule(self.add_comment, (t, msg))
            except exc.UnknownAnswer as e:
                self.log.warn('%s: %s', e, e.answer)
                self.schedule(self.add_comment, (t, msg))
            except exc.Wait5Min as e:
                self.schedule(self.add_comment, (t, msg))
                self.schedule_first(self.switch_user)
            except exc.EmptyAnswer as e:
                self.log.info('Removing %s from targets', t)
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.schedule(self.add_comment, (t, msg))
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except UnicodeDecodeError as e:
                self.log.exception(e)
                self.w.sleep(self.errortimeout)

    def forumwipe_loop(self):
        for f in self.forums:
            self.counter_tick()
            try:
                self.addtopic(self.msgfun(), self.sbjfun(), f)
            except exc.Success as e:
                self.counters['topics'] += 1
                self.w.sleep(self.topic_successtimeout)
            except exc.Wait5Min as e:
                self.topic_successtimeout = self.topic_successtimeout + 0.1
                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',
                    self.topic_successtimeout)
                self.w.sleep(self.topic_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.long_sleep(10)
            except exc.UnknownAnswer as e:
                self.log.warning('%s: %s', e, e.answer)
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                self.log.error(e)
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.log.warn(e)
                self.w.sleep(self.errortimeout)

    def get_targets(self):
        found_count = 0
        for user, forum in self.forums:
            targets = []
            self.log.debug('Scanning first page of the forum %s:%s', user, forum)
            page = self.site.get_page('1', forum, user)
            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))
            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))
            for t in found:
                if (t in self.pc.sets['closed']
                    or t in self.pc.sets['bumplimit']
                    or t in self.targets):
                    continue
                targets.append(t)
            lt = len(targets)
            found_count += lt
            if lt > 0:
                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)
            else:
                self.log.debug('Found no new targets in forum %s:%s', user, forum)
            self.targets.extend(targets)
        return found_count

    def scan_targets_loop(self):
        with cstate(self, WipeState.scanning_for_targets):
            while len(self.targets) == 0:
                c = self.get_targets()
                if c == 0:
                    self.log.info('No targets found at all, sleeping for 30 seconds')
                    self.long_sleep(30)
            self.schedule(self.comment_loop)
        if len(self.forums) == 0:
            self.schedule(self.wait_loop)

    def wait_loop(self):
        if len(self.targets) > 0:
            self.schedule(self.comment_loop)
            return
        if len(self.forums) == 0:
            with cstate(self, WipeState.waiting_for_targets):
                while len(self.forums) == 0:
                    # To prevent a busy loop.
                    self.counter_tick()
                    self.w.sleep(1)
        self.schedule(self.scan_targets_loop)

    def _run(self):
        self.schedule(self.dologin)
        self.schedule(self.wait_loop)
        self.schedule(self.counter_ticker.tick)
        try:
            self.perform_tasks()
        except NetError as e:
            self.log.error(e)
        except WorkerInterrupt as e:
            self.log.warning(e)
        except Exception as e:
            self.log.exception(e)
        self.return_user()
# tw_flag = False
# if len(self.targets) > 0:
#     with cstate(self, WipeState.posting_comment):
#         while len(self.targets) > 0:
#             self.threadwipe_loop()
#     if not tw_flag:
#         tw_flag = True
# if tw_flag:
#     # Sleep for topic_successtimeout after last comment
#     # to prevent a timeout spike
#     self.w.sleep(self.topic_successtimeout)
#     tw_flag = False
# with cstate(self, WipeState.posting_topic):
# self.forumwipe_loop()
/n/n/n",1
